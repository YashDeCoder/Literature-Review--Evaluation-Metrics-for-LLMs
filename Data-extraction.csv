,YEAR,TITLE,ABSTRACT,KEYWORDS,AUTHORS,VENUE,JOURNAL or Conference,DOI,URL,CIT,E1,E2,E3,E4,I1,I2,I3,Included?,Researcher,Metric,Notes,,,,,,,Old ID
1,2024,Efficient and green large language models for software engineering: Vision and the road ahead,"Large Language Models (LLMs) have recently shown remarkable capabilities in various software engineering tasks, spurring the rapid growth of the Large Language Models for Software Engineering (LLM4SE) area. However, limited attention has been paid to developing efficient LLM4SE techniques that demand minimal computational cost, time, and memory resources, as well as green LLM4SE solutions that reduce energy consumption, water usage, and carbon emissions. This paper aims to redirect the focus of the research community towards the efficiency and greenness of LLM4SE, while also sharing potential research directions to achieve this goal. It commences with a brief overview of the significance of LLM4SE and highlights the need for efficient and green LLM4SE solutions. Subsequently, the paper presents a vision for a future where efficient and green LLM4SE revolutionizes the LLM-based software engineering tool landscape, benefiting various stakeholders, including industry, individual practitioners, and society. The paper then delineates a roadmap for future research, outlining specific research paths and potential solutions for the research community to pursue. While not intended to be a definitive guide, the paper aims to inspire further progress, with the ultimate goal of establishing efficient and green LLM4SE as a central element in the future of software engineering.",General and reference → Surveys and overviews; • Software and its engineering → Software development techniques; • Computing methodologies → Artificial intelligence,"J Shi, Z Yang, D Lo",ACM Transactions on Software Engineering and Methodology (TOSEM),,,https://dl.acm.org/doi/abs/10.1145/3708525,13,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,generic,,,,,,,
2,2024,Agentless: Demystifying llm-based software engineering agents,"Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless -- an agentless approach to automatically solve software development problems. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic three-phase process of localization, repair, and patch validation, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (32.00%, 96 correct fixes) and low cost ($0.70) compared with all existing open-source software agents! Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patch or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-S by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the current overlooked potential of a simple, interpretable technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction. ",LLM4SE,"CS Xia, Y Deng, S Dunn, L Zhang",,,,https://arxiv.org/abs/2407.01489,3,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,"report, this wasn't included because it didn't meet the inclusion criteria of being focused on benchmarking but on LLM agents and they use the SWE benchmark",,,,,,,
3,2025,Revisiting sentiment analysis for software engineering in the era of large language models,"Software development involves collaborative interactions where stakeholders express opinions across various platforms. Recognizing the sentiments conveyed in these interactions is crucial for the effective development and ongoing maintenance of software systems. For software products, analyzing the sentiment of user feedback, e.g., reviews, comments, and forum posts can provide valuable insights into user satisfaction and areas for improvement. This can guide the development of future updates and features. However, accurately identifying sentiments in software engineering datasets remains challenging.
This study investigates bigger large language models (bLLMs) in addressing the labeled data shortage that hampers fine-tuned smaller large language models (sLLMs) in software engineering tasks. We conduct a comprehensive empirical study using five established datasets to assess three open source bLLMs in zero-shot and few-shot scenarios. Additionally, we compare them with fine-tuned sLLMs, using sLLMs to learn contextual embeddings of text from software platforms.
Our experimental findings demonstrate that bLLMs exhibit state-of-the-art performance on datasets marked by limited training data and imbalanced distributions. bLLMs can also achieve excellent performance under a zero-shot setting. However, when ample training data are available or the dataset exhibits a more balanced distribution, fine-tuned sLLMs can still achieve superior results.",Software and its engineering → Maintaining software;,"T Zhang, IC Irsan, F Thung, D Lo",,,,https://dl.acm.org/doi/abs/10.1145/3697009,0,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
4,2025,Towards an understanding of large language models in software engineering tasks,"Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.","Empirical study· Literature review· Large language models· Software Engineering, LLM4SE","Z Zheng, K Ning, Q Zhong, J Chen, W Chen‚Ä¶",,,,https://link.springer.com/article/10.1007/s10664-024-10602-0,3,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,"generic, Didn't meet inclusion criteria 1",,,,,,,
5,2024,Evocodebench: An evolving code generation benchmark with domain-specific evaluations,"How to evaluate Large Language Models (LLMs) in code generation remains an open question. Many benchmarks have been proposed, but they have two limitations, i.e., data leakage and lack of domain-specific evaluation.The former hurts the fairness of benchmarks, and the latter hinders practitioners from selecting superior LLMs for specific programming domains.To address these two limitations, we propose a new benchmark - EvoCodeBench, which has the following advances: (1) Evolving data. EvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid data leakage. This paper releases the first version - EvoCodeBench-2403, containing 275 samples from 25 repositories.(2) A domain taxonomy and domain labels. Based on the statistics of open-source communities, we design a programming domain taxonomy consisting of 10 popular domains. Based on the taxonomy, we annotate each sample in EvoCodeBench with a domain label. EvoCodeBench provides a broad platform for domain-specific evaluations.(3) Domain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific Improvement (DSI) and define LLMs' comfort and strange domains. These evaluations help practitioners select superior LLMs in specific domains and discover the shortcomings of existing LLMs.Besides, EvoCodeBench is collected by a rigorous pipeline and aligns with real-world repositories in multiple aspects (e.g., code distributions).We evaluate 8 popular LLMs (e.g., gpt-4, DeepSeek Coder, StarCoder 2) on EvoCodeBench and summarize some insights. EvoCodeBench reveals the actual abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is only 20.74%. Besides, we evaluate LLMs in different domains and discover their comfort and strange domains. For example, gpt-4 performs best in most domains but falls behind others in the Internet domain. StarCoder 2-15B unexpectedly performs well in the Database domain and even outperforms 33B LLMs. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.",LLM4SE,"J Li, G Li, X Zhang, Y Zhao, Y Dong‚Ä¶",,,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/6a059625a6027aca18302803743abaa2-Abstract-Datasets_and_Benchmarks_Track.html,3,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,pass@k,,,,,,,
6,2024,JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models,"Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs’ capabilities. However, after consolidating the
latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8% of benchmarks in-
volve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs’ capability to generate Java
code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a
mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks
primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented
Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced fea-
tures in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.
To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java
projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students,
reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and
tests. To better evaluate LLM’s capability against JavaBench, we introduce a systematic evaluation design covering three context set-
tings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several inter-
esting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students",LLM4SE,"J Cao, Z Chen, J Wu, SC Cheung, C Xu",,,,https://dl.acm.org/doi/abs/10.1145/3691620.3695470,0,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,"pass@k, Java",,,,,,,
7,2023,Large language models are few-shot testers: Exploring llm-based general bug reproduction,"Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose Libro, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of Libro shows that, on the widely studied Defects4J benchmark, Libro can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate Libro against 31 bug reports submitted after the collection of the LLM training data terminated: Libro produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show Libro has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.",LLM4SE,"S Kang, J Yoon, S Yoo",,,,https://ieeexplore.ieee.org/abstract/document/10172763/,2,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,,framework,,,,,,,
8,2023,"Large language model assisted software engineering: prospects, challenges, and a case study","Large language models such as OpenAI’s GPT and Google’s Bard offer new opportunities for supporting software engineering processes. Large language model assisted software engineering promises to support developers in a conversational way with expert knowledge over the whole software lifecycle. Current applications range from requirements extraction, ambiguity resolution, code and test case generation, code review and translation to verification and repair of software vulnerabilities. In this paper we present our position on the potential benefits and challenges associated with the adoption of language models in software engineering. In particular, we focus on the possible applications of large language models for requirements engineering, system design, code and test generation, code quality reviews, and software process management. We also give a short review of the state-of-the-art of large language model support for software construction and illustrate our position by a case study on the object-oriented development of a simple “search and rescue” scenario.",LLM4SE,"L Belzner, T Gabor, M Wirsing",,,,https://link.springer.com/chapter/10.1007/978-3-031-46002-9_23,5,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
9,2024,"Top leaderboard ranking= top coding proficiency, always? evoeval: Evolving coding benchmarks via llm","LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows that compared to the high performance obtained on standard benchmarks like HumanEval, there is a significant drop in performance (on average 39.4%) when using EvoEval. Additionally, the decrease in performance can range from 19.6% to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential overfitting of existing benchmarks. Furthermore, we showcase various insights, including the brittleness of instruction-following models when encountering rewording or subtle changes as well as the importance of learning problem composition and decomposition. EvoEval not only provides comprehensive benchmarks, but can be used to further evolve arbitrary problems to keep up with advances and the ever-changing landscape of LLMs for code. We have open-sourced our benchmarks, tools, and complete LLM generations at this https URL",LLM4SE,"CS Xia, Y Deng, L Zhang",,,,https://arxiv.org/abs/2403.19114,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,pass@k,,,,,,,
10,2024,Multitask-based evaluation of open-source llm on software vulnerability,"This paper proposes a pipeline for quantitatively evaluating interactive Large Language Models (LLMs) using publicly available datasets. We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks. This evaluation assesses the multi-tasking capabilities of LLMs based on this dataset. We find that the existing state-of-the-art approaches and pre-trained Language Models (LMs) are generally superior to LLMs in software vulnerability detection. However, in software vulnerability assessment and location, certain LLMs (e.g., CodeLlama and WizardCoder) have demonstrated superior performance compared to pre-trained LMs, and providing more contextual information can enhance the vulnerability assessment capabilities of LLMs. Moreover, LLMs exhibit strong vulnerability description capabilities, but their tendency to produce excessive output significantly weakens their performance compared to pre-trained LMs. Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights into the capabilities of LLMs in handling software vulnerabilities.","Software vulnerability analysis, large language
model, LLM4SE","X Yin, C Ni, S Wang",,,,https://ieeexplore.ieee.org/abstract/document/10706805/,368,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"F1-score, Recall, Precision, Accuracy, acc@k,  FPR metrics, ROUGE-1, ROUGE-2, ROUGE-L","F1-score, Recall, Precision, Accuracy, acc@k,  FPR metrics, ROUGE-1, ROUGE-2, ROUGE-L",,,,,,,
11,2022,On the validity of pre-trained transformers for natural language processing in the software engineering domain,"Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.",LLM4SE,"J Von der Mosel, A Trautsch‚Ä¶",,,,https://ieeexplore.ieee.org/abstract/document/9785808/,8,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,"generic, and I don't see any relevant metrics. Metrics used seem to be focused on statistics",,,,,,,
12,2024,Can LLMs replace manual annotation of software engineering artifacts?,"Experimental evaluations of software engineering innovations, e.g., tools and processes, often include human-subject studies as a component of a multi-pronged strategy to obtain greater generalizability of the findings. However, human-subject studies in our field are challenging, due to the cost and difficulty of finding and employing suitable subjects, ideally, professional programmers with varying degrees of experience. Meanwhile, large language models (LLMs) have recently started to demonstrate human-level performance in several areas. This paper explores the possibility of substituting costly human subjects with much cheaper LLM queries in evaluations of code and code-related artifacts. We study this idea by applying six state-of-the-art LLMs to ten annotation tasks from five datasets created by prior work, such as judging the accuracy of a natural language summary of a method or deciding whether a code change fixes a static analysis warning. Our results show that replacing some human annotation effort with LLMs can produce inter-rater agreements equal or close to human-rater agreement. To help decide when and how to use LLMs in human-subject studies, we propose model-model agreement as a predictor of whether a given task is suitable for LLMs at all, and model confidence as a means to select specific samples where LLMs can safely replace human annotators. Overall, our work is the first step toward mixed human-LLM evaluations in software engineering. ","LLMs, human subjects, evaluation, LLM4SE","T Ahmed, P Devanbu, C Treude, M Pradel",,,,https://arxiv.org/abs/2408.05534,1,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,human annotators,human annotators,,,,,,,
13,2023,Prompt engineering or fine tuning: An empirical assessment of large language models in automated software engineering tasks,"The rapid advancements in large language models (LLMs) have greatly expanded the potential for automated code-related tasks. Two primary methodologies are used in this domain: prompt engineering and fine-tuning. Prompt engineering involves applying different strategies to query LLMs, like ChatGPT, while fine-tuning further adapts pre-trained models, such as CodeBERT, by training them on task-specific data. Despite the growth in the area, there remains a lack of comprehensive comparative analysis between the approaches for code models. In this paper, we evaluate GPT-4 using three prompt engineering strategies -- basic prompting, in-context learning, and task-specific prompting -- and compare it against 17 fine-tuned models across three code-related tasks: code summarization, generation, and translation. Our results indicate that GPT-4 with prompt engineering does not consistently outperform fine-tuned models. For instance, in code generation, GPT-4 is outperformed by fine-tuned models by 28.3% points on the MBPP dataset. It also shows mixed results for code translation tasks. Additionally, a user study was conducted involving 27 graduate students and 10 industry practitioners. The study revealed that GPT-4 with conversational prompts, incorporating human feedback during interaction, significantly improved performance compared to automated prompting. Participants often provided explicit instructions or added context during these interactions. These findings suggest that GPT-4 with conversational prompting holds significant promise for automated code-related tasks, whereas fully automated prompt engineering without human involvement still requires further investigation. ","Prompt engineering, Fine-tuning, LLM4SE,
Empirical study, Survey","J Shin, C Tang, T Mohati, M Nayebi, S Wang‚Ä¶",,,,https://arxiv.org/abs/2310.10508,7,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"BLEU, pass@k, Human Annotators","BLEU, pass@k, human annotators",,,,,,,
14,2024,Mutation-based consistency testing for evaluating the code understanding capability of llms,"Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.
In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.
We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development of LLM-based software engineering.","Software and its engineering → Software testing and debugging; Empirical software validation, LLM4SE","Z Li, D Shin",,,,https://dl.acm.org/doi/abs/10.1145/3644815.3644946,6,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,,Does not describe metrics properly,,,,,,,
15,2023,Codetf: One-stop transformer library for state-of-the-art code llm,"Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners. ",Transformer · code large language models · code understanding · code generation · code intelligence,"NDQ Bui, H Le, Y Wang, J Li, AD Gotmare‚Ä¶",,,,https://arxiv.org/abs/2306.00029,5,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,This might interesting to use for future studies,,,,,,,
16,2024,Evaluating large language models in class-level code generation,"Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios.
To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.","Class-level Code Generation, Large Language Model, Benchmark, LLM4SE","X Du, M Liu, K Wang, H Wang, J Liu, Y Chen‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3597503.3639219,5,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"pass@k, dep@k","pass@k, dep@k",,,,,,,
17,2025,FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation,"Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks
lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the
ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from
83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task
instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementa-
tion requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other rel-
evant parts in the code repository, providing a more comprehensive evaluation method of LLMs’ automated software engineering capa-
bilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such
repository-level incremental code development. Our code will soon be publicly available at https://github.com/microsoft/FEA-Bench.",LLM4SE,"W Li, X Zhang, Z Guo, S Mao, W Luo, G Peng‚Ä¶",,,,https://arxiv.org/abs/2503.06680,7,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,Again not sure if I want to add it cause I can't seem to find the metrics,,,,,,,
18,2024,Autocoderover: Autonomous program improvement,"Researchers have made significant progress in automating the software development process in the past decades. Automated techniques for issue summarization, bug reproduction, fault localization, and program repair have been built to ease the workload of developers. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. program repair to fix bugs) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving Github issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM’s understanding of the issue’s root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on the recently proposed SWE-bench-lite (300 real-life Github issues) show increased efficacy in solving Github issues (19% on SWE-bench-lite), which is higher than the efficacy of the recently reported Swe-agent. Interestingly,
our approach resolved 57 GitHub issues in about 4 minutes each (pass@1), whereas developers spent more than 2.68 days on average. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.","Software and its engineering → Automatic programming;
Maintaining software; Software testing and debugging; • Com-
puting methodologies → Natural language processing, LLM4SE","Y Zhang, H Ruan, Z Fan, A Roychoudhury",,,,https://dl.acm.org/doi/abs/10.1145/3650212.3680384,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,pass@k,,,,,,,
19,2024,Can llm replace stack overflow? a study on robustness and reliability of large language model code generation,"Recently, large language models (LLMs) have shown an extraordinary ability to understand natural language and generate programming code. It has been a common practice for software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability, and robustness of the code generation from LLMs
have not yet been thoroughly studied. The executable code is not equivalent to reliable and robust code, especially in the context of real-world software development. For example, the misuse of APIs in the generated code could lead to severe problems, such as resource leaks, program crashes, etc. Existing code evaluation benchmarks and datasets focus on crafting small tasks such as programming questions in coding
interviews. However, this deviates from the problems developers typically consult LLMs about. To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs. We collect 1208 coding questions from Stack Overflow on 18 representative Java APIs. We summarize the common misuse patterns of these APIs and evaluate them on current popular LLMs. The
evaluation results show that even GPT-4 has 62% of the generated code that contains API misuses. It would cause unexpected consequences if the code is introduced into real-world software",LLM4SE,"L Zhong, Z Wang",,,,https://ojs.aaai.org/index.php/AAAI/article/view/30185,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"API misuse rate, Compilation rate, Overall API Misuse rate","API misuse rate, Compilation rate, Overall API Misuse rate",,,,,,,
20,2024,Copilot evaluation harness: Evaluating llm-guided software programming,"The integration of Large Language Models (LLMs) into Development Environments (IDEs) has become a focal point in modern software development. LLMs such as OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment developer productivity by serving as intelligent, chat-driven programming assistants. However, utilizing LLMs out of the box is unlikely to be optimal for any given scenario. Rather, each system requires the LLM to be honed to its set of heuristics to ensure the best performance. In this paper, we introduce the Copilot evaluation harness: a set of data and tools for evaluating LLM-guided IDE interactions, covering various programming scenarios and languages. We propose our metrics as a more robust and information-dense evaluation than previous state of the art evaluation systems. We design and compute both static and execution based success metrics for scenarios encompassing a wide range of developer tasks, including code generation from natural language (generate), documentation generation from code (doc), test case generation (test), bug-fixing (fix), and workspace understanding and query resolution (workspace). These success metrics are designed to evaluate the performance of LLMs within a given IDE and its respective parameter space. Our learnings from evaluating three common LLMs using these metrics can inform the development and validation of future scenarios in LLM guided IDEs. ","Large Language Models, VSCode, Copilot, Code
Generation Evaluation, LLM4SE","A Agarwal, A Chan, S Chandel, J Jang, S Miller‚Ä¶",,,,https://arxiv.org/abs/2402.14261,1,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"Syntax Correctness, Test pass rate, MRR","Syntax Correctness, Test pass rate, MRR",,,,,,,
21,2022,An empirical study of the effectiveness of an ensemble of stand-alone sentiment detection tools for software engineering datasets,"Sentiment analysis in software engineering (SE) has shown promise to analyze and support diverse development activities. Recently, several tools are proposed to detect sentiments in software artifacts. While the tools improve accuracy over off-the-shelf tools, recent research shows that their performance could still be unsatisfactory. A more accurate sentiment detector for SE can help reduce noise in analysis of software scenarios where sentiment analysis is required. Recently, combinations, i.e., hybrids of stand-alone classifiers are found to offer better performance than the stand-alone classifiers for fault detection. However, we are aware of no such approach for sentiment detection for software artifacts. We report the results of an empirical study that we conducted to determine the feasibility of developing an ensemble engine by combining the polarity labels of stand-alone SE-specific sentiment detectors. Our study has two phases. In the first phase, we pick five SE-specific sentiment detection tools from two recently published papers by Lin et al. [29, 30], who first reported negative results with stand alone sentiment detectors and then proposed an improved SE-specific sentiment detector, POME [29]. We report the study results on 17,581 units (sentences/documents) coming from six currently available sentiment benchmarks for software engineering. We find that the existing tools can be complementary to each other in 85-95% of the cases, i.e., one is wrong but another is right. However, a majority voting-based ensemble of those tools fails to improve the accuracy of sentiment detection. We develop Sentisead, a supervised tool by combining the polarity labels and bag of words as features. Sentisead improves the performance (F1-score) of the individual tools by 4% (over Senti4SD [5]) – 100% (over POME [29]). The initial development of Sentisead occurred before we observed the use of deep learning models for SE-specific sentiment detection. In particular, recent papers show the superiority of advanced language-based pre-trained transformer models (PTM) over rule-based and shallow learning models. Consequently, in a second phase, we compare and improve Sentisead infrastructure using the PTMs. We find that a Sentisead infrastructure with RoBERTa as the ensemble of the five stand-alone rule-based and shallow learning SE-specific tools from Lin et al. [29, 30] offers the best F1-score of 0.805 across the six datasets, while a stand-alone RoBERTa shows an F1-score of 0.801.","Information Systems → Sentiment analysis; • Computing Methodologies → Machine
Learning LLM4SE","G Uddin, YG Gu√©h√©nuc, F Khomh, CK Roy",,,,https://dl.acm.org/doi/abs/10.1145/3491211,8,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
22,2024,Llm-based test-driven interactive code generation: User study and empirical evaluation,"Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow TiCoder for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.","Intent disambiguation, code generation, LLMs,
human factors, cognitive load, test generation, LLM4SE","S Fakhoury, A Naik, G Sakkas‚Ä¶",,,,https://ieeexplore.ieee.org/abstract/document/10606356/,1,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"pass@k@m, pass@k","Describes pass@k@m and is said to be a deterministic metric? No clue how, pass@k",,,,,,,
23,2024,A performance study of llm-generated code on leetcode,"This study evaluates the efficiency of code generation by Large Language Models (LLMs) and measures their performance against human-crafted solutions using a dataset from Leetcode. We compare 18 LLMs, considering factors such as model temperature and success rate, and their impact on code performance. This research introduces a novel method for measuring and comparing the speed of LLM-generated code, revealing that LLMs produce code with comparable performance, irrespective of the adopted LLM. We also find that LLMs are capable of generating code that is, on average, more efficient than the code written by humans. The paper further discusses the use of Leetcode as a benchmarking dataset, the limitations imposed by potential data contamination, and the platform’s measurement reliability. We believe that our findings contribute to a better understanding of LLM capabilities in code generation and set the stage for future optimizations in the field.",LLM4SE,"T Coignion, C Quinton, R Rouvoy",,,,https://dl.acm.org/doi/abs/10.1145/3661167.3661221,17,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,pass@k,,,,,,,
24,2024,How efficient is llm-generated code? a rigorous &high-standard benchmark,"The emergence of large language models (LLMs) has significantly pushed the frontiers of program synthesis. Advancement of LLM-based program synthesis calls for a thorough evaluation of LLM-generated code. Most evaluation frameworks focus on the (functional) correctness of generated code; efficiency, as an important measure of code quality, has been overlooked in existing evaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a rigorous and high-standard benchmark for evaluating the capability of LLMs in generating efficient code. Firstly, we propose a new efficiency metric called eff@k, which generalizes the pass@k metric from correctness to efficiency and appropriately handles right-censored execution time. Furthermore, we derive an unbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we also provide a numerically stable implementation for the new estimator. Secondly, to set a high-standard for efficiency evaluation, we employ a human expert to design best algorithms and implementations as our reference solutions of efficiency, many of which are much more efficient than existing canonical solutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous evaluation, we employ a human expert to curate strong test case generators to filter out wrong code and differentiate suboptimal algorithms. An extensive study across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still fall short of generating expert-level efficient code. Using two subsets of our problem set, we demonstrate that such deficiency is because current LLMs struggle in designing advanced algorithms and are barely aware of implementation optimization. Our benchmark is publicly available at this https URL . ",LLM4SE,"R Qiu, WW Zeng, J Ezick, C Lott, H Tong",,,,https://arxiv.org/abs/2406.06647,9,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"pass@k, eff@k ","This is extremely interesting for RQ2, as they develop a new metric from an existing one. They use pass@k and eff@k (metric developed by paper). But this seems to be a poster :(, pass@k ",,,,,,,
25,2024,Sciassess: Benchmarking llm proficiency in scientific literature analysis,"Recent breakthroughs in Large Language Models (LLMs) have revolutionized scientific literature analysis. However, existing benchmarks fail to adequately evaluate the proficiency of LLMs in this domain, particularly in scenarios requiring higher-level abilities beyond mere memorization and the handling of multimodal data. In response to this gap, we introduce SciAssess, a benchmark specifically designed for the comprehensive evaluation of LLMs in scientific literature analysis. It aims to thoroughly assess the efficacy of LLMs by evaluating their capabilities in Memorization (L1), Comprehension (L2), and Analysis \& Reasoning (L3). It encompasses a variety of tasks drawn from diverse scientific fields, including biology, chemistry, material, and medicine. To ensure the reliability of SciAssess, rigorous quality control measures have been implemented, ensuring accuracy, anonymization, and compliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting their strengths and areas for improvement. We hope this evaluation supports the ongoing development of LLM applications in scientific literature analysis. SciAssess and its resources are available at \url{this https URL}. ",,"H Cai, X Cai, J Chang, S Li, L Yao, C Wang‚Ä¶",,,,https://arxiv.org/abs/2403.01976,0,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,,"ROUGE-L, Accuracy","Metrics are described, but are again about accuracy and f1-score, so not interesting for the study.",,,,,,,
26,2024,Studying llm performance on closed-and open-source data,"Large Language models (LLMs) are finding wide use in software engineering practice. These models are extremely data-hungry, and are largely trained on open-source (OSS) code distributed with permissive licenses. In terms of actual use however, a great deal of software development still occurs in the for-profit/proprietary sphere, where the code under development is not, and never has been, in the public domain; thus, many developers, do their work, and use LLMs, in settings where the models may not be as familiar with the code under development. In such settings, do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds? In this paper, we examine this issue using proprietary, closed-source software data from Microsoft, where most proprietary code is in C# and C++. We find that performance for C# changes little from OSS --> proprietary code, but does significantly reduce for C++; we find that this difference is attributable to differences in identifiers. We also find that some performance degradation, in some cases, can be ameliorated efficiently by in-context learning. ","GPT 3.x, LLMs, Generalization, LLM4SE","T Ahmed, C Bird, P Devanbu, S Chakraborty",,,,https://arxiv.org/abs/2402.15100,8,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"BLEU-CN, Meteor, ROUGE-L","Metric used, BLEU-CN, Meteor, ROUGE-L, (section 4.1.3)",,,,,,,
27,2024,Automatic semantic augmentation of language model prompts (for code summarization),"Large Language Models (LLM) are a new class of computation engines, ""programmed"" via prompt engineering. Researchers are still learning how to best ""program"" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.
One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of ""code analysis"" and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps.
Prior work shows that LLM performance on code summarization benefits from embedding a few code & summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization.
We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU1. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.","LLM, Code Summarization, Program Analysis, Prompt Engineering, LLM4SE ","T Ahmed, KS Pai, P Devanbu, E Barr",,,,https://dl.acm.org/doi/abs/10.1145/3597503.3639183,1,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"BLEU, ROUGE-L, Meteor","Uses ,BLEU, ROUGE-L, Meteor",,,,,,,
28,2024,Lost in translation: A study of bugs introduced by large language models while translating code,"Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation---with correct translations ranging from 2.1% to 47.3% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset---consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs---can help drive research in this area.","code translation, bug taxonomy, llm, LLM4SE","R Pan, AR Ibrahimzada, R Krishna, D Sankar‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3597503.3639226,4,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,3.1 describes/mentions metrics and why they didn't choose them. But I don't see them describing anything else. so its a maybe ,3.1 describes/mentions metrics and why they didn't choose them. But I don't see them describing anything else. so its a maybe ,,,,,,,
29,2024,Using LLMs in software requirements specifications: an empirical evaluation,"The creation of a Software Requirements Specification (SRS) document is important for any software development project. Given the recent prowess of Large Language Models (LLMs) in answering natural language queries and generating sophisticated textual outputs, our study explores their capability to produce accurate, coherent, and structured drafts of these documents to accelerate the software development lifecycle. We assess the performance of GPT-4 and CodeLlama in drafting an SRS for a university club management system and compare it against human benchmarks using eight distinct criteria. Our results suggest that LLMs can match the output quality of an entry-level software engineer to generate an SRS, delivering complete and consistent drafts. We also evaluate the capabilities of LLMs to identify and rectify problems in a given requirements document. Our experiments indicate that GPT-4 is capable of identifying issues and giving constructive feedback for rectifying them, while CodeLlama's results for validation were not as encouraging. We repeated the generation exercise for four distinct use cases to study the time saved by employing LLMs for SRS generation. The experiment demonstrates that LLMs may facilitate a significant reduction in development time for entry-level software engineers. Hence, we conclude that the LLMs can be gainfully used by software engineers to increase productivity by saving time and effort in generating, validating and rectifying software requirements.","Requirements engineering, software requirements specifications, empirical research, large language models, LLM4SE","M Krishna, B Gaur, A Verma‚Ä¶",,,,https://ieeexplore.ieee.org/abstract/document/10628461/,12,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Used human annotators,Used human annotators,,,,,,,
30,2025,Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution,"The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data. ",,"Y Wei, O Duchenne, J Copet, Q Carbonneaux‚Ä¶",,,,https://arxiv.org/abs/2502.18449,3,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
31,2023,Text-to-sql empowered by large language models: A benchmark evaluation,"Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications. ",LLM4SE,"D Gao, H Wang, Y Li, X Sun, Y Qian, B Ding‚Ä¶",,,,https://arxiv.org/abs/2308.15363,10,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"Exact Match, Execution Accuracy","Exact Match, Execution Accuracy",,,,,,,
32,2024,Core: Resolving code quality issues using llms,"As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues. We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The proposer LLM of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go un-detected by the static analysis. The ranker LLM evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer. We conduct a variety of experiments on two public benchmarks to show the ability of CORE: (1) to generate code revisions acceptable to both static analysis tools and human reviewers (the latter evaluated with user study on a subset of the Python benchmark), (2) to reduce human review efforts by detecting and eliminating revisions with unintended changes, (3) to readily work across multiple languages (Python and Java), static analysis tools (CodeQL and SonarQube) and quality checks (52 and 10 checks, respectively), and (4) to achieve fix rate comparable to a rule-based automated program repair tool but with much smaller engineering efforts (on the Java benchmark). CORE could revise 59.2% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM reduced false positives by 25.8% in these cases. CORE produced revisions that passed the static analysis tool in 76.8% Java files (across 10 quality checks) comparable to 78.3% of a specialized program repair tool, with significantly much less engineering efforts. We release code, data, and supplementary material publicly at http://aka.ms/COREMSRI.",• Software and its engineering → Software maintenance tools; Automatic programming. LLM4SE,"N Wadhwa, J Pradhan, A Sonwane, SP Sahu‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3643762,3,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,"There seems to be some mention of metrics but it is unclear, since it seems to rely on metrics from the benchmarks used without describing them properly.","There seems to be some mention of metrics but it is unclear, since it seems to rely on metrics from the benchmarks used without describing them properly.",,,,,,,
33,2023,Evaluating and explaining large language models for code using syntactic structures,"Large Language Models (LLMs) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial AI-based developer tools, such as GitHub CoPilot. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining LLMs for code are inextricably linked. That is, in order to explain a model's predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions.
To this end, this paper introduces ASTxplainer, an explainability method specific to LLMs for code that enables both new methods for LLM evaluation and visualizations of LLM predictions that aid end-users in understanding model predictions. At its core, ASTxplainer provides an automated method for aligning token predictions with AST nodes, by extracting and aggregating normalized model logits within AST structures. To demonstrate the practical benefit of ASTxplainer, we illustrate the insights that our framework can provide by performing an empirical evaluation on 12 popular LLMs for code using a curated dataset of the most popular GitHub projects. Additionally, we perform a user study examining the usefulness of an ASTxplainer-derived visualization of model predictions aimed at enabling model users to explain predictions. The results of these studies illustrate the potential for ASTxplainer to provide insights into LLM effectiveness, and aid end-users in understanding predictions. ","explainability, interpretability, large language
models, dl4se, LLM4SE","DN Palacio, A Velasco, D Rodriguez-Cardenas‚Ä¶",,,,https://arxiv.org/abs/2308.03873,0,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Extremely complex metrics that make it hard to understand.,Extremely complex metrics that make it hard to understand.,,,,,,,
34,2024,Generating domain models from natural language text using NLP: a benchmark dataset and experimental comparison of tools,"Software requirements specification describes users’ needs and expectations on some target system. Requirements documents are typically represented by unstructured natural language text. Such texts are the basis for the various subsequent activities in software development, such as software analysis and design. As part of software analysis, domain models are made that describe the key concepts and relations between them. Since the analysis process is performed manually by business analysts, it is time-consuming and may introduce mistakes. Recently, researchers have worked toward automating the synthesis of domain models from textual software requirements. Current studies on this topic have limitations in terms of the volume and heterogeneity of experimental datasets. To remedy this, we provide a curated dataset of software requirements to be utilized as a benchmark by algorithms that transform textual requirements documents into domain models. We present a detailed evaluation of two text-to-model approaches: one based on a large-language model (ChatGPT) and one building on grammatical rules (txt2Model). Our evaluation reveals that both tools yield promising results with relatively high F-scores for modeling the classes, attributes, methods, and relationships, with txt2Model performing better than ChatGPT on average. Both tools have relatively lower performance and high variance when it comes to the relation types. We believe our dataset and experimental evaluation pave to way to advance the field of automated model generation from requirements.",Software functional requirements · Software models · Text-to-model transformation · Benchmark dataset,"F Bozyigit, T Bardakci, A Khalilipour‚Ä¶",,,,https://link.springer.com/article/10.1007/s10270-024-01176-y,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
35,2023,Automated program repair in the era of large pre-trained language models,"Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed. In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.",LLM4SE,"CS Xia, Y Wei, L Zhang",,,,https://ieeexplore.ieee.org/abstract/document/10172803/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,"I believe they are doing human checks to see semantic similarity of a patch, which is not interesting for the LR","I believe they are doing human checks to see semantic similarity of a patch, which is not interesting for the LR",,,,,,,
36,2024,Codefuse-13b: A pretrained multi-lingual code large language model,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM 2. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high-quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",Software and its engineering,"P Di, J Li, H Yu, W Jiang, W Cai, Y Cao‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3639477.3639719,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
37,2022,Few-shot training llms for project-specific code-summarization,"Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.","deep learning, code summarization, large language model","T Ahmed, P Devanbu",,,,https://dl.acm.org/doi/abs/10.1145/3551349.3559555,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
38,2023,Codescope: An execution-based multilingual multitask multidimensional benchmark for evaluating llms on code understanding and generation,"Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers 43 programming languages and eight coding tasks. It evaluates the coding performance of LLMs from three dimensions (perspectives): length, difficulty, and efficiency. To facilitate execution-based evaluations of code generation, we develop MultiCodeEngine, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at this https URL. ",LLM4SE,"W Yan, H Liu, Y Wang, Y Li, Q Chen, W Wang‚Ä¶",,,,https://arxiv.org/abs/2311.08588,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"pass@k, BLEU, ROUGE, Meteor, Coverage Percentage","They use a bunch, pass@k, BLEU, ROUGE, Meteor, coverage percentage ",,,,,,,
39,2024,Scieval: A multi-level large language model evaluation benchmark for scientific research,"Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a ""dynamic"" subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions. The codes and data are publicly available on https://github.com/OpenDFM/SciEval. ",,"L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen‚Ä¶",,,,https://ojs.aaai.org/index.php/AAAI/article/view/29872,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
40,2024,Cyberseceval 2: A wide-ranging cybersecurity evaluation suite for large language models,"Large language models (LLMs) introduce new security risks, but there are few comprehensive evaluation suites to measure and reduce these risks. We present BenchmarkName, a novel benchmark to quantify LLM security risks and capabilities. We introduce two new areas for testing: prompt injection and code interpreter abuse. We evaluated multiple state-of-the-art (SOTA) LLMs, including GPT-4, Mistral, Meta Llama 3 70B-Instruct, and Code Llama. Our results show that conditioning away risk of attack remains an unsolved problem; for example, all tested models showed between 26% and 41% successful prompt injection tests. We further introduce the safety-utility tradeoff: conditioning an LLM to reject unsafe prompts can cause the LLM to falsely reject answering benign prompts, which lowers utility. We propose quantifying this tradeoff using False Refusal Rate (FRR). As an illustration, we introduce a novel test set to quantify FRR for cyberattack helpfulness risk. We find many LLMs able to successfully comply with ""borderline"" benign requests while still rejecting most unsafe requests. Finally, we quantify the utility of LLMs for automating a core cybersecurity task, that of exploiting software vulnerabilities. This is important because the offensive capabilities of LLMs are of intense interest; we quantify this by creating novel test sets for four representative problems. We find that models with coding capabilities perform better than those without, but that further work is needed for LLMs to become proficient at exploit generation. Our code is open source and can be used to evaluate other LLMs. ",LLM4SE,"M Bhatt, S Chennabasappa, Y Li, C Nikolaidis‚Ä¶",,,,https://arxiv.org/abs/2404.13161,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,There isn't a direct view of the metrics,There isn't a direct view of the metrics,,,,,,,
41,2024,Docbench: A benchmark for evaluating llm-based document reading systems,"Recently, there has been a growing interest among large language model (LLM) developers in LLM-based document reading systems, which enable users to upload their own documents and pose questions related to the document contents, going beyond simple reading comprehension tasks. Consequently, these systems have been carefully designed to tackle challenges such as file parsing, metadata extraction, multi-modal information understanding and long-context reading. However, no current benchmark exists to evaluate their performance in such scenarios, where a raw file and questions are provided as input, and a corresponding response is expected as output. In this paper, we introduce DocBench, a new benchmark designed to evaluate LLM-based document reading systems. Our benchmark involves a meticulously crafted process, including the recruitment of human annotators and the generation of synthetic questions. It includes 229 real documents and 1,102 questions, spanning across five different domains and four major types of questions. We evaluate both proprietary LLM-based systems accessible via web interfaces or APIs, and a parse-then-read pipeline employing open-source LLMs. Our evaluations reveal noticeable gaps between existing LLM-based document reading systems and human performance, underscoring the challenges of developing proficient systems. To summarize, DocBench aims to establish a standardized benchmark for evaluating LLM-based document reading systems under diverse real-world scenarios, thereby guiding future advancements in this research area. ",,"A Zou, W Yu, H Zhang, K Ma, D Cai, Z Zhang‚Ä¶",,,,https://arxiv.org/abs/2407.10701,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Uses accuracy,Uses accuracy,,,,,,,
42,2024,Inadequacies of large language model benchmarks in the era of generative artificial intelligence,"The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their own LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of benchmark functionality and integrity. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral profiling to accurately capture LLMs' complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems' integration into society. ","Artificial Intelligence (AI), AI Evaluation,
Benchmark, Evaluation Frameworks, Large Language Model
(LLM).","TR McIntosh, T Susnjak, N Arachchilage, T Liu‚Ä¶",,,,https://arxiv.org/abs/2402.09880,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,"This probably won't be included, but will be interesting to add in the limitations section","This probably won't be included, but will be interesting to add in the limitations section",,,,,,,
43,2024,Repository-level code translation benchmark targeting rust,"Recent advancements in large language models (LLMs) have demonstrated impressive capabilities in code translation, typically evaluated using benchmarks like CodeTransOcean. However, these benchmarks fail to capture real-world complexities by focusing primarily on simple function-level translations and overlooking repository-level context (e.g., dependencies). Moreover, LLMs' effectiveness in translating to newer, low-resource languages like Rust remains largely underexplored. To address this gap, we introduce RustRepoTrans, the first repository-level code translation benchmark, comprising 375 tasks translating into Rust from C++, Java, and Python. Using this benchmark, we evaluate four state-of-the-art LLMs, analyzing their errors to assess limitations in complex translation scenarios. Among them, Claude-3.5 performs best with 43.5% Pass@1, excelling in both basic functionality and additional translation abilities, such as noise robustness and syntactical difference identification. However, even Claude-3.5 experiences a 30.8% performance drop (Pass@1 from 74.3% to 43.5%) when handling repository-level context compared to previous benchmarks without such context. We also find that LLMs struggle with language differences in complex tasks, and dependencies further increase translation difficulty.",LLM4SE,"G Ou, M Liu, Y Chen, X Peng, Z Zheng",,,,https://arxiv.org/abs/2411.13990,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,DSR@k,DSR@k,,,,,,,
44,2025,Exploring automated assertion generation via large language models,"Unit testing aims to validate the correctness of software system units and has become an essential practice in software development and maintenance. However, it is incredibly time-consuming and labor-intensive for testing experts to write unit test cases manually, including test inputs (i.e., prefixes) and test oracles (i.e., assertions). Very recently, some techniques have been proposed to apply Large Language Models (LLMs) to generate unit assertions and have proven the potential in reducing manual testing efforts. However, there has been no systematic comparison of the effectiveness of these LLMs, and their pros and cons remain unexplored.
To bridge this gap, we perform the first extensive study on applying various LLMs to automated assertion generation. The experimental results on two independent datasets show that studied LLMs outperform six state-of-the-art techniques with a prediction accuracy of 51.82%–58.71% and 38.72%–48.19%. The improvements achieve 29.60% and 12.47% on average. Besides, as a representative LLM, CodeT5 consistently outperforms all studied LLMs and all baselines on both datasets, with an average improvement of 13.85% and 26.64%, respectively. We also explore the performance of generated assertions in detecting real-world bugs, and find LLMs are able to detect 32 bugs from Defects4J on average, with an improvement of 52.38% against the most recent approach EditAS. Inspired by the findings, we construct a simplistic retrieval-and-repair-enhanced LLM-based approach by transforming the assertion generation problem into a program repair task for retrieved similar assertions. Surprisingly, such a simplistic approach can further improve the prediction accuracy of LLMs by 9.40% on average, leading to new records on both datasets. Besides, we provide additional discussions from different aspects (e.g., the impact of assertion types and test lengths) to illustrate the capacity and limitations of LLM-based approaches. Finally, we further pinpoint various practical guidelines (e.g., the improvement of multiple candidate assertions) for advanced LLM-based assertion generation in the near future. Overall, our work underscores the promising future of adopting off-the-shelf LLMs to generate accurate and meaningful assertions in real-world test cases and reduce the manual efforts of unit testing experts in practical scenarios.","Software and its engineering → Software testing and debugging;Unit Testing, Assertion Generation, LLM, AI4SE, LLM4SE","Q Zhang, W Sun, C Fang, B Yu, H Li, M Yan‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3699598,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,BugFound,BugFound metric is mentioned ,,,,,,,
45,2025,Bugs in large language models generated code: An empirical study,"Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines samples of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using online surveys with over 50 LLM practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. This study sheds light on the distinctive characteristics of LLM-generated code.",Large language models · Bugs · Software testing · Empirical study,"F Tambon, A Moradi-Dakhel, A Nikanjam‚Ä¶",,,,https://link.springer.com/article/10.1007/s10664-025-10614-4,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
46,2024,CyberMetric: a benchmark dataset based on retrieval-augmented generation for evaluating LLMs in cybersecurity knowledge,"Large Language Models (LLMs) are increasingly used across various domains, from software development to cyber threat intelligence. Understanding all the different cybersecurity fields, including topics such as cryptography, reverse engineering, and risk assessment, poses a challenge even for human experts. The research community needs a diverse, accurate, and up-to-date dataset to test the general knowledge of LLMs in cybersecurity. To address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000, and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets comprising 80, 500, 2000, and 10,000 questions, respectively. By utilizing GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents, including NIST standards, research papers, publicly accessible books, RFCs, and other publications in the cybersecurity domain, to generate questions, each with four possible answers. The results underwent several rounds of error checking and refinement. Human experts invested over 200 hours validating the questions and solutions to ensure their accuracy and relevance and to filter out any questions unrelated to cybersecurity. We have evaluated and compared 25 state-of-the-art LLM models on the CyberMetric datasets. In addition to our primary goal of evaluating LLMs, we involved 30 human participants to solve CyberMetric-80 in a closed-book scenario. The results can serve as a reference for comparing the general cybersecurity knowledge of humans and LLMs. The findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct, Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs. Additionally, the top LLMs were more accurate than humans on CyberMetric-80, although highly experienced human experts still outperformed small models such as Llama-3-8B, Phi-2 or Gemma-7b. The CyberMetric dataset is publicly available for the research community and can be downloaded from the projects' website: https://github.com/CyberMetric.",LLM4SE,"N Tihanyi, MA Ferrag, R Jain, T Bisztray‚Ä¶",,,,https://ieeexplore.ieee.org/abstract/document/10679494/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Uses accuracy,Uses accuracy,,,,,,,
47,2024,Large Language Models' Expert-level Global History Knowledge Benchmark (HiST-LLM),"Large Language Models (LLMs) have the potential to transform humanities and social science research, yet their history knowledge and comprehension at a graduate level remains untested. Benchmarking LLMs in history is particularly challenging, given that human knowledge of history is inherently unbalanced, with more information available on Western history and recent periods. We introduce the History Seshat Test for LLMs (HiST-LLM), based on a subset of the Seshat Global History Databank, which provides a structured representation of human historical knowledge, containing 36,000 data points across 600 historical societies and over 2,700 scholarly references. This dataset covers every major world region from the Neolithic period to the Industrial Revolution and includes information reviewed and assembled by history experts and graduate research assistants. Using this dataset, we benchmark a total of seven models from the Gemini, OpenAI, and Llama families. We find that, in a four-choice format, LLMs have a balanced accuracy ranging from 33.6% (Llama-3.1-8B) to 46% (GPT-4-Turbo), outperforming random guessing (25%) but falling short of expert comprehension. LLMs perform better on earlier historical periods. Regionally, performance is more even but still better for the Americas and lowest in Oceania and Sub-Saharan Africa for the more advanced models. Our benchmark shows that while LLMs possess some expert-level historical knowledge, there is considerable room for improvement.",,"J Hauser, D Kondor, J Reddish‚Ä¶",,,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/38cc5cba8e513547b96bc326e25610dc-Abstract-Datasets_and_Benchmarks_Track.html,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
48,2023,Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization,"While large language models (LLMs) can already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluations of five LLM-based systems to assess their instruction-following capabilities in controllable summarization. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) no LLM-based evaluation methods can achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation capabilities. We make our collected benchmark InstruSum publicly available to facilitate future research in this direction. ",,"Y Liu, AR Fabbri, J Chen, Y Zhao, S Han, S Joty‚Ä¶",,,,https://arxiv.org/abs/2311.09184,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,This might still be interesting for later due to it mentioning LLM-as-a-Judge not being competent enough/does not align with human annotators,This might still be interesting for later due to it mentioning LLM-as-a-Judge not being competent enough/does not align with human annotators,,,,,,,
49,2025,From code to courtroom: Llms as the new software judges,"Recently, Large Language Models (LLMs) have been increasingly used to automate SE tasks such as code generation and summariza-
tion. However, evaluating the quality of LLM-generated software artifacts remains challenging. Human evaluation, while effective,
is very costly and time-consuming. Traditional automated metrics like BLEU rely on high-quality references and struggle to capture
nuanced aspects of software quality, such as readability and usefulness. In response, the LLM-as-a-Judge paradigm, which employs
LLMs for automated evaluation, has emerged. Given that LLMs possess strong coding abilities and reasoning skills, they hold promise
as cost-effective and scalable surrogates for human evaluators. Nevertheless, LLM-as-a-Judge research in the SE community is still in
its early stages, with many breakthroughs needed.
This forward-looking SE 2030 paper aims to steer the research community toward advancing LLM-as-a-Judge for evaluating LLM-
generated software artifacts, while also sharing potential research paths to achieve this goal. We provide a literature review of existing
SE studies on LLM-as-a-Judge and envision these frameworks as reliable, robust, and scalable human surrogates capable of evaluating
software artifacts with consistent, multi-faceted assessments by 2030 and beyond. To validate this vision, we analyze the limitations
of current studies, identify key research gaps, and outline a detailed roadmap to guide future developments of LLM-as-a-Judge in
software engineering. While not intended to be a definitive guide, our work aims to foster further research and adoption of LLM-as-
a-Judge frameworks within the SE community, ultimately improving the effectiveness and scalability of software artifact evaluation
methods.","Large Language Models, Software Engineering, LLM-as-a-Judge, Research Roadmap, LLM4SE","J He, J Shi, TY Zhuo, C Treude, J Sun, Z Xing‚Ä¶",,,,https://arxiv.org/abs/2503.02246,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,,LLM-as-Judge,"LLM-as-Judge, but is this a LR? Then I think it must be excluded",,,,,,,
50,2024,Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions,"Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks ranging from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing diverse function calls as tools to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding complex instructions. Fulfilling both of these characteristics can pose a great challenge for this http URL assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions only with essential information. Our extensive evaluation of 60 LLMs shows that LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%. The results underscore the need for further advancements in this area. ",LLM4SE,"TY Zhuo, MC Vu, J Chim, H Hu, W Yu‚Ä¶",,,,https://arxiv.org/abs/2406.15877,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Uses pass@k but it is not explained properly,Uses pass@k but it is not explained properly,,,,,,,
51,2024,Magis: Llm-based multi-agent framework for github issue resolution,"In software development, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code. Large Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level. To overcome this challenge, we empirically study the reason why LLMs fail to resolve GitHub issues and analyze the major factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, significantly outperforming the baselines.Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM.",LLM4SE,"W Tao, Y Zhou, Y Wang, W Zhang‚Ä¶",,,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/5d1f02132ef51602adf07000ca5b6138-Abstract-Conference.html,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
52,2023,Fully autonomous programming with large language models,"Current approaches to program synthesis with Large Language Models (LLMs) exhibit a ""near miss syndrome"": they tend to generate programs that semantically resemble the correct answer (as measured by text similarity metrics or human evaluation), but achieve a low or even zero accuracy as measured by unit tests due to small imperfections, such as the wrong input or output format. This calls for an approach known as Synthesize, Execute, Debug (SED), whereby a draft of the solution is generated first, followed by a program repair phase addressing the failed tests. To effectively apply this approach to instruction-driven LLMs, one needs to determine which prompts perform best as instructions for LLMs, as well as strike a balance between repairing unsuccessful programs and replacing them with newly generated ones. We explore these trade-offs empirically, comparing replace-focused, repair-focused, and hybrid debug strategies, as well as different template-based and model-based prompt-generation techniques. We use OpenAI Codex as the LLM and Program Synthesis Benchmark 2 as a database of problem descriptions and tests for evaluation. The resulting framework outperforms both conventional usage of Codex without the repair phase and traditional genetic programming approaches.","Software and its engineering → Software design engineering; •
Computing methodologies → Neural networks; Model develop-
ment and analysis; Search methodologies LLM4SE","V Liventsev, A Grishina, A H√§rm√§‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3583131.3590481,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,"Confusing metrics, not sure if I can include them","Confusing metrics, not sure if I can include them",,,,,,,
53,2024,Enhancing trust in llm-generated code summaries with calibrated confidence scores,"A good summary can often be very useful during program comprehension. While a brief, fluent, and relevant summary can be helpful, it does require significant human effort to produce. Often, good summaries are unavailable in software projects, thus making maintenance more difficult. There has been a considerable body of research into automated AI-based methods, using Large Language models (LLMs), to generate summaries of code; there also has been quite a bit work on ways to measure the performance of such summarization methods, with special attention paid to how closely these AI-generated summaries resemble a summary a human might have produced. Measures such as BERTScore and BLEU have been suggested and evaluated with human-subject studies.
However, LLM-produced summaries can be too long, irrelevant, etc: generally, too dissimilar to what a human might say. Given an LLM-produced code summary, how can we judge if a summary is good enough? Given some input source code, and an LLM-generated summary, existing approaches can help judge brevity, fluency and relevance; however, it's difficult to gauge whether an LLM-produced summary sufficiently resembles what a human might produce, without a ""golden"" human-produced summary to compare against. We study this resemblance question as a calibration problem: given just the summary from an LLM, can we compute a confidence measure, that provides a reliable indication of whether the summary sufficiently resembles what a human would have produced in this situation? We examine this question using several LLMs, for several languages, and in several different settings. Our investigation suggests approaches to provide reliable predictions of the likelihood that an LLM-generated summary would sufficiently resemble a summary a human might write for the same code. ","Software and its engineering → Software maintenance tools; Documentation;
Additional Key Words and Phrases: LLMs, Calibration, Code Summarization, LLM4SE","Y Virk, P Devanbu, T Ahmed",,,,https://arxiv.org/abs/2404.19318,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"BERTScore, Brier, Sucess rate, ECE, Skill score",Seems interesting but complicated,,,,,,,
54,2023,Evaluating explanations for software patches generated by large language models,"Large language models (LLMs) have recently been integrated in a variety of applications including software engineering tasks. In this work, we study the use of LLMs to enhance the explainability of software patches. In particular, we evaluate the performance of GPT 3.5 in explaining patches generated by the search-based automated program repair system ARJA-e for 30 bugs from the popular Defects4J benchmark. We also investigate the performance achieved when explaining the corresponding patches written by software developers. We find that on average 84% of the LLM explanations for machine-generated patches were correct and 54% were complete for the studied categories in at least 1 out of 3 runs. Furthermore, we find that the LLM generates more accurate explanations for machine-generated patches than for human-written ones.","Large Language Models · Software Patches · AI
Explainability · Program Repair · Genetic Improvement, LLM4SE","D Sobania, A Geiger, J Callan, A Brownlee‚Ä¶",,,,https://link.springer.com/chapter/10.1007/978-3-031-48796-5_12,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Manual assesment (human annotators),Manual assesment (human annotators),,,,,,,
55,2022,GPT2SP: A transformer-based agile story point estimation approach,"Story point estimation is a task to estimate the overall effort required to fully implement a product backlog item. Various estimation approaches (e.g., Planning Poker, Analogy, and expert judgment) are widely-used, yet they are still inaccurate and may be subjective, leading to ineffective sprint planning. Recent work proposed Deep-SE, a deep learning-based Agile story point estimation approach, yet it is still inaccurate, not transferable to other projects, and not interpretable. In this paper, we propose GPT2SP, a Transformer-based Agile Story Point Estimation approach. Our GPT2SP employs a GPT-2 pre-trained language model with a GPT-2 Transformer-based architecture, allowing our GPT2SP models to better capture the relationship among words while considering the context surrounding a given word and its position in the sequence and be transferable to other projects, while being interpretable. Through an extensive evaluation on 23,313 issues that span across 16 open-source software projects with 10 existing baseline approaches for within- and cross-project scenarios, our results show that our GPT2SP approach achieves a median MAE of 1.16, which is (1) 34%-57% more accurate than existing baseline approaches for within-project estimations; (2) 39%-49% more accurate than existing baseline approaches for cross-project estimations. The ablation study also shows that the GPT-2 architecture used in our approach substantially improves Deep-SE by 6%-47%, highlighting the significant advancement of the AI for Agile story point estimation. Finally, we develop a proof-of-concept tool to help practitioners better understand the most important words that contributed to the story point estimation of the given issue with the best supporting examples from past estimates. Our survey study with 16 Agile practitioners shows that the story point estimation task is perceived as an extremely challenging task. In addition, our AI-based story point estimation with explanations is perceived as more useful and trustworthy than without explanations, highlighting the practical need of our Explainable AI-based story point estimation approach.","Agile story point estimation, AI for SE, explainable AI, LLM4SE","M Fu, C Tantithamthavorn",,,,https://ieeexplore.ieee.org/abstract/document/9732669/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,MAE so statistical metric,MAE so statistical metric,,,,,,,
56,2024,Bias testing and mitigation in llm-based code generation,"As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social bias and unfairness, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models but are underexplored in the literature. This paper presents a novel bias testing framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive empirical study on the biases in code generated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison, Claude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal that biases are prevalent. For example, 13.47% to 49.10% of the codes generated by these LLMs have biased behaviors towards gender. Moreover, we study five bias mitigation prompt strategies that are commonly used in current code generation scenarios, i.e., zero-shot, one-shot, few-shot, and two Chain-of-Thought (CoT) prompts, with and without provided feedback-driven refinement. Our evaluation results illustrate that using direct prompt engineering strategies has limited effectiveness in mitigating bias, but our test execution feedback can help to reduce the ratio of code biases to a large extent (e.g., from 59.88% to 4.79% for GPT-4)1.","• Software and its engineering → Software creation and management; • Computing methodologies
→ Machine learning.
Additional Key Words and Phrases: Fairness testing, code generation","D Huang, JM Zhang, Q Bu, X Xie, J Chen‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3724117,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
57,2024,Collu-Bench: A Benchmark for Predicting Language Model Hallucinations in Code,"Despite their success, large language models (LLMs) face the critical challenge of hallucinations, generating plausible but incorrect content. While much research has focused on hallucinations in multiple modalities including images and natural language text, less attention has been given to hallucinations in source code, which leads to incorrect and vulnerable code that causes significant financial loss. To pave the way for research in LLMs' hallucinations in code, we introduce Collu-Bench, a benchmark for predicting code hallucinations of LLMs across code generation (CG) and automated program repair (APR) tasks. Collu-Bench includes 13,234 code hallucination instances collected from five datasets and 11 diverse LLMs, ranging from open-source models to commercial ones. To better understand and predict code hallucinations, Collu-Bench provides detailed features such as the per-step log probabilities of LLMs' output, token types, and the execution feedback of LLMs' generated code for in-depth analysis. In addition, we conduct experiments to predict hallucination on Collu-Bench, using both traditional machine learning techniques and neural networks, which achieves 22.03 -- 33.15% accuracy. Our experiments draw insightful findings of code hallucination patterns, reveal the challenge of accurately localizing LLMs' hallucinations, and highlight the need for more sophisticated techniques. ",LLM4SE,"N Jiang, Q Li, L Tan, T Zhang",,,,https://arxiv.org/abs/2410.09997,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Does not describe metrics,Does not describe metrics,,,,,,,
58,2024,Effibench: Benchmarking the efficiency of automatically generated code,"Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in greencomputing and sustainability efforts — the efficiency of the generated code — has often been neglected. This paper presents Effibench, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EffiBench, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) to generate efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average \textbf{3.12} times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 code are \textbf{13.89} and \textbf{43.92} times that of the canonical solutions. The source code of EffiBench is released on https://github.com/huangd1999/EffiBench. We also provide the LeaderBoard in https://huggingface.co/spaces/EffiBench/effibench-leaderboard.",LLM4SE,"D Huang, Y Qing, W Shang, H Cui‚Ä¶",,,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/15807b6e09d691fe5e96cdecde6d7b80-Abstract-Datasets_and_Benchmarks_Track.html,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"pass@k, NTMU, ET, NET, MU, TMU","Describes interesting metrics but idk if I want to cover them, efficiency computational cost etc.",,,,,,,
59,2023,Benchmarking large language model capabilities for conditional generation,"Pre-trained large language models (PLMs) underlie most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside techniques like few-shot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks--while they can be used to compare systems at a high level--relate to the real world use cases for which people have been adopting them. In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input and output language. Our results show that PLMs differ in their applicability to different data regimes and their generalization to multiple languages and inform which PLMs to use for a given generation task setup. We share best practices to be taken into consideration when benchmarking generation capabilities during the development of upcoming PLMs. ",LLM4SE,"J Maynez, P Agrawal, S Gehrmann",,,,https://arxiv.org/abs/2306.16793,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
60,2024,Large language models for test-free fault localization,"Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3%--54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.","Software and its engineering → Software functional prop-
erties; • Computing methodologies → Neural networks. LLM4SE","AZH Yang, C Le Goues, R Martins‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3597503.3623342,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
61,2024,A quantitative and qualitative evaluation of LLM-based explainable fault localization,"Fault Localization (FL), in which a developer seeks to identify which part of the code is malfunctioning and needs to be fixed, is a recurring challenge in debugging. To reduce developer burden, many automated FL techniques have been proposed. However, prior work has noted that existing techniques fail to provide rationales for the suggested locations, hindering developer adoption of these techniques. With this in mind, we propose AutoFL, a Large Language Model (LLM)-based FL technique that generates an explanation of the bug along with a suggested fault location. AutoFL prompts an LLM to use function calls to navigate a repository, so that it can effectively localize faults over a large software repository and overcome the limit of the LLM context length. Extensive experiments on 798 real-world bugs in Java and Python reveal AutoFL improves method-level acc@1 by up to 233.3% over baselines. Furthermore, developers were interviewed on their impression of AutoFL-generated explanations, showing that developers generally liked the natural language explanations of AutoFL, and that they preferred reading a few, high-quality explanations instead of many.","• Software and its engineering → Software testing and debugging; Software defect
analysis.
Additional Key Words and Phrases: language models, fault localization, debugging. LLM4SE","S Kang, G An, S Yoo",,,,https://dl.acm.org/doi/abs/10.1145/3660771,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,,"acc@k, precision@k, Reciprocal Rank (RR), Average Precision (AP)",acc@k,,,,,,,
62,2024,Diversity empowers intelligence: Integrating expertise of software engineering agents,"Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems. The most advanced open-source SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite. However, these sophisticated agent frameworks exhibit varying strengths, excelling in certain tasks while underperforming in others. To fully harness the diversity of these agents, we propose DEI (Diversity Empowered Intelligence), a framework that leverages their unique expertise. DEI functions as a meta-module atop existing SWE agent frameworks, managing agent collectives for enhanced problem-solving. Experimental results show that a DEI-guided committee of agents is able to surpass the best individual agent's performance by a large margin. For instance, a group of open-source SWE agents, with a maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3% resolve rate with DEI, making a 25% improvement and beating most closed-source solutions. Our best-performing group excels with a 55% resolve rate, securing the highest ranking on SWE-Bench Lite. Our findings contribute to the growing body of research on collaborative AI systems and their potential to solve complex software engineering challenges.",LLM4SE,"K Zhang, W Yao, Z Liu, Y Feng, Z Liu, R RN‚Ä¶",,,,https://openreview.net/forum?id=cKlzKs3Nnb,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
63,2023,Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation,"In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. Third, we find that generating the entire class all at once (i.e. holistic generation strategy) is the best generation strategy only for GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and compositional) is better strategies for the other models with limited ability of understanding long instructions and utilizing the middle information. Lastly, we find the limited model ability of generating method-dependent code and discuss the frequent error types in generated classes. Our benchmark is available at this https URL. ","Class-level Code Generation, Large Language Model, Benchmark, LLM4SE","X Du, M Liu, K Wang, H Wang, J Liu, Y Chen‚Ä¶",,,,https://arxiv.org/abs/2308.01861,,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,FALSE,,duplicate,duplicate,,,,,,,
64,2024,Large language models as software components: A taxonomy for llm-integrated applications,"Large Language Models (LLMs) have become widely adopted recently. Research explores their use both as autonomous agents and as tools for software engineering. LLM-integrated applications, on the other hand, are software systems that leverage an LLM to perform tasks that would otherwise be impossible or require significant coding effort. While LLM-integrated application engineering is emerging as new discipline, its terminology, concepts and methods need to be established. This study provides a taxonomy for LLM-integrated applications, offering a framework for analyzing and describing these systems. It also demonstrates various ways to utilize LLMs in applications, as well as options for implementing such integrations.
Following established methods, we analyze a sample of recent LLM-integrated applications to identify relevant dimensions. We evaluate the taxonomy by applying it to additional cases. This review shows that applications integrate LLMs in numerous ways for various purposes. Frequently, they comprise multiple LLM integrations, which we term ``LLM components''. To gain a clear understanding of an application's architecture, we examine each LLM component separately. We identify thirteen dimensions along which to characterize an LLM component, including the LLM skills leveraged, the format of the output, and more. LLM-integrated applications are described as combinations of their LLM components. We suggest a concise representation using feature vectors for visualization.
The taxonomy is effective for describing LLM-integrated applications. It can contribute to theory building in the nascent field of LLM-integrated application engineering and aid in developing such systems. Researchers and practitioners explore numerous creative ways to leverage LLMs in applications. Though challenges persist, integrating LLMs may revolutionize the way software systems are built. ","large language model, LLM-integrated, taxonomy, copilot, architecture, AI agent, LLM
component, LLM4SE",I Weber,,,,https://arxiv.org/abs/2406.10300,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
65,2023,"Better patching using llm prompting, via self-consistency","Enterprise applications of Large Language Models (LLMs) hold promise for question answering on enterprise SQL databases. However, the extent to which LLMs can accurately respond to enterprise questions in such databases remains unclear, given the absence of suitable Text-to-SQL benchmarks tailored to enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to enhance LLM-based question answering by providing business context is not well understood. This study aims to evaluate the accuracy of LLM-powered question answering systems in the context of enterprise questions and SQL databases, while also exploring the role of knowledge graphs in improving accuracy. To achieve this, we introduce a benchmark comprising an enterprise SQL schema in the insurance domain, a range of enterprise queries encompassing reporting to metrics, and a contextual layer incorporating an ontology and mappings that define a knowledge graph. Our primary finding reveals that question answering using GPT-4, with zero-shot prompts directly on SQL databases, achieves an accuracy of 16%. Notably, this accuracy increases to 54% when questions are posed over a Knowledge Graph representation of the enterprise SQL database. Therefore, investing in Knowledge Graph provides higher accuracy for LLM powered question answering systems.",LLM4SE,"T Ahmed, P Devanbu",,,,https://ieeexplore.ieee.org/abstract/document/10298561/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,No describtion of metric,No describtion of metric,,,,,,,
66,2024,A benchmark to understand the role of knowledge graphs on large language model's accuracy for question answering on enterprise SQL databases,"Enterprise applications of Large Language Models (LLMs) hold promise for question answering on enterprise SQL databases. However, the extent to which LLMs can accurately respond to enterprise questions in such databases remains unclear, given the absence of suitable Text-to-SQL benchmarks tailored to enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to enhance LLM-based question answering by providing business context is not well understood. This study aims to evaluate the accuracy of LLM-powered question answering systems in the context of enterprise questions and SQL databases, while also exploring the role of knowledge graphs in improving accuracy. To achieve this, we introduce a benchmark comprising an enterprise SQL schema in the insurance domain, a range of enterprise queries encompassing reporting to metrics, and a contextual layer incorporating an ontology and mappings that define a knowledge graph. Our primary finding reveals that question answering using GPT-4, with zero-shot prompts directly on SQL databases, achieves an accuracy of 16%. Notably, this accuracy increases to 54% when questions are posed over a Knowledge Graph representation of the enterprise SQL database. Therefore, investing in Knowledge Graph provides higher accuracy for LLM powered question answering systems.",LLM4SE,"J Sequeda, D Allemang, B Jacob",,,,https://dl.acm.org/doi/abs/10.1145/3661304.3661901,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,No describtion of metric,No describtion of metric,,,,,,,
67,2025,On the effectiveness of large language models in domain-specific code generation,"Large language models (LLMs) such as ChatGPT have shown remarkable capabilities in code generation. Despite significant achievements, they rely on enormous training data to acquire a broad spectrum of open-domain knowledge. Besides, their evaluation revolves around open-domain benchmarks like HumanEval, which primarily consist of programming contests. Therefore, it is hard to fully characterize the intricacies and challenges associated with particular domains (e.g., Web, game, and math). In this article, we conduct an in-depth study of the LLMs in domain-specific code generation. Our results demonstrate that LLMs exhibit sub-optimal performance in generating domain-specific code, due to their limited proficiency in utilizing domain-specific libraries. We further observe that incorporating API knowledge as prompts can empower LLMs to generate more professional code. Based on these findings, we further investigate how to effectively incorporate API knowledge into the code generation process. We experiment with three strategies for incorporating domain knowledge, namely, external knowledge inquirer, chain-of-thought prompting, and chain-of-thought fine-tuning. We refer to these strategies as a new code generation approach called DomCoder. Experimental results show that all strategies of DomCoder improve the effectiveness of domain-specific code generation under certain settings.","Software and its engineering → Automatic programming;
Additional Key Words and Phrases: large language models, code generation, domain-specific program
generation. LLM4SE","X Gu, M Chen, Y Lin, Y Hu, H Zhang, C Wan‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3697012,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"BLEU, CodeBLEU",BLEU and CodeBLEU,,,,,,,
68,2024,Cloudeval-yaml: A practical benchmark for cloud configuration generation,"Among the thriving ecosystem of cloud computing and the proliferation of Large Language Model (LLM)-based code generation tools, there is a lack of benchmarking for code generation in cloud-native applications. In response to this need, we present CloudEval-YAML, a practical benchmark for cloud configuration generation. CloudEval-YAML tackles the diversity challenge by focusing on YAML, the de facto standard of numerous cloud-native tools. We develop the CloudEval-YAML benchmark with practicality in mind: the dataset consists of hand-written problems with unit tests targeting practical scenarios. We further enhanced the dataset to meet practical needs by rephrasing questions in a concise, abbreviated, and bilingual manner. The dataset consists of 1011 problems that take more than 1200 human hours to complete. To improve practicality during evaluation, we build a scalable evaluation platform for CloudEval-YAML that achieves a 20 times speedup over a single machine. To the best of our knowledge, the CloudEval-YAML dataset is the first hand-written dataset targeting cloud-native applications. We present an in-depth evaluation of 12 LLMs, leading to a deeper understanding of the problems and LLMs, as well as effective methods to improve task performance and reduce cost.",LLM4SE,"Y Xu, Y Chen, X Zhang, X Lin, P Hu‚Ä¶",,,,https://proceedings.mlsys.org/paper_files/paper/2024/hash/554e056fe2b6d9fd27ffcd3367ae1267-Abstract-Conference.html,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"BLEU, Edit Distance, Exact Match, Key-Value Exact Match, Key-Value Wildcard Match, Unit Test",BLEU and a bunch of others,,,,,,,
69,2024,On evaluating the efficiency of source code generated by llms,"Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming. First, we evaluate the efficiency of the code generated by LLMs on two benchmarks, HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several prompts that would enable LLMs to generate more efficient code.",LLM4SE,"C Niu, T Zhang, C Li, B Luo, V Ng",,,,https://dl.acm.org/doi/abs/10.1145/3650105.3652295,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,"pass@k, LeetCodeEval beats score",,,,,,,
70,2024,Chatunitest: A framework for llm-based test generation,"Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests. Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage. Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the software testing domain. ChatUniTest is available at https://github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at https://www.youtube.com/watch?v=GmfxQUqm2ZQ.","Large Language Models, Automatic Unit Testing Generation, LLM4SE","Y Chen, Z Hu, C Zhi, J Han, S Deng, J Yin",,,,https://dl.acm.org/doi/abs/10.1145/3663529.3663801,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Might be interesting for thesis,Might be interesting for thesis,,,,,,,
71,2023,Competition-level problems are effective llm evaluators,"Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently. This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills. We first provide a comprehensive evaluation of GPT-4's peiceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered. Surprisingly, the peiceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems. We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges. Through our work, we emphasis the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future. ",LLM4SE,"Y Huang, Z Lin, X Liu, Y Gong, S Lu, F Lei‚Ä¶",,,,https://arxiv.org/abs/2312.02143,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"pass@k, ACC","pass@k, ACC",,,,,,,
72,2024,Livecodebench: Holistic and contamination free evaluation of large language models for code,"Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model ",LLM4SE,"N Jain, K Han, A Gu, WD Li, F Yan, T Zhang‚Ä¶",,,,https://arxiv.org/abs/2403.07974,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,pass@k,,,,,,,
73,2024,Agentharm: A benchmark for measuring harmfulness of llm agents,"The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at this https URL. ",,"M Andriushchenko, A Souly, M Dziemian‚Ä¶",,,,https://arxiv.org/abs/2410.09024,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
74,2023,Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation,"Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus – a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.
",LLM4SE,"J Liu, CS Xia, Y Wang, L Zhang",,,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,pass@k,,,,,,,
75,2023,Impact of large language models on generating software specifications,"Software specifications are essential for many Software Engineering (SE) tasks such as bug detection and test generation. Many existing approaches are proposed to extract the specifications defined in natural language form (e.g., comments) into formal machine readable form (e.g., first order logic). However, existing approaches suffer from limited generalizability and require manual efforts. The recent emergence of Large Language Models (LLMs), which have been successfully applied to numerous SE tasks, offers a promising avenue for automating this process. In this paper, we conduct the first empirical study to evaluate the capabilities of LLMs for generating software specifications from software comments or documentation. We evaluate LLMs performance with Few Shot Learning (FSL) and compare the performance of 13 state of the art LLMs with traditional approaches on three public datasets. In addition, we conduct a comparative diagnosis of the failure cases from both LLMs and traditional methods, identifying their unique strengths and weaknesses. Our study offers valuable insights for future research to improve specification generation. ","software specifications, large language models,
few-shot learning, LLM4SE","D Xie, B Yoo, N Jiang, M Kim, L Tan, X Zhang‚Ä¶",,,,https://arxiv.org/abs/2306.03324,,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,"recall, precision, F1-score","recall, precision, F1-score",,,,,,,
76,2024,Understanding the weakness of large language model agents within a complex android environment,"Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e. understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, prompt, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena.","Large Language Model; AI Agent; Task Planning, LLM4SE","M Xing, R Zhang, H Xue, Q Chen, F Yang‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3637528.3671650,,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,It describes metrics for agents and not for general LLM usage,It describes metrics for agents and not for general LLM usage,,,,,,,
77,2024,Experimenting a new programming practice with llms,"The recent development on large language models makes automatically constructing small programs possible. It thus has the potential to free software engineers from low-level coding and allow us to focus on the perhaps more interesting parts of software development, such as requirement engineering and system testing. In this project, we develop a prototype named AISD (AI-aided Software Development), which is capable of taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation. Different from existing attempts, AISD is designed to keep the user in the loop, i.e., by repeatedly taking user feedback on use cases, high-level system designs, and prototype implementations through system testing. AISD has been evaluated with a novel benchmark of non-trivial software projects. The experimental results suggest that it might be possible to imagine a future where software engineering is reduced to requirement engineering and system testing only. ","Requirement engineering, system testing, large language model,
code generation, LLM4SE","S Zhang, J Wang, G Dong, J Sun, Y Zhang‚Ä¶",,,,https://arxiv.org/abs/2401.01062,,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Statistical,Statistical,,,,,,,
78,2023,Owl: A large language model for it operations,"With the rapid development of IT operations, it has become increasingly crucial to efficiently manage and analyze large volumes of data for practical applications. The techniques of Natural Language Processing (NLP) have shown remarkable capabilities for various tasks, including named entity recognition, machine translation and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various NLP downstream tasks. However, there is a lack of specialized LLMs for IT operations. In this paper, we introduce the OWL, a large language model trained on our collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks. Furthermore, we evaluate the performance of our OWL on the OWL-Bench established by us and open IT-related benchmarks. OWL demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs. ",LLM4SE,"H Guo, J Yang, J Liu, L Yang, L Chai, J Bai‚Ä¶",,,,https://arxiv.org/abs/2309.09298,,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,Statistical,Statistical,,,,,,,
79,2024,Greening large language models of code,"Large language models of code have shown remarkable effectiveness across various software engineering tasks. Despite the availability of many cloud services built upon these powerful models, there remain several scenarios where developers cannot take full advantage of them, stemming from factors such as restricted or unreliable internet access, institutional privacy policies that prohibit external transmission of code to third-party vendors, and more. Therefore, developing a compact, efficient, and yet energy-saving model for deployment on developers' devices becomes essential.
To this aim, we propose Avatar, a novel approach that crafts a deployable model from a large language model of code by optimizing it in terms of model size, inference latency, energy consumption, and carbon footprint while maintaining a comparable level of effectiveness (e.g., prediction accuracy on downstream tasks). The key idea of Avatar is to formulate the optimization of language models as a multi-objective configuration tuning problem and solve it with the help of a Satisfiability Modulo Theories (SMT) solver and a tailored optimization algorithm. The SMT solver is used to form an appropriate configuration space, while the optimization algorithm identifies the Pareto-optimal set of configurations for training the optimized models using knowledge distillation. We evaluate Avatar with two popular language models of code, i.e., CodeBERT and GraphCodeBERT, on two popular tasks, i.e., vulnerability prediction and clone detection. We use Avatar to produce optimized models with a small size (3 MB), which is 160× smaller than the original large models. On the two tasks, the optimized models significantly reduce the energy consumption (up to 184× less), carbon footprint (up to 157× less), and inference latency (up to 76× faster), with only a negligible loss in effectiveness (1.67%).","Language Models of Code, Configuration Tuning, Multi-Objective
Optimization, LLM4SE","J Shi, Z Yang, HJ Kang, B Xu, J He, D Lo",,,,https://dl.acm.org/doi/abs/10.1145/3639475.3640097,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
80,2023,On the evaluation of neural code translation: Taxonomy and benchmark,"In recent years, neural code translation has gained increasing attention. While most of the research focuses on improving model architectures and training processes, we notice that the evaluation process and benchmark for code translation models are severely limited: they primarily treat source code as natural languages and provide a holistic accuracy score while disregarding the full spectrum of model capabilities across different translation types and complexity. In this paper, we present a comprehensive investigation of four state-of-the-art models and analyze in-depth the advantages and limitations of three existing benchmarks. Based on the empirical results, we develop a taxonomy that categorizes code translation tasks into four primary types according to their complexity and knowledge dependence: token level (type 1), syntactic level (type 2), library level (type 3), and algorithm level (type 4). We then conduct a thorough analysis of how existing approaches perform across these four categories. Our findings indicate that while state-of-the-art code translation models excel in type-1 and type-2 translations, they struggle with knowledge-dependent ones such as type-3 and type-4. Existing benchmarks are biased towards trivial translations, such as keyword mapping. To overcome these limitations, we construct G-TransEval, a new benchmark by manually curating type-3 and type-4 translation pairs and unit test cases. Results on our new benchmark suggest that G-TransEval can exhibit more comprehensive and finer-grained capability of code translation models and thus provide a more rigorous evaluation. Our studies also provide more insightful findings and suggestions for future research, such as building type-3 and type-4 training data and ensembling multiple pretraining approaches.","Code Translation, Empirical Study, Benchmark,
Evaluation, LLM4SE","M Jiao, T Yu, X Li, G Qiu, X Gu‚Ä¶",,,,https://ieeexplore.ieee.org/abstract/document/10298408/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
81,2024,Debugbench: Evaluating debugging capability of large language models,"Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging. ",LLM4SE,"R Tian, Y Ye, Y Qin, X Cong, Y Lin, Y Pan, Y Wu‚Ä¶",,,,https://arxiv.org/abs/2401.04621,,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,"Pass rate, human evaluators","Pass rate, human evaluators",,,,,,,
82,2024,Automatically inspecting thousands of static bug warnings with large language model: How far are we?,"Static analysis tools for capturing bugs and vulnerabilities in software programs are widely employed in practice, as they have the unique advantages of high coverage and independence from the execution environment. However, existing tools for analyzing large codebases often produce a great deal of false warnings over genuine bug reports. As a result, developers are required to manually inspect and confirm each warning, a challenging, time-consuming, and automation-essential task.
This article advocates a fast, general, and easily extensible approach called Llm4sa that automatically inspects a sheer volume of static warnings by harnessing (some of) the powers of Large Language Models (LLMs). Our key insight is that LLMs have advanced program understanding capabilities, enabling them to effectively act as human experts in conducting manual inspections on bug warnings with their relevant code snippets. In this spirit, we propose a static analysis to effectively extract the relevant code snippets via program dependence traversal guided by the bug warning reports themselves. Then, by formulating customized questions that are enriched with domain knowledge and representative cases to query LLMs, Llm4sa can remove a great deal of false warnings and facilitate bug discovery significantly. Our experiments demonstrate that Llm4sa is practical in automatically inspecting thousands of static warnings from Juliet benchmark programs and 11 real-world C/C++ projects, showcasing a high precision (81.13%) and a recall rate (94.64%) for a total of 9,547 bug warnings. Our research introduces new opportunities and methodologies for using the LLMs to reduce human labor costs, improve the precision of static analyzers, and ensure software trustworthiness","• Software and its engineering; • Security and privacy → Software and application
security;
Additional Key Words and Phrases: Large language model, static analysis, AI for program analysis, static bug
warning, false alarms, LLM4SE","C Wen, Y Cai, B Zhang, J Su, Z Xu, D Liu‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3653718,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
83,2024,"Risk taxonomy, mitigation, and assessment benchmarks of large language model systems","Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems. ","Large Language Model Systems, Safety, Security, Risk Taxonomy. LLM4SE","T Cui, Y Wang, C Fu, Y Xiao, S Li, X Deng, Y Liu‚Ä¶",,,,https://arxiv.org/abs/2401.05778,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
84,2024,"Llms cannot reliably identify and reason about security vulnerabilities (yet?): A comprehensive evaluation, framework, and benchmarks","Large Language Models (LLMs) have been suggested for use in automated vulnerability repair, but benchmarks showing they can consistently identify security-related bugs are lacking. We thus develop SecLLMHolmes, a fully automated evaluation framework that performs the most detailed investigation to date on whether LLMs can reliably identify and reason about security-related bugs. We construct a set of 228 code scenarios and analyze eight of the most capable LLMs across eight different investigative dimensions using our framework. Our evaluation shows LLMs provide non-deterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios. Most importantly, our findings reveal significant non-robustness in even the most advanced models like ‘PaLM2’ and ‘GPT-4’: by merely changing function or variable names, or by the addition of library functions in the source code, these models can yield incorrect answers in 26% and 17% of cases, respectively. These findings demonstrate that further LLM advances are needed before LLMs can be used as general purpose security assistants.",LLM4SE,"S Ullah, M Han, S Pujar, H Pearce‚Ä¶",,,,https://ieeexplore.ieee.org/abstract/document/10646663/,,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,Statistical,Statistical,,,,,,,
85,2023,Llmrec: Benchmarking large language models on recommendation task,"Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at this https URL. ",,"J Liu, C Liu, P Zhou, Q Ye, D Chong, K Zhou‚Ä¶",,,,https://arxiv.org/abs/2308.12241,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"BLEU, ROUGE-1, ROUGE-2, ROUGE-L",This just doesn't make sense to include since it mentioned recommendation tasks. But we will only include the fact that it uses BLEU and ROUGE,,,,,,,
86,2024,"Toolsandbox: A stateful, conversational, interactive evaluation benchmark for llm tool use capabilities","Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities. ToolSandbox evaluation framework is released at this https URL",LLM4SE,"J Lu, T Holleis, Y Zhang, B Aumayer, F Nan‚Ä¶",,,,https://arxiv.org/abs/2408.04682,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
87,2024,Outside the comfort zone: Analysing llm capabilities in software vulnerability detection,"The significant increase in software production driven by automation and faster development lifecycles has resulted in a corresponding surge in software vulnerabilities. In parallel, the evolving landscape of software vulnerability detection, highlighting the shift from traditional methods to machine learning and large language models (LLMs), provides massive opportunities at the cost of resource-demanding computations. This paper thoroughly analyses LLMs’ capabilities in detecting vulnerabilities within source code by testing models beyond their usual applications to study their potential in cybersecurity tasks. We evaluate the performance of six open-source models that are specifically trained for vulnerability detection against six general-purpose LLMs, three of which were further fine-tuned on a dataset that we compiled. Our dataset, alongside five state-of-the-art benchmark datasets, were used to create a pipeline to leverage a binary classification task, namely classifying code into vulnerable and non-vulnerable. The findings highlight significant variations in classification accuracy across benchmarks, revealing the critical influence of fine-tuning in enhancing the detection capabilities of small LLMs over their larger counterparts, yet only in the specific scenarios in which they were trained. Further experiments and analysis also underscore the issues with current benchmark datasets, particularly around mislabeling and their impact on model training and performance, which raises concerns about the current state of practice. We also discuss the road ahead in the field suggesting strategies for improved model training and dataset curation.","Software vulnerability detection · Source code analysis ·
Large language models · Cybersecurity, LLM4SE","Y Guo, C Patsakis, Q Hu, Q Tang, F Casino",,,,https://link.springer.com/chapter/10.1007/978-3-031-70879-4_14,,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,Statistical,Statistical,,,,,,,
88,2024,Llm maybe longlm: Self-extend llm context window without tuning,"It is well known that LLMs cannot generalize well to long contexts whose lengths are larger than the training sequence length. This poses challenges when employing LLMs for processing long input sequences during inference. In this work, we argue that LLMs themselves have inherent capabilities to handle long contexts without fine-tuning. To achieve this goal, we propose SelfExtend to extend the context window of LLMs by constructing bi-level attention information: the grouped attention and the neighbor attention. The grouped attention captures the dependencies among tokens that are far apart, while neighbor attention captures dependencies among adjacent tokens within a specified range. The two-level attentions are computed based on the original model's self-attention mechanism during inference. With minor code modification, our SelfExtend can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments on multiple benchmarks and the results show that our SelfExtend can effectively extend existing LLMs' context window length. The code can be found at \url{this https URL}. ",,"H Jin, X Han, J Yang, Z Jiang, Z Liu, CY Chang‚Ä¶",,,,https://arxiv.org/abs/2401.01325,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,No mention of metrics,No mention of metrics,,,,,,,
89,2025,Chineseecomqa: A scalable e-commerce concept evaluation benchmark for large language models,"With the increasing use of Large Language Models (LLMs) in fields such as e-commerce, domain-specific concept evaluation benchmarks are crucial for assessing their domain capabilities. Existing LLMs may generate factually incorrect information within the complex e-commerce applications. Therefore, it is necessary to build an e-commerce concept benchmark. Existing benchmarks encounter two primary challenges: (1) handle the heterogeneous and diverse nature of tasks, (2) distinguish between generality and specificity within the e-commerce field. To address these problems, we propose \textbf{ChineseEcomQA}, a scalable question-answering benchmark focused on fundamental e-commerce concepts. ChineseEcomQA is built on three core characteristics: \textbf{Focus on Fundamental Concept}, \textbf{E-commerce Generality} and \textbf{E-commerce Expertise}. Fundamental concepts are designed to be applicable across a diverse array of e-commerce tasks, thus addressing the challenge of heterogeneity and diversity. Additionally, by carefully balancing generality and specificity, ChineseEcomQA effectively differentiates between broad e-commerce concepts, allowing for precise validation of domain capabilities. We achieve this through a scalable benchmark construction process that combines LLM validation, Retrieval-Augmented Generation (RAG) validation, and rigorous manual annotation. Based on ChineseEcomQA, we conduct extensive evaluations on mainstream LLMs and provide some valuable insights. We hope that ChineseEcomQA could guide future domain-specific evaluations, and facilitate broader LLM adoption in e-commerce applications. ","Large Language Models, E-commerce, Benchmark","H Chen, K Lv, C Hu, Y Li, Y Yuan, Y He‚Ä¶",,,,https://arxiv.org/abs/2502.20196,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
90,2023,Large language model-aware in-context learning for code generation,"Large Language Models (LLMs) have shown impressive In-Context Learning (ICL) ability in code generation. LLMs take a prompt context consisting of a few demonstration examples and a new requirement as input, and output new programs without any parameter update. Existing studies have found that the performance of ICL-based code generation heavily depends on the quality of demonstration examples and thus arises research on selecting demonstration examples: given a new requirement, a few demonstration examples are selected from a candidate pool, where LLMs are expected to learn the pattern hidden in these selected demonstration examples. Existing approaches are mostly based on heuristics or randomly selecting examples. However, the distribution of randomly selected examples usually varies greatly, making the performance of LLMs less robust. The heuristics retrieve examples by only considering textual similarities of requirements, leading to sub-optimal performance.
To fill this gap, we propose a Large language model-Aware selection approach for In-context-Learning-based code generation named LAIL. LAIL uses LLMs themselves to select examples. It requires LLMs themselves to label a candidate example as a positive example or a negative example for a requirement. Positive examples are helpful for LLMs to generate correct programs, while negative examples are trivial and should be ignored. Based on the labeled positive and negative data, LAIL trains a model-aware retriever to learn the preference of LLMs and select demonstration examples that LLMs need. During the inference, given a new requirement, LAIL uses the trained retriever to select a few examples and feed them into LLMs to generate desired programs. We apply LAIL to four widely used LLMs and evaluate it on five code generation datasets. Extensive experiments demonstrate that LAIL outperforms the state-of-the-art (SOTA) baselines by 11.58%, 3.33%, and 5.07% on CodeGen-Multi-16B, 1.32%, 2.29%, and 1.20% on CodeLlama-34B, and achieves 4.38%, 2.85%, and 2.74% improvements on Text-davinci-003 in terms of Pass@1 at MBJP, MBPP, and MBCPP, respectively. In addition to function-level code generation, LAIL improves the performance of LLMs on DevEval, a repository-level code generation dataset, which achieves 10.04%, 8.12%, and 4.63% improvements compared to the SOTA baselines at Pass@1, 3, and 5 on CodeLlama-7B. Human evaluation further verifies that the generated programs of LAIL are superior in correctness, code quality, and maintainability. Besides, LAIL has satisfactory transferability across different LLMs and datasets, where the retriever learned on one LLM (dataset) can be transferred to other LLMs (datasets).","Computing methodologies → Neural networks; • Software and its engineering → Automatic pro-
gramming.
Additional Key Words and Phrases: Code generation, in-context-learning, large language model, LLM4SE","J Li, C Tao, J Li, G Li, Z Jin, H Zhang, Z Fang‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3715908,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,Design own metric and pass@k,,,,,,,
91,2024,The larger the better? improved llm code-generation via budget reallocation,"It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model. We consider a standard unit-test setup, which can be used to select the correct output from the smaller model. Our findings reveal that the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs. ",LLM4SE,"M Hassid, T Remez, J Gehring, R Schwartz‚Ä¶",,,,https://arxiv.org/abs/2404.00725,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,"pass@k (pass_flops@f, pass_time@t)",,,,,,,
92,2024,"Smaller, weaker, yet better: Training llm reasoners via compute-optimal sampling","Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners. ",,"H Bansal, A Hosseini, R Agarwal, VQ Tran‚Ä¶",,,,https://arxiv.org/abs/2408.16737,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
93,2024,A deep dive into large language models for automated bug localization and repair,"Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks, including automated program repair (APR). In this study, we take a deep dive into automated bug localization and repair utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug fixing. This methodological separation of bug localization and fixing using different LLMs enables effective integration of diverse contextual information and improved incorporation of inductive biases. We introduce Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework that integrates a bug localization model, an adjustment model to address tokenizer inconsistencies, and a bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We investigate various styles of prompting to the bug fixing model to identify the most effective prompts that better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art (SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable performance on several other widely-used APR datasets, including Defects4J. In the Defects4J benchmark, our approach consistently ranks above other methods, achieving superior results in the Top-10, Top-30, Top-50, and Top-100 metrics. Besides examining Toggle’s generalizability to unseen data, evaluating the effectiveness of various prompts, we also investigate the impact of additional contextual information such as buggy lines and code comments on bug localization, and explore the importance of the adjustment model. Our extensive experiments offer valuable insights and answers to critical research questions.","Software and its engineering → Software testing and debugging.
Additional Key Words and Phrases: Automated Bug Localization and Fix, Large Language Models, LLM4SE","SB Hossain, N Jiang, Q Zhou, X Li, WH Chiang‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3660773,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"Exact Match, Unit Test",Exact Match and test cases,,,,,,,
94,2024,History matters: Temporal knowledge editing in large language model,"The imperative task of revising or updating the knowledge stored within large language models arises from two distinct sources: intrinsic errors inherent in the model which should be corrected and outdated knowledge due to external shifts in the real world which should be updated. Prevailing efforts in model editing conflate these two distinct categories of edits arising from distinct reasons and directly modify the original knowledge in models into new knowledge. However, we argue that preserving the model's original knowledge remains pertinent. Specifically, if a model's knowledge becomes outdated due to evolving worldly dynamics, it should retain recollection of the historical knowledge while integrating the newfound knowledge. In this work, we introduce the task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe (Assessment of TempOral Knowledge Editing) to evaluate current model editing methods. We find that while existing model editing methods are effective at making models remember new knowledge, the edited model catastrophically forgets historical knowledge. To address this gap, we propose a simple and general framework termed Multi-Editing with Time Objective (METO) for enhancing existing editing models, which edits both historical and new knowledge concurrently and optimizes the model's prediction for the time of each fact. Our assessments demonstrate that while AToKe is still difficult, METO maintains the effectiveness of learning new knowledge and meanwhile substantially improves the performance of edited models on utilizing historical knowledge. ","NLP: (Large) Language Models, NLP: Interpretability, Analysis, and Evaluation of NLP Models, KRR: Applications, General ","X Yin, J Jiang, L Yang, X Wan",,,,https://ojs.aaai.org/index.php/AAAI/article/view/29912,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
95,2023,SummEdits: Measuring LLM ability at factual reasoning through the lens of summarization,"With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8% below estimated human performance, highlighting the gaps in LLMs’ ability to reason about facts and detect inconsistencies when they occur.",,"P Laban, W Kry≈õci≈Ñski, D Agarwal‚Ä¶",,,,https://aclanthology.org/2023.emnlp-main.600/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
96,2023,What can large language models do in chemistry? a comprehensive benchmark on eight tasks,"Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4,GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs’ performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.",,"T Guo, B Nan, Z Liang, Z Guo‚Ä¶",,,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/bbb330189ce02be00cf7346167028ab1-Abstract-Datasets_and_Benchmarks.html,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
97,2023,Llm-assisted code cleaning for training accurate code generators,"Natural language to code generation is an important application area of LLMs and has received wide attention from the community. The majority of relevant studies have exclusively concentrated on increasing the quantity and functional correctness of training sets while disregarding other stylistic elements of programs. More recently, data quality has garnered a lot of interest and multiple works have showcased its importance for improving performance. In this work, we investigate data quality for code and find that making the code more structured and readable leads to improved code generation performance of the system. We build a novel data-cleaning pipeline that uses these principles to transform existing programs by 1.) renaming variables, 2.) modularizing and decomposing complex code into smaller helper sub-functions, and 3.) inserting natural-language based plans via LLM based transformations. We evaluate our approach on two challenging algorithmic code generation benchmarks and find that fine-tuning CodeLLaMa-7B on our transformed modularized programs improves the performance by up to 30% compared to fine-tuning on the original dataset. Additionally, we demonstrate improved performance from using a smaller amount of higher-quality data, finding that a model fine-tuned on the entire original dataset is outperformed by a model trained on 15% of our cleaned dataset. Even in comparison to closed-source models, our models outperform the much larger AlphaCoder models. ",LLM4SE,"N Jain, T Zhang, WL Chiang, JE Gonzalez‚Ä¶",,,,https://arxiv.org/abs/2311.14904,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,pass@k,,,,,,,
98,2024,Lab-bench: Measuring capabilities of language models for biology research,"There is widespread optimism that frontier Large Language Models (LLMs) and LLM-augmented systems have the potential to rapidly accelerate scientific discovery across disciplines. Today, many benchmarks exist to measure LLM knowledge and reasoning on textbook-style science questions, but few if any benchmarks are designed to evaluate language model performance on practical tasks required for scientific research, such as literature search, protocol planning, and data analysis. As a step toward building such benchmarks, we introduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset of over 2,400 multiple choice questions for evaluating AI systems on a range of practical biology research capabilities, including recall and reasoning over literature, interpretation of figures, access and navigation of databases, and comprehension and manipulation of DNA and protein sequences. Importantly, in contrast to previous scientific benchmarks, we expect that an AI system that can achieve consistently high scores on the more difficult LAB-Bench tasks would serve as a useful assistant for researchers in areas such as literature search and molecular cloning. As an initial assessment of the emergent scientific task capabilities of frontier language models, we measure performance of several against our benchmark and report results compared to human expert biology researchers. We will continue to update and expand LAB-Bench over time, and expect it to serve as a useful tool in the development of automated research systems going forward. A public subset of LAB-Bench is available for use at the following URL: this https URL",,"JM Laurent, JD Janizek, M Ruzo, MM Hinks‚Ä¶",,,,https://arxiv.org/abs/2407.10362,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
99,2024,"Repairagent: An autonomous, llm-based agent for program repair","Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering. ",LLM4SE,"I Bouzenia, P Devanbu, M Pradel",,,,https://arxiv.org/abs/2403.17134,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Not properly described how to measure a correct solution,Not properly described how to measure a correct solution,,,,,,,
100,2024,When benchmarks are targets: Revealing the sensitivity of large language model leaderboards,"Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at this https URL. ",,"N Alzahrani, H Alyahya, Y Alnumay‚Ä¶",,,,https://arxiv.org/abs/2402.01781,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Again good paper for limitations,Again good paper for limitations,,,,,,,
101,2023,Seqtrack: Sequence to sequence learning for visual object tracking," In this paper, we present a new sequence-to-sequence learning framework for visual tracking, dubbed SeqTrack. It casts visual tracking as a sequence generation problem, which predicts object bounding boxes in an autoregressive fashion. This is different from prior Siamese trackers and transformer trackers, which rely on designing complicated head networks, such as classification and regression heads. SeqTrack only adopts a simple encoder-decoder transformer architecture. The encoder extracts visual features with a bidirectional transformer, while the decoder generates a sequence of bounding box values autoregressively with a causal transformer. The loss function is a plain cross-entropy. Such a sequence learning paradigm not only simplifies tracking framework, but also achieves competitive performance on benchmarks. For instance, SeqTrack gets 72.5% AUC on LaSOT, establishing a new state-of-the-art performance. Code and models are available at https://github.com/microsoft/VideoX.",,"X Chen, H Peng, D Wang, H Lu‚Ä¶",,,,http://openaccess.thecvf.com/content/CVPR2023/html/Chen_SeqTrack_Sequence_to_Sequence_Learning_for_Visual_Object_Tracking_CVPR_2023_paper.html,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
102,2024,Debug like a human: A large language model debugger via verifying runtime execution step-by-step,"    Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state-of-the-art performance in code debugging for various LLM selections. 
",LLM4SE,"L Zhong, Z Wang, J Shang",,,,https://arxiv.org/abs/2402.16906,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
103,2023,Winoqueer: A community-in-the-loop benchmark for anti-lgbtq+ bias in large language models,,,"VK Felkner, HCH Chang, E Jang, J May",,,,https://arxiv.org/abs/2306.15087,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
104,2022,Github issue classification using bert-style models,,,"S Bharadwaj, T Kadam",,,,https://dl.acm.org/doi/abs/10.1145/3528588.3528663,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
105,2025,LLMzSz {\L}: a comprehensive LLM benchmark for Polish,,,"K Jassem, M Ciesi√≥≈Çka, F Grali≈Ñski, P Jab≈Ço≈Ñski‚Ä¶",,,,https://arxiv.org/abs/2501.02266,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
106,2024,Agentfl: Scaling llm-based fault localization to project-level context,,LLM4SE,"Y Qin, S Wang, Y Lou, J Dong, K Wang, X Li‚Ä¶",,,,https://arxiv.org/abs/2403.16362,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
107,2023,LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation,"There has been a recent explosion of research on Large Language Models (LLMs) for software engineering tasks, in particular code generation. However, results from LLMs can be highly unstable; nondeterministically returning very different codes for the same prompt. Non-determinism is a potential menace to scientific conclusion validity. When non-determinism is high, scientific conclusions simply cannot be relied upon unless researchers change their behaviour to control for it in their empirical analyses. This paper conducts an empirical study to demonstrate that non-determinism is, indeed, high, thereby underlining the need for this behavioural change. We choose to study ChatGPT because it is already highly prevalent in the code generation research literature. We report results from a study of 829 code generation problems from three code generation benchmarks (i.e., CodeContests, APPS, and HumanEval). Our results reveal high degrees of non-determinism: the ratio of coding tasks with zero equal test output across different requests is 75.76%, 51.00%, and 47.56% for CodeContests, APPS, and HumanEval, respectively. In addition, we find that setting the temperature to 0 does not guarantee determinism in code generation, although it indeed brings less non-determinism than the default configuration (temperature=1). These results confirm that there is, currently, a significant threat to scientific conclusion validity. In order to put LLM-based research on firmer scientific foundations, researchers need to take into account non-determinism in drawing their conclusions.",LLM4SE,"S Ouyang, JM Zhang, M Harman, M Wang",,,,https://ui.adsabs.harvard.edu/abs/2023arXiv230802828O/abstract,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Interesting for limitations,Interesting for limitations,,,,,,,
108,2024,Routerbench: A benchmark for multi-llm routing system,"As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present RouterBench, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and deliver a comparative analysis of various routing approaches through RouterBench, highlighting their potentials and limitations within our evaluation framework. This work not only formalizes and advances the development of LLM routing systems but also sets a standard for their assessment, paving the way for more accessible and economically viable LLM deployments. The code and data are available at this https URL. ",,"QJ Hu, J Bieker, X Li, N Jiang, B Keigwin‚Ä¶",,,,https://arxiv.org/abs/2403.12031,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
109,2025,Cityeqa: A hierarchical llm agent on embodied question answering benchmark in city space,,,"Y Zhao, K Xu, Z Zhu, Y Hu, Z Zheng, Y Chen‚Ä¶",,,,https://arxiv.org/abs/2502.12532,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
110,2024,Time domain speech enhancement with CNN and time-attention transformer,,,"N Saleem, TS Gunawan, S Dhahbi, S Bourouis",,,,https://www.sciencedirect.com/science/article/pii/S1051200424000332,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
111,2022,"Recipe for a general, powerful, scalable graph transformer",,,"L Ramp√°≈°ek, M Galkin, VP Dwivedi‚Ä¶",,,,https://proceedings.neurips.cc/paper_files/paper/2022/hash/5d4834a159f1547b267a05a4e2b7cf5e-Abstract-Conference.html,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
112,2023,GPTCloneBench: A comprehensive benchmark of semantic clones and cross-language clones using GPT-3 model and SemanticCloneBench,"With the emergence of Machine Learning, there has been a surge in leveraging its capabilities for problem-solving across various domains. In the code clone realm, the identification of type-4 or semantic clones has emerged as a crucial yet challenging task. Researchers aim to utilize Machine Learning to tackle this challenge, often relying on the Big-CloneBench dataset. However, it’s worth noting that BigCloneBench, originally not designed for semantic clone detection, presents several limitations that hinder its suitability as a comprehensive training dataset for this specific purpose. Furthermore, CLCDSA dataset suffers from a lack of reusable examples aligning with real-world software systems, rendering it inadequate for cross-language clone detection approaches. In this work, we present a comprehensive semantic clone and cross-language clone benchmark, GPTCloneBench 1 by exploiting SemanticCloneBench and OpenAI’s GPT-3 model. In particular, using code fragments from SemanticCloneBench as sample inputs along with appropriate prompt engineering for GPT-3 model, we generate semantic and cross-language clones for these specific fragments and then conduct a combination of extensive manual analysis, tool-assisted filtering, functionality testing and automated validation in building the benchmark. From 79,928 clone pairs of GPT-3 output, we created a benchmark with 37,149 true semantic clone pairs, 19,288 false semantic pairs(Type-1/Type-2), and 20,770 cross-language clones across four languages (Java, C, C#, and Python). Our benchmark is 15-fold larger than SemanticCloneBench, has more functional code examples for software systems and programming language support than CLCDSA, and overcomes BigCloneBench’s qualities, quantification, and language variety limitations. GPTCloneBench can be found here1.","Software Clone, SemanticCloneBench,
GPT-3, Language Model, Machine Learning, Cross
Language Clone, Semantic Clone, BigCloneBench.","AI Alam, PR Roy, F Al-Omari, CK Roy‚Ä¶",,,,https://ieeexplore.ieee.org/abstract/document/10336319/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
113,2024,When llm-based code generation meets the software development process,"Software process models are essential to facilitate collaboration and communication among software teams to solve complex development tasks. Inspired by these software engineering practices, we present FlowGen - a code generation framework that emulates software process models based on multiple Large Language Model (LLM) agents. We emulate three process models, FlowGenWaterfall, FlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e., requirement engineer, architect, developer, tester, and scrum master) that correspond to everyday development activities and organize their communication patterns. The agents work collaboratively using chain-of-thought and prompt composition with continuous self-refinement to improve the code quality. We use GPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion) to evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Our findings show that FlowGenScrum excels compared to other process models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement over RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum achieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming Reflexion. Notably, integrating CodeT into FlowGenScrum resulted in statistically significant improvements, achieving the highest Pass@1 scores. Our analysis also reveals that the development activities impacted code smell and exception handling differently, with design and code review adding more exception handling and reducing code smells. Finally, FlowGen models maintain stable Pass@1 scores across GPT3.5 versions and temperature values, highlighting the effectiveness of software process models in enhancing the quality and stability of LLM-generated code. ","Large Language Model, Code Generation,
Agents, Software Process Model, LLM4SE","F Lin, DJ Kim",,,,https://ui.adsabs.harvard.edu/abs/2024arXiv240315852L/abstract,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"Human Annotators, pass@k",Human annotators and pass@k,,,,,,,
114,2025,Metrex: A benchmark for verilog code metric reasoning using llms,"Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMs' reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4% more designs (within a 5% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning.","LLM, Verilog, Metrics, Post-synthesis, Reasoning,
Chain-of-Thought, LLM4SE","M Abdelatty, J Ma, S Reda",,,,https://dl.acm.org/doi/abs/10.1145/3658617.3697625,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,acc@k,acc@k,,,,,,,
115,2024,KorNAT: LLM alignment benchmark for Korean social Values and common knowledge,,,"J Lee, M Kim, S Kim, J Kim, S Won, H Lee‚Ä¶",,,,https://arxiv.org/abs/2402.13605,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
116,2023,Codetransocean: A comprehensive multilingual benchmark for code translation,"Recent code translation techniques exploit neural machine translation models to translate source code from one programming language to another to satisfy production compatibility or to improve efficiency of codebase maintenance. Most existing code translation datasets only focus on a single pair of popular programming languages. To advance research on code translation and meet diverse requirements of real-world applications, we construct CodeTransOcean, a large-scale comprehensive benchmark that supports the largest variety of programming languages for code translation. CodeTransOcean consists of three novel multilingual datasets, namely, MultilingualTrans supporting translations between multiple popular programming languages, NicheTrans for translating between niche programming languages and popular ones, and LLMTrans for evaluating executability of translated code by large language models (LLMs). CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for translating deep learning code across different frameworks. We develop multilingual modeling approaches for code translation and demonstrate their great potential in improving the translation quality of both low-resource and high-resource language pairs and boosting the training efficiency. We also propose a novel evaluation metric Debugging Success Rate@K for program-level code translation. Last but not least, we evaluate LLM ChatGPT on our datasets and investigate its potential for fuzzy execution predictions. We build baselines for CodeTransOcean and analyze challenges of code translation for guiding future research. The CodeTransOcean datasets and code are publicly available at this https URL. ",,"W Yan, Y Tian, Y Li, Q Chen, W Wang",,,,https://arxiv.org/abs/2310.04951,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"DSR@k, BLEU, CodeBLEU","Debugging Success Rate (DSR), Introduces DSR@1 debugging success rate, interesting to see how it is made. BLUE, CodeBLUE is used as well",,,,,,,
117,2024,Llmrg: Improving recommendations through large language model reasoning graphs,,,"Y Wang, Z Chu, X Ouyang, S Wang, H Hao‚Ä¶",,,,https://ojs.aaai.org/index.php/AAAI/article/view/29887,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
118,2024,Codepori: Large scale model for autonomous software development by using multi-agents,,,"Z Rasheed, M Waseem, M Saari, K Syst√§‚Ä¶",,,,https://ui.adsabs.harvard.edu/abs/2024arXiv240201411R/abstract,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
119,2024,Dtllm-vlt: Diverse text generation for visual language tracking based on llm,,,"X Li, X Feng, S Hu, M Wu, D Zhang‚Ä¶",,,,https://openaccess.thecvf.com/content/CVPR2024W/VDU/html/Li_DTLLM-VLT_Diverse_Text_Generation_for_Visual_Language_Tracking_Based_on_CVPRW_2024_paper.html,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
120,2024,How to understand whole software repository?,,,"Y Ma, Q Yang, R Cao, B Li, F Huang, Y Li",,,,https://arxiv.org/abs/2406.01422,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
121,2023,Clarifygpt: Empowering llm-based code generation with intention clarification,,,"F Mu, L Shi, S Wang, Z Yu, B Zhang, C Wang‚Ä¶",,,,https://arxiv.org/abs/2310.10996,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
122,2023,Diversity of thought improves reasoning abilities of large language models,,,"R Naik, V Chandrasekaran, M Yuksekgonul, H Palangi‚Ä¶",,,,https://openreview.net/forum?id=FvfhHucpLd,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
123,2024,Gitbug-java: A reproducible benchmark of recent java bugs,"Bug-fix benchmarks are essential for evaluating methodologies in automatic program repair (APR) and fault localization (FL). However, existing benchmarks, exemplified by Defects4J, need to evolve to incorporate recent bug-fixes aligned with contemporary development practices. Moreover, reproducibility, a key scientific principle, has been lacking in bug-fix benchmarks. To address these gaps, we present GitBug-Java, a reproducible benchmark of recent Java bugs. GitBug-Java features 199 bugs extracted from the 2023 commit history of 55 notable open-source repositories. The methodology for building GitBug-Java ensures the preservation of bug-fixes in fully-reproducible environments. We publish GitBug-Java at https://github.com/gitbugactions/gitbug-java.",LLM4SE,"A Silva, N Saavedra, M Monperrus",,,,https://dl.acm.org/doi/abs/10.1145/3643991.3644884,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,There is no mention of evaluationˇmetrics,There is no mention of evaluationˇmetrics,,,,,,,
124,2024,Vul-rag: Enhancing llm-based vulnerability detection via knowledge-level rag,,,"X Du, G Zheng, K Wang, J Feng, W Deng, M Liu‚Ä¶",,,,https://arxiv.org/abs/2406.11147,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
125,2022,Equiformer: Equivariant graph attention transformer for 3d atomistic graphs,,,"YL Liao, T Smidt",,,,https://arxiv.org/abs/2206.11990,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
126,2023,Llm-based code generation method for golang compiler testing,,,Q Gu,,,,https://dl.acm.org/doi/abs/10.1145/3611643.3617850,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
127,2022,"DB-BERT: a Database Tuning Tool that"" Reads the Manual""",,,I Trummer,,,,https://dl.acm.org/doi/abs/10.1145/3514221.3517843,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
128,2022,Transformer and CNN hybrid deep neural network for semantic segmentation of very-high-resolution remote sensing imagery,,,"C Zhang, W Jiang, Y Zhang, W Wang‚Ä¶",,,,https://ieeexplore.ieee.org/abstract/document/9686732/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
129,2024,Multi-objective fine-tuning for enhanced program repair with llms,,,"B Yang, H Tian, J Ren, H Zhang, J Klein‚Ä¶",,,,https://arxiv.org/abs/2404.12636,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
130,2023,Enhancing recommender systems with large language model reasoning graphs,,,"Y Wang, Z Chu, X Ouyang, S Wang, H Hao‚Ä¶",,,,https://arxiv.org/abs/2308.10835,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
131,2023,Api-bank: A comprehensive benchmark for tool-augmented llms,"Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question. ",,"M Li, Y Zhao, B Yu, F Song, H Li, H Yu, Z Li‚Ä¶",,,,https://arxiv.org/abs/2304.08244,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"ROUGE-L, accuracy","ROUGE-L, accuracy",,,,,,,
132,2024,Code-aware prompting: A study of coverage-guided test generation in regression setting using llm,,,"G Ryan, S Jain, M Shang, S Wang, X Ma‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3643769,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
133,2024,From llm to nmt: Advancing low-resource machine translation with claude,,,"M Enis, M Hopkins",,,,https://arxiv.org/abs/2404.13813,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
134,2023,Code structure‚Äìguided transformer for source code summarization,"Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality.",LLM4SE,"S Gao, C Gao, Y He, J Zeng, L Nie, X Xia‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3522674,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"BLEU, ROUGE-L, Meteor","BLEU, ROUGE, Meteor",,,,,,,
135,2025,MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation,,,"H Xue, F Tang, M Hu, Y Liu, Q Huang, Y Li‚Ä¶",,,,https://arxiv.org/abs/2502.11903,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
136,2023,Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution,,,"E Nguyen, M Poli, M Faizi, A Thomas‚Ä¶",,,,https://proceedings.neurips.cc/paper_files/paper/2023/hash/86ab6927ee4ae9bde4247793c46797c7-Abstract-Conference.html,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
137,2023,Codeapex: A bilingual programming evaluation benchmark for large language models,"    With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. Evaluating the programming capabilities of LLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has numerous downstream applications. In this paper, we propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension, code generation, and code correction abilities of LLMs. Programming comprehension task tests LLMs on multiple-choice exam questions covering conceptual understanding, commonsense reasoning, and multi-hop reasoning. The code generation task evaluates LLMs through completing C++ functions based on provided descriptions and prototypes. The code correction task asks LLMs to fix real-world erroneous code segments with different error messages. We evaluate 12 widely used LLMs, including both general-purpose and specialized models. GPT-4 exhibits the best programming capabilities, achieving approximate accuracy of 69%, 54%, and 66% on the three tasks, respectively. Compared to human performance, there is still significant room for improvement in LLM programming. We hope that CodeApex can serve as a reference for evaluating the coding capabilities of LLMs, further promoting their development and growth. 
",LLM4SE,"L Fu, H Chai, S Luo, K Du, W Zhang, L Fan‚Ä¶",,,,https://arxiv.org/abs/2309.01940,,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,accuracy,accuracy,,,,,,,
138,2022,Persformer: 3d lane detection via perspective transformer and the openlane benchmark,,,"L Chen, C Sima, Y Li, Z Zheng, J Xu, X Geng‚Ä¶",,,,https://link.springer.com/chapter/10.1007/978-3-031-19839-7_32,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
139,2024,Integration of Mamba and Transformer-MAT for Long-Short Range Time Series Forecasting with Application to Weather Dynamics,,,"W Zhang, J Huang, R Wang, C Wei‚Ä¶",,,,https://ieeexplore.ieee.org/abstract/document/10823516/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
140,2024,Knowledge transfer from high-resource to low-resource programming languages for code llms,,,"F Cassano, J Gouwar, F Lucchetti‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3689735,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
141,2024,Exploring and evaluating hallucinations in llm-powered code generation,"The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future. ",LLM4SE,"F Liu, Y Liu, L Shi, H Huang, R Wang, Z Yang‚Ä¶",,,,https://arxiv.org/abs/2404.00971,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,Hallucination Metrics,Hallucination metrics (not sure if I want to add it),,,,,,,
142,2024,Structcoder: Structure-aware transformer for code generation,,,"S Tipirneni, M Zhu, CK Reddy",,,,https://dl.acm.org/doi/abs/10.1145/3636430,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
143,2025,MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents,,,"K Zhu, H Du, Z Hong, X Yang, S Guo, Z Wang‚Ä¶",,,,https://arxiv.org/abs/2503.01935,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
144,2024,Pedestrian attribute recognition: A new benchmark dataset and a large language model augmented framework,,,"J Jin, X Wang, Q Zhu, H Wang, C Li",,,,https://arxiv.org/abs/2408.09720,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
145,2024,Salad-bench: A hierarchical and comprehensive safety benchmark for large language models,"In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile this http URL-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics. Data and evaluator are released under this https URL. ",,"L Li, B Dong, R Wang, X Hu, W Zuo, D Lin‚Ä¶",,,,https://arxiv.org/abs/2402.05044,,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,statistical,statistical,,,,,,,
146,2024,SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types,,,"Y Mou, S Zhang, W Ye",,,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/de7b99107c53e60257c727dc73daf1d1-Abstract-Datasets_and_Benchmarks_Track.html,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
147,2025,Mlgym: A new framework and benchmark for advancing ai research agents,,,"D Nathani, L Madaan, N Roberts, N Bashlykov‚Ä¶",,,,https://arxiv.org/abs/2502.14499,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
148,2022,Temporal and contextual transformer for multi-camera editing of TV shows,,,"A Rao, X Jiang, S Wang, Y Guo, Z Liu, B Dai‚Ä¶",,,,https://arxiv.org/abs/2210.08737,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
149,2024,Coverup: Coverage-guided llm-based test generation,,,"JA Pizzorno, ED Berger",,,,https://arxiv.org/abs/2403.16218,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
150,2024,Mossformer2: Combining transformer and rnn-free recurrent network for enhanced time-domain monaural speech separation,,,"S Zhao, Y Ma, C Ni, C Zhang, H Wang‚Ä¶",,,,https://ieeexplore.ieee.org/abstract/document/10445985/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
151,2023,Embrace divergence for richer insights: A multi-document summarization benchmark and a case study on summarizing diverse information from news articles,"Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40% of the diverse information on average.",,"KH Huang, P Laban, AR Fabbri, PK Choubey‚Ä¶",,,,https://arxiv.org/abs/2309.09369,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,LLM-as-Judge,LLM-as-Judge,,,,,,,
152,2024,"The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism","Current evaluations of large language models (LLMs) often overlook non-determinism, typically focusing on a single output per example. This limits our understanding of LLM performance variability in real-world applications. Our study addresses this issue by exploring key questions about the performance differences between greedy decoding and sampling, identifying benchmarks' consistency regarding non-determinism, and examining unique model behaviors. Through extensive experiments, we observe that greedy decoding generally outperforms sampling methods for most evaluated tasks. We also observe consistent performance across different LLM sizes and alignment methods, noting that alignment can reduce sampling variance. Moreover, our best-of-N sampling approach demonstrates that smaller LLMs can match or surpass larger models such as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This research shows the importance of considering non-determinism in LLM evaluations and provides insights for future LLM development and evaluation. 
",,"Y Song, G Wang, S Li, BY Lin",,,,https://arxiv.org/abs/2407.10457,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,Interesting but for limitations,Interesting but for limitations,,,,,,,
153,2024,Is a large language model a good annotator for event extraction?,"Event extraction is an important task in natural language processing that focuses on mining event-related information from unstructured text. Despite considerable advancements, it is still challenging to achieve satisfactory performance in this task, and issues like data scarcity and imbalance obstruct progress. In this paper, we introduce an innovative approach where we employ Large Language Models (LLMs) as expert annotators for event extraction. We strategically include sample data from the training dataset in the prompt as a reference, ensuring alignment between the data distribution of LLM-generated samples and that of the benchmark dataset. This enables us to craft an augmented dataset that complements existing benchmarks, alleviating the challenges of data imbalance and scarcity and thereby enhancing the performance of fine-tuned models. We conducted extensive experiments to validate the efficacy of our proposed method, and we believe that this approach holds great potential for propelling the development and application of more advanced and reliable event extraction systems in real-world scenarios. ","	NLP: Information Extraction, NLP: (Large) Language Models ","R Chen, C Qin, W Jiang, D Choi",,,,https://ojs.aaai.org/index.php/AAAI/article/view/29730,,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,statistical,statistical,,,,,,,
154,2022,Swin transformer v2: Scaling up capacity and resolution,,,"Z Liu, H Hu, Y Lin, Z Yao, Z Xie, Y Wei‚Ä¶",,,,http://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
155,2022,"A data-scalable transformer for medical image segmentation: architecture, model efficiency, and benchmark",,,"Y Gao, M Zhou, D Liu, Z Yan, S Zhang‚Ä¶",,,,https://arxiv.org/abs/2203.00131,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
156,2024,Medical mT5: an open-source multilingual text-to-text LLM for the medical domain,,,"I Garc√≠a-Ferrero, R Agerri, AA Salazar, E Cabrio‚Ä¶",,,,https://arxiv.org/abs/2404.07613,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
157,2024,Self-collaboration code generation via chatgpt,"Although large language models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, (1) Multiple LLM agents act as distinct “experts,” each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9–47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.","Code generation, large language models, multi-agent collaboration,
software development, LLM4SE","Y Dong, X Jiang, Z Jin, G Li",,,,https://dl.acm.org/doi/abs/10.1145/3672459,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,pass@k,pass@k,,,,,,,
158,2024,The effect of sampling temperature on problem solving in large language models,"In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used nine popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.6. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to generalize across LLMs, prompt-engineering techniques, and problem domains. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature",,M Renze,,,,https://aclanthology.org/2024.findings-emnlp.432/,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"String distance, BLEU, SBERT","String distance, BLEU, SBERT",,,,,,,
159,2024,Self-play fine-tuning converts weak language models to strong language models,,,"Z Chen, Y Deng, H Yuan, K Ji, Q Gu",,,,https://arxiv.org/abs/2401.01335,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
160,2023,StudentEval: A benchmark of student-written prompts for large language models of code,"Code LLMs are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. StudentEval contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code LLM, and we observed very mixed success rates. We use StudentEval to evaluate 5 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or less) effective than they actually are, which has implications for how to teach with Code LLMs. ",LLM4SE,"HML Babe, S Nguyen, Y Zi, A Guha‚Ä¶",,,,https://arxiv.org/abs/2306.04556,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
161,2023,Mossformer: Pushing the performance limit of monaural speech separation using gated single-head transformer with convolution-augmented joint self-attentions,,,"S Zhao, B Ma",,,,https://ieeexplore.ieee.org/abstract/document/10096646/,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
162,2023,Where did the gap go? reassessing the long-range graph benchmark,"The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduced a set of graph learning tasks strongly dependent on long-range interaction between vertices. Empirical evidence suggests that on these tasks Graph Transformers significantly outperform Message Passing GNNs (MPGNNs). In this paper, we carefully reevaluate multiple MPGNN baselines as well as the Graph Transformer GPS (Rampášek et al. 2022) on LRGB. Through a rigorous empirical analysis, we demonstrate that the reported performance gap is overestimated due to suboptimal hyperparameter choices. It is noteworthy that across multiple datasets the performance gap completely vanishes after basic hyperparameter optimization. In addition, we discuss the impact of lacking feature normalization for LRGB's vision datasets and highlight a spurious implementation of LRGB's link prediction metric. The principal aim of our paper is to establish a higher standard of empirical rigor within the graph machine learning community. ",,"J T√∂nshoff, M Ritzert, E Rosenbluth‚Ä¶",,,,https://arxiv.org/abs/2309.00367,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
163,2024,Apollo: A Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People,,,"X Wang, N Chen, J Chen, Y Wang, G Zhen‚Ä¶",,,,https://arxiv.org/abs/2403.03640,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
164,2022,L3Cube-HingCorpus and HingBERT: A code mixed Hindi-English dataset and BERT language models,,,"R Nayak, R Joshi",,,,https://arxiv.org/abs/2204.08398,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
165,2024,Erbench: An entity-relationship based automatically verifiable hallucination benchmark for large language models,"Large language models (LLMs) have achieved unprecedented performances in various applications, yet evaluating them is still challenging. Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity. We contend that utilizing existing relational databases based on the entity-relationship (ER) model is a promising approach for constructing benchmarks as they contain structured knowledge that can be used to question LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational databases have integrity constraints that can be used to better construct complex in-depth questions and verify answers: (1) functional dependencies can be used to pinpoint critical keywords that an LLM must know to properly answer a given question containing certain attribute values; and (2) foreign key constraints can be used to join relations and construct multi-hop questions, which can be arbitrarily long and used to debug intermediate answers. We thus propose ERBench, which uses these integrity constraints to convert any database into an LLM benchmark. ERBench supports continuous evaluation as databases change, multimodal questions, and various prompt engineering techniques. In our experiments, we construct LLM benchmarks using databases of multiple domains and make an extensive comparison of contemporary LLMs. We show how ERBench can properly evaluate any LLM by not only checking for answer correctness, but also effectively verifying the rationales by looking for the right keywords.",,"J Oh, S Kim, J Seo, J Wang, R Xu‚Ä¶",,,,https://proceedings.neurips.cc/paper_files/paper/2024/hash/5ef9853a6cdea40ae3e301a6d8dc32b5-Abstract-Datasets_and_Benchmarks_Track.html,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"Hallucination Metrics, Accuracy","Hallucination, Accuracy. Not sure if I should add it",,,,,,,
166,2024,Meta large language model compiler: Foundation models of compiler optimization,,,"C Cummins, V Seeker, D Grubisic, B Roziere‚Ä¶",,,,https://arxiv.org/abs/2407.02524,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
167,2024,Honeybee: Locality-enhanced projector for multimodal llm,,,"J Cha, W Kang, J Mun, B Roh",,,,http://openaccess.thecvf.com/content/CVPR2024/html/Cha_Honeybee_Locality-enhanced_Projector_for_Multimodal_LLM_CVPR_2024_paper.html,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
168,2024,DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use. ",,"D Guo, Q Zhu, D Yang, Z Xie, K Dong, W Zhang‚Ä¶",,,,https://arxiv.org/abs/2401.14196,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
169,2022,Autopruner: transformer-based call graph pruning,,,"T Le-Cong, HJ Kang, TG Nguyen, SA Haryono‚Ä¶",,,,https://dl.acm.org/doi/abs/10.1145/3540250.3549175,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
170,2025,Enriching automatic test case generation by extracting relevant test inputs from bug reports,,,"WC Ou√©draogo, L Plein, K Kabor√©, A Habib‚Ä¶",,,,https://link.springer.com/article/10.1007/s10664-025-10635-z,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
171,2025,Explainable automated debugging via large language model-driven scientific debugging,"Automated debugging techniques have the potential to reduce developer effort in debugging. However, while developers want rationales for the provided automatic debugging results, existing techniques are ill-suited to provide them, as their deduction process differs significantly froof human developers. Inspired by the way developers interact with code when debugging, we propose Automated Scientific Debugging (AutoSD), a technique that prompts large language models to automatically generate hypotheses, uses debuggers to interact with buggy code, and thus automatically reach conclusions prior to patch generation. In doing so, we aim to produce explanations of how a specific patch has been generated, with the hope that these explanations will lead to enhanced developer decision-making. Our empirical analysis on three program repair benchmarks shows that AutoSDperforms competitively with other program repair baselines, and that it can indicate when it is confident in its results. Furthermore, we perform a human study with 20 participants to evaluate AutoSD-generated explanations. Participants with access to explanations judged patch correctness more accurately in five out of six real-world bugs studied. Furthermore, 70% of participants answered that they wanted explanations when using repair tools, and 55% answered that they were satisfied with the Scientific Debugging presentation.","Automated Program Repair · Machine Learning, LLM4SE","S Kang, B Chen, S Yoo, JG Lou",,,,https://link.springer.com/article/10.1007/s10664-024-10594-x,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
172,2024,Evolving code with a large language model,"Algorithms that use Large Language Models (LLMs) to evolve code arrived on the Genetic Programming (GP) scene very recently. We present LLM_GP, a general LLM-based evolutionary algorithm designed to evolve code. Like GP, it uses evolutionary operators, but its designs and implementations of those operators significantly differ from GP’s because they enlist an LLM, using prompting and the LLM’s pre-trained pattern matching and sequence completion capability. We also present a demonstration-level variant of LLM_GP and share its code. By presentations that range from formal to hands-on, we cover design and LLM-usage considerations as well as the scientific challenges that arise when using an LLM for genetic programming.",,"E Hemberg, S Moskal, UM O'Reilly",,,,https://link.springer.com/article/10.1007/s10710-024-09494-2,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
173,2024,State of what art? a call for multi-prompt llm evaluation,"Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a large-scale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.",,"M Mizrahi, G Kaplan, D Malkin, R Dror‚Ä¶",,,,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00681/123885,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,"New metrics, but not sure how?","New metrics, but not sure how?",,,,,,,
174,2022,Improving across-dataset brain tissue segmentation for MRI imaging using transformer,,,"VM Rao, Z Wan, S Arabshahi, DJ Ma, PY Lee‚Ä¶",,,,https://www.frontiersin.org/articles/10.3389/fnimg.2022.1023481/full,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
175,2024,Devbench: A comprehensive benchmark for software development,"Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of coding, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval, encompassing stages including software design, environment setup, implementation, acceptance testing, and unit testing. DevEval features four programming languages, multiple domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications. 
",,"B Li, W Wu, Z Tang, L Shi, J Yang‚Ä¶",,,,https://se-research.bytedance.com/publication/arxiv24a/arxiv24a.pdf,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,,"general principles, faithfulness","general principles, faithfulness",,,,,,,
176,2024,Cybermetric: A benchmark dataset for evaluating large language models knowledge in cybersecurity,"Large Language Models (LLMs) excel across var-
ious domains, from computer vision to medical diagnostics.
However, understanding the diverse landscape of cybersecurity,
encompassing cryptography, reverse engineering, and managerial
facets like risk assessment, presents a challenge, even for human
experts. In this paper, we introduce CyberMetric, a benchmark
dataset comprising 10,000 questions sourced from standards,
certifications, research papers, books, and other publications in
the cybersecurity domain. The questions are created through
a collaborative process, i.e., merging expert knowledge with
LLMs, including GPT-3.5 and Falcon-180B. Human experts spent
over 200 hours verifying their accuracy and relevance. Beyond
assessing LLMs’ knowledge, the dataset’s main goal is to facilitate
a fair comparison between humans and different LLMs in
cybersecurity. To achieve this, we carefully selected 80 questions
covering a wide range of topics within cybersecurity and involved
30 participants of diverse expertise levels, facilitating a compre-
hensive comparison between human and machine intelligence in
this area. The findings revealed that LLMs outperformed humans
in almost every aspect of cybersecurity.","Benchmrk Dataset, Large Language Models,
LLM cybersecurity knowledge, Generative AI, LLM4SE","N Tihanyi, MA Ferrag, R Jain‚Ä¶",,,,https://www.researchgate.net/profile/Mohamed-Amine-Ferrag/publication/378156241_CyberMetric_A_Benchmark_Dataset_for_Evaluating_Large_Language_Models_Knowledge_in_Cybersecurity/links/65caffe11e1ec12eff8a809d/CyberMetric-A-Benchmark-Dataset-for-Evaluating-Large-Language-Models-Knowledge-in-Cybersecurity.pdf,,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
177,2024,Benchmarking public large language model,,,VM Malode,,,,https://opus4.kobv.de/opus4-haw/files/4593/I001854150Thesis.pdf,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
178,2023,Generate and pray: Using sallms to evaluate the security of llm generated code,"With the growing popularity of Large Language Mod-
els (LLMs) in software engineers’ daily practices, it is important
to ensure that the code generated by these tools is not only
functionally correct but also free of vulnerabilities. Although
LLMs can help developers to be more productive, prior empirical
studies have shown that LLMs can generate insecure code. There
are two contributing factors to the insecure code generation.
First, existing datasets used to evaluate LLMs do not adequately
represent genuine software engineering tasks sensitive to security.
Instead, they are often based on competitive programming chal-
lenges or classroom-type coding tasks. In real-world applications,
the code produced is integrated into larger codebases, introduc-
ing potential security risks. Second, existing evaluation metrics
primarily focus on the functional correctness of the generated
code while ignoring security considerations. Therefore, in this
paper, we described SALLM, a framework to benchmark LLMs’
abilities to generate secure code systematically. This framework
has three major components: a novel dataset of security-centric
Python prompts, configurable assessment techniques to evaluate
the generated code, and novel metrics to evaluate the models’
performance from the perspective of secure code generation.","security evaluation, large language models, pre-
trained transformer model, metrics, LLM4SE","ML Siddiq, JCS Santos",,,,https://lsiddiqsunny.github.io/public/2311.00889.pdf,,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,,"secure@k, vulnerable@k","secure@k, vulnerable@k. New metrics defined",,,,,,,
179,2024,Efficiency in language understanding and generation: An evaluation of four open-source large language models,"This study provides a comprehensive evaluation of the efficiency of Large Language Models (LLMs) in performing diverse lan-
guage understanding and generation tasks. Through a systematic comparison of open-source models including GPT-Neo, Bloom,
FLAN-T5, and Mistral-7B, the research explores their performance across widely recognized benchmarks such as GLUE, Super-
GLUE, LAMBADA, and SQuAD. Our findings reveal significant variations in model accuracy, computational efficiency, scalability,
and adaptability, underscoring the influence of model architecture and training paradigms on performance outcomes. The study
identifies key factors contributing to the models’ efficiency and offers insights into potential optimization strategies for enhancing
their applicability in real-world NLP applications. By highlighting the strengths and limitations of current LLMs, this research
contributes to the ongoing development of more effective, efficient, and adaptable language models, paving the way for future
advancements in the field of natural language processing","Large Language Models, Natural Language Processing, Model Eciency, Computational
Eciency, Scalability, Adaptability","SM Wong, H Leung, KY Wong",,,,https://assets-eu.researchsquare.com/files/rs-4063228/v1_covered_c9d874ce-2b06-41ed-aac8-fbda3c333399.pdf,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,"accuracy, efficiency, scalability, ","accuracy, efficiency, scalability, ",,,,,,,
180,2024,Achieving higher factual accuracy in llama llm with weighted distribution of retrieval-augmented generation,"Introducing a novel concept, the integration of a weighted distribution of Retrieval-Augmented Generation (RAG) with the Llama
Large language model significantly enhances factual accuracy and contextual relevance in generated text. Experimental results show
substantial improvements in precision, recall, F1 score, and BLEU score, demonstrating the effectiveness of the weighted RAG
mechanism in prioritizing high-quality information during the generation process. Human evaluations further validate the model’s
practical applicability and reliability, highlighting its potential for deployment in high-stakes environments. The contributions
of this research provide a scalable framework for improving language models, offering new avenues for dynamic context-aware
weighting and real-time feedback integration. Future work will focus on refining the weighting mechanism, exploring advanced
retrieval algorithms, and expanding applications to multilingual settings and domain-specific corpora, driving continued innovation
in natural language processing.","Retrieval-Augmented Generation, Llama Large, Factual Accuracy, Contextual Relevance, NLP, Language Models","Z Gai, L Tong, Q Ge",,,,https://files.osf.io/v1/resources/ctw8v_v1/providers/osfstorage/664c901d2f167d17c20e7d42?action=download&direct&version=1,,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,,"Human annotators, BLEU, F1-score, precision, recall. Not included because when trying to find the citation for it, it is seen to be unreliable","Human annotators, BLEU, F1-score, precision, recall. Not included because when trying to find the citation for it, it is seen to be unreliable",,,,,,,
181,2023,Pentestgpt: An llm-empowered automatic penetration testing tool,"Penetration testing, a crucial industrial practice for ensur-
ing system security, has traditionally resisted automation due
to the extensive expertise required by human professionals.
Large Language Models (LLMs) have shown significant ad-
vancements in various domains, and their emergent abilities
suggest their potential to revolutionize industries. In this work,
we establish a comprehensive benchmark using real-world
penetration testing targets and further use it to explore the
capabilities of LLMs in this domain. Our findings reveal that
while LLMs demonstrate proficiency in specific sub-tasks
within the penetration testing process, such as using testing
tools, interpreting outputs, and proposing subsequent actions,
they also encounter difficulties maintaining a whole context
of the overall testing scenario.
Based on these insights, we introduce PENTESTGPT, an
LLM-empowered automated penetration testing framework
that leverages the abundant domain knowledge inherent in
LLMs. PENTESTGPT is meticulously designed with three
self-interacting modules, each addressing individual sub-tasks
of penetration testing, to mitigate the challenges related to
context loss. Our evaluation shows that PENTESTGPT not
only outperforms LLMs with a task-completion increase of
228.6% compared to the GPT-3.5 model among the bench-
mark targets, but also proves effective in tackling real-world
penetration testing targets and CTF challenges. Having been
open-sourced on GitHub, PENTESTGPT has garnered over
6,200 stars in 9 months and fostered active community engage-
ment, attesting to its value and impact in both the academic
and industrial spheres.",,"G Deng, Y Liu, V Mayoral-Vilches, P Liu‚Ä¶",,,,https://aliasrobotics.com/files/PentestGPT_paper.pdf,,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,,,,,,,,,,
182,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
183,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
184,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
185,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
186,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
187,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
188,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
189,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
190,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
191,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
192,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
193,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
194,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
195,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
196,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
197,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
198,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
199,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
200,,,,,,,,,,,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,,,,,,,,
,,,,,,,,,,,,,,,,,,45,,,,,,,,,,