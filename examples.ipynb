{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1 Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yisrani/Documents/Papers/Publish&Perish/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "from sacrebleu import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.6606328636027614, 'precisions': [0.8888888888888888, 0.75, 0.5714285714285714, 0.5], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 9, 'reference_length': 9}\n",
      "BLEU = 66.06 88.9/75.0/57.1/50.0 (BP = 1.000 ratio = 1.000 hyp_len = 9 ref_len = 9)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to accept the generous offer.\"\n",
    "reference = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym - Larger text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.9923716383464694, 'precisions': [0.9966329966329966, 0.9932432432432432, 0.9898305084745763, 0.9897959183673469], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 297, 'reference_length': 297}\n",
      "BLEU = 99.24 99.7/99.3/99.0/99.0 (BP = 1.000 ratio = 1.000 hyp_len = 297 ref_len = 297)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     Their impact spans various fields such as education, software engineering, and research. She was happy to accept the generous offer.\"\n",
    "reference = \"It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     Their impact spans various fields such as education, software engineering, and research.  She was happy to accept the kind offer.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [1.0, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 1, 'reference_length': 1}\n",
      "BLEU = 0.00 100.0/0.0/0.0/0.0 (BP = 1.000 ratio = 1.000 hyp_len = 1 ref_len = 1)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"-\"\n",
    "reference = \"-\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 4, 'reference_length': 4}\n",
      "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 4 ref_len = 4)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"- - - -\"\n",
    "reference = \"- - - -\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.668740304976422, 'precisions': [0.8, 0.75, 0.6666666666666666, 0.5], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 5, 'reference_length': 5}\n",
      "BLEU = 66.87 80.0/75.0/66.7/50.0 (BP = 1.000 ratio = 1.000 hyp_len = 5 ref_len = 5)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"I will always help!\"\n",
    "reference = \"I will always help.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.4412484512922977, 'precisions': [0.875, 0.7142857142857143, 0.5, 0.2], 'brevity_penalty': 0.8824969025845955, 'length_ratio': 0.8888888888888888, 'translation_length': 8, 'reference_length': 9}\n",
      "BLEU = 44.12 87.5/71.4/50.0/20.0 (BP = 0.882 ratio = 0.889 hyp_len = 8 ref_len = 9)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to-accept the kind offer.\"\n",
    "reference = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.6580370064762462, 'precisions': [0.9, 0.7777777777777778, 0.625, 0.42857142857142855], 'brevity_penalty': 1.0, 'length_ratio': 1.1111111111111112, 'translation_length': 10, 'reference_length': 9}\n",
      "BLEU = 65.80 90.0/77.8/62.5/42.9 (BP = 1.000 ratio = 1.111 hyp_len = 10 ref_len = 9)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to - accept the kind offer.\"\n",
    "reference = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.38940039153570244, 'precisions': [0.875, 0.7142857142857143, 0.5, 0.2], 'brevity_penalty': 0.7788007830714049, 'length_ratio': 0.8, 'translation_length': 8, 'reference_length': 10}\n",
      "BLEU = 38.94 87.5/71.4/50.0/20.0 (BP = 0.779 ratio = 0.800 hyp_len = 8 ref_len = 10)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to-accept the kind offer.\"\n",
    "reference = \"She was happy to - accept the kind offer.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.6580370064762462, 'precisions': [0.9, 0.7777777777777778, 0.625, 0.42857142857142855], 'brevity_penalty': 1.0, 'length_ratio': 1.1111111111111112, 'translation_length': 10, 'reference_length': 9}\n",
      "BLEU = 65.80 90.0/77.8/62.5/42.9 (BP = 1.000 ratio = 1.111 hyp_len = 10 ref_len = 9)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to accept, the kind offer.\"\n",
    "reference = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.1808713155931068, 'precisions': [0.3076923076923077, 0.24, 0.16666666666666666, 0.08695652173913043], 'brevity_penalty': 1.0, 'length_ratio': 2.888888888888889, 'translation_length': 26, 'reference_length': 9}\n",
      "BLEU = 18.09 30.8/24.0/16.7/8.7 (BP = 1.000 ratio = 2.889 hyp_len = 26 ref_len = 9)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy t/;,/?#$@!&^%*()+o accept the kind offer.\"\n",
    "reference = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.23620142333755398, 'precisions': [0.36, 0.2916666666666667, 0.21739130434782608, 0.13636363636363635], 'brevity_penalty': 1.0, 'length_ratio': 2.7777777777777777, 'translation_length': 25, 'reference_length': 9}\n",
      "BLEU = 23.62 36.0/29.2/21.7/13.6 (BP = 1.000 ratio = 2.778 hyp_len = 25 ref_len = 9)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to /;,/?#$@!&^%*()+ accept the kind offer.\"\n",
    "reference = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 6, 'reference_length': 6}\n",
      "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 6 ref_len = 6)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"functionName( type param ):\"\n",
    "reference = \"functionName( type param ):\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.6666666666666666, 0.5, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 3, 'reference_length': 3}\n",
      "BLEU = 0.00 66.7/50.0/50.0/0.0 (BP = 1.000 ratio = 1.000 hyp_len = 3 ref_len = 3)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"o/k\"\n",
    "reference = \"o/j\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.6666666666666666, 0.5, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 3, 'reference_length': 3}\n",
      "BLEU = 0.00 66.7/50.0/50.0/0.0 (BP = 1.000 ratio = 1.000 hyp_len = 3 ref_len = 3)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"o/ k\"\n",
    "reference = \"o/j\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0.0], 'brevity_penalty': 0.1353352832366127, 'length_ratio': 0.3333333333333333, 'translation_length': 1, 'reference_length': 3}\n",
      "BLEU = 0.00 0.0/0.0/0.0/0.0 (BP = 0.135 ratio = 0.333 hyp_len = 1 ref_len = 3)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"ok\"\n",
    "reference = \"o/j\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols at start of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.5969491792019646, 'precisions': [0.8888888888888888, 0.75, 0.5714285714285714, 0.3333333333333333], 'brevity_penalty': 1.0, 'length_ratio': 1.125, 'translation_length': 9, 'reference_length': 8}\n",
      "BLEU = 59.69 88.9/75.0/57.1/33.3 (BP = 1.000 ratio = 1.125 hyp_len = 9 ref_len = 8)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy +to accept the kind offer\"\n",
    "reference = \"She was happy to accept the kind offer\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols after word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.5969491792019646, 'precisions': [0.8888888888888888, 0.75, 0.5714285714285714, 0.3333333333333333], 'brevity_penalty': 1.0, 'length_ratio': 1.125, 'translation_length': 9, 'reference_length': 8}\n",
      "BLEU = 59.69 88.9/75.0/57.1/33.3 (BP = 1.000 ratio = 1.125 hyp_len = 9 ref_len = 8)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to+ accept the kind offer\"\n",
    "reference = \"She was happy to accept the kind offer\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [1.0, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 1, 'reference_length': 1}\n",
      "BLEU = 0.00 100.0/0.0/0.0/0.0 (BP = 1.000 ratio = 1.000 hyp_len = 1 ref_len = 1)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"0\"\n",
    "reference = \"0\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [1.0, 0.0, 0.0, 0.0], 'brevity_penalty': 0.36787944117144233, 'length_ratio': 0.5, 'translation_length': 1, 'reference_length': 2}\n",
      "BLEU = 0.00 100.0/0.0/0.0/0.0 (BP = 0.368 ratio = 0.500 hyp_len = 1 ref_len = 2)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"0\"\n",
    "reference = \"123 0\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.0, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 2, 'reference_length': 2}\n",
      "BLEU = 0.00 0.0/0.0/0.0/0.0 (BP = 1.000 ratio = 1.000 hyp_len = 2 ref_len = 2)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"higher values\"\n",
    "reference = \"HIGHER VALUES\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order + Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.5, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 2, 'reference_length': 2}\n",
      "BLEU = 0.00 50.0/50.0/0.0/0.0 (BP = 1.000 ratio = 1.000 hyp_len = 2 ref_len = 2)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"higher values\"\n",
    "reference = \"VALUES higher\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [1.0, 0.6666666666666666, 1.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 4, 'reference_length': 4}\n",
      "BLEU = 75.98 100.0/66.7/100.0/50.0 (BP = 1.000 ratio = 1.000 hyp_len = 4 ref_len = 4)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"higher values higher values\"\n",
    "reference = \"values higher values higher\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper + abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/0l/m9q4hqlx7j95m4nz7kp_6fmc0000gp/T/ipykernel_58369/3720519415.py:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  body = '''\n"
     ]
    }
   ],
   "source": [
    "body = '''\n",
    "monte carlo methods @xcite appeared about sixty years ago with the need to evaluate numerical values for various complex problems . \n",
    "these methods evolved and were applied early to quantum problems , thus putting within reach exact numerical solutions to non - trivial quantum problems @xcite . \n",
    "many improvements of these methods followed , avoiding critical slowing down near phase transitions and allowing to work directly in the continuous imaginary time limit @xcite . in recent years , \n",
    "interest in methods that work in the canonical ensemble with global updates yet allow access to green functions has intensified @xcite . \n",
    "however , a method that works well for a given hamiltonian often needs major modifications for another . \n",
    "for example , the addition of a 4-site ring exchange term in the bosonic hubbard model required special developments for a treatment by the stochastic series expansion algorithm @xcite , as well as by the wordline algorithm @xcite . \n",
    "this can result in long delays . \n",
    "it is , therefore , advantageous to have at one s disposal an algorithm that can be applied to a very wide class of hamiltonians without requiring any changes . in a recent publication @xcite , \n",
    "the stochastic green function ( sgf ) algorithm was presented , which meets this goal . \n",
    "the algorithm can be applied to any lattice hamiltonian of the form @xmath3 where @xmath1 is diagonal in the chosen occupation number basis and @xmath2 has only positive matrix elements . \n",
    "this includes all kinds of systems that can be treated by other methods presented in ref.@xcite , for instance bose - hubbard models with or without a trap , bose - fermi mixtures in one dimension , heisenberg models ... in particular hamiltonians for which the non - diagonal part @xmath2 is non - trivial ( the eigen - basis is unknown ) are easily treated , such as the bose - hubbard model with ring exchange @xcite , or multi - species hamiltonians in which a given species can be turned into another one ( see eq.([twospecies ] ) and fig . \n",
    "[ density ] and [ momentum ] for a concrete example ) . \n",
    "systems for which it is not possible to find a basis in which @xmath1 is diagonal and @xmath2 has only positive matrix elements are said to have a `` sign problem '' , which usually arises with fermionic and frustrated systems . as other qmc methods , the sgf algorithm does not solve this problem .    the algorithm allows to measure several quantities of interest , such as the energy , the local density , local compressibility , density - density correlation functions ... in particular the winding is sampled and gives access to the superfluid density . \n",
    "equal - time n - body green functions are probably the most interesting quantities that can be measured by the algorithm , by giving access to momentum distribution functions which allow direct comparisons with experiments . \n",
    "all details on measurements are given in ref.@xcite . \n",
    "in addition the algorithm has the property of being easy to code , due in part to a simple update scheme in which all moves are accepted with a probability of 1 . despite of such generality and simplicity , \n",
    "the algorithm might suffer from a reduced efficiency , compared to other algorithms in situations where they can be applied . \n",
    "the purpose of this paper is to present a `` directed '' update scheme that ( i ) keeps the simplicity and generality of the original sgf algorithm , and ( ii ) enhances its efficiency by improving the sampling over the imaginary time axis . \n",
    "while the sgf algorithm is not intended to compete with the speed of other algorithms , the improvment resulting from the directed update scheme is remarkable ( see section v ) . \n",
    "but what makes the strength of the sgf method is that it allows to simulate hamiltonians that can not be treated by other methods or that would require special developments ( see eq.([twospecies ] ) for a concrete example ) . \n",
    "the paper is organized as follows : we introduce in section ii the notations and definitions used in ref.@xcite . in section iii , we propose a simplification of the update scheme used in the original sgf algorithm , and determine how to satisfy detailed balance . \n",
    "a generalization of the simplified update scheme is presented in section iv , which constitutes the directed updated scheme . \n",
    "finally section v shows how to determine the introduced optimization parameters , and presents some tests of the algorithm and a comparison with the original version . \n",
    "in this section , we recall the expression of the `` green operator '' introduced in the sgf algorithm , and the extended partition function which is considered . although not required for understanding this paper , we refer the reader to ref.@xcite for full details on the algorithm . as many qmc algorithms , \n",
    "the sgf algorithm samples the partition function @xmath4 the algorithm has the property of working in the canonical ensemble . in order to define the green operator , \n",
    "we first define the `` normalized '' creation and annihilation operators , @xmath5 where @xmath6 and @xmath7 are the usual creation and annihilation operators of bosons , and @xmath8 is the number operator . from ( [ normalizedoperators ] ) \n",
    "one can show the following relations for any state @xmath9 in the occupation number representation , @xmath10 with the particular case @xmath11 . \n",
    "appart from this exception , the operators @xmath12 and @xmath13 change a state @xmath9 by respectively creating and annihilating one particle , but they do not change the norm of the state .    using the notation @xmath14 to denote two subsets of site indices @xmath15 and @xmath16 with the constraint that all indices in subset @xmath17 are different from the indices in subset @xmath18 ( but several indices in one subset may be equal ) , we define the green operator @xmath19 by @xmath20 where @xmath21 is a matrix that depends on the application of the algorithm @xcite . in order to sample the partition function ( [ partitionfunction ] ) , an extended partition function @xmath22 is considered by breaking up the propagator @xmath23 , and introducing the green operator between the broken parts , @xmath24 defining the time dependant operators @xmath25 and @xmath26 \n",
    ", @xmath27 and working in the occupation number basis in which @xmath1 is diagonal , the extended partition function takes the form @xmath28 where the sum @xmath29 implicitly runs over complete sets of states @xmath30 . we will systematically use the labels @xmath31 and @xmath32 to denote the states appearing on the left and the right of the green operator , and use the notation @xmath33 to denote the diagonal energy @xmath34 . \n",
    "we will also denote by @xmath35 and @xmath36 the time indices of the @xmath2 operators appearing on the left and the right of @xmath19 .    as a result , \n",
    "the extended partition function is a sum over all possible configurations , each being determined by a set of time indices @xmath37 and a set of states @xmath38 , @xmath39 , @xmath40,@xmath41 , @xmath42 . \n",
    "the algorithm consists in updating those configurations by making use of the green operator . assuming that the green operator is acting at time @xmath43 \n",
    ", it can `` create '' a @xmath2 operator ( that is to say a @xmath2 operator can be inserted in the operator string ) at the same time , thus introducing a new intermediate state , then it can be shifted to a different time . while shifting , any @xmath2 operator encountered by the green operator is `` destroyed '' ( that is to say removed from the operator string ) . assuming a left ( or right ) move , creating an operator will update the state @xmath44 ( or @xmath41 ) , \n",
    "while destroying will update the state @xmath41 ( or @xmath44 ) . \n",
    "when a diagonal configuration of the green operator occurs , @xmath45 , such a configuration associated to the extended partition function ( [ extendedpartitionfunction ] ) is also a configuration associated to the partition function ( [ partitionfunction ] ) . \n",
    "measurements can be done when this occurs ( see ref.@xcite for details on measurements ) . \n",
    "next section presents a simple update scheme that meets the requirements of ergodicity and detailed balance . \n",
    "before introducing the directed update , we start by simplifying the update scheme used in the original sgf algorithm \n",
    ".      we will assume in the following that a left move of the green operator is chosen . \n",
    "in the original version , the green operator @xmath26 can choose to create or not on its right a @xmath2 operator at time @xmath43 . \n",
    "then a time shift @xmath46 to the left is chosen for the green operator with an exponential distribution in the range @xmath47 . \n",
    "if an operator is encountered while shifting the green operator , then the operator is destroyed and the move stops there . as a result , four possible situations can occur during one move :    1 .   no creation , shift , no destruction . \n",
    "2 .   creation , shift , no destruction . \n",
    "3 .   no creation , shift , destruction . \n",
    "4 .   creation , shift , destruction . \n",
    "it appears that the first possibility `` no creation , no destruction '' is actually useless , since no change is performed in the operator string . the idea is to get rid of this possibility by forcing the green operator to destroy an operator if no creation is chosen \n",
    "a further simplification can be done by noticing that the last possibility `` creation , destruction '' is not necessary for the ergodicity of the algorithm , and can be avoided by restricting the range of the time shift after having created an operator . \n",
    "therefore we replace the original update scheme by the following : we assume that the green operator is acting at time @xmath43 and that the operator on its left is acting at time @xmath35 . the green operator @xmath26 chooses to create or not an operator on its right at time @xmath43 . \n",
    "if creation is chosen , then a time shift @xmath46 of the green operator is chosen to the left in the range @xmath48 , with the probability distribution defined below . \n",
    "if no creation is chosen , then the green operator is directly shifted to the operator on its left at time @xmath35 , and the operator is destroyed . as a result \n",
    "only two possibilities have to be considered :    1 . \n",
    "creation , shift . \n",
    "2 .   shift , destruction . \n",
    "figure [ simplfiedupdatescheme ] shows the associated organigram . \n",
    "section iii.b explains how detailed balance can be satisfied with this simplified update scheme .          when updating the configurations according to the chosen update scheme , we need to generate different transitions from initial to final states with probabilities that satisfy detailed balance . in this section \n",
    "we propose a choice for these probabilities , and determine the corresponding acceptance factors . \n",
    "we denote the probability of the initial ( final ) configuration by @xmath49 ( @xmath50 ) . \n",
    "we denote by @xmath51 the probability of the transition from configuration @xmath17 to configuration @xmath52 , and by @xmath53 the probability of the reverse transition . \n",
    "finally we denote by @xmath54 the acceptance rate of the transition from @xmath17 to @xmath52 , and by @xmath55 the acceptance rate of the reverse transition . \n",
    "the detailed balance can be written as @xmath56 we will make use of the metropolis solution @xcite , @xmath57 with @xmath58 we will use primed ( non - primed ) labels for states and time indices to denote final ( initial ) configurations . \n",
    "we consider here the case where a left move is chosen , an operator is created on the right of the green operator at time @xmath43 , and a new state is chosen . \n",
    "then a time shift to the left is chosen for the green operator in the range @xmath59 . \n",
    "it is important to note that @xmath60 and @xmath61 correspond to the time indices of the operators appearing on the left and the right of the green operator after the new operator has been inserted , that is to say at the moment where the time shift needs to be performed . \n",
    "thus we have @xmath62 and @xmath63 . \n",
    "the probability of the initial configuration is the boltzmann weight appearing in the extended partition function ( [ extendedpartitionfunction ] ) : @xmath64 the probability of the final configuration takes the form : @xmath65 it is important here to realize that the green operator only inserted on its right the operator @xmath66 , before being shifted from @xmath61 to @xmath67 . \n",
    "therefore we have the equalities @xmath68 , @xmath69 , @xmath70 , and @xmath71 . \n",
    "the probability @xmath51 of the transition from the initial configuration to the final configuration is the probability @xmath72 of a left move , times the probability @xmath73 of a creation , times the probability @xmath74 to choose the new state @xmath75 , times the probability @xmath76 to shift the green operator by @xmath77 , knowing that the states on the left and the right of the green operator at the moment of the shift are @xmath78 and @xmath79 : @xmath80 the probability of the reverse transition is simply the probability @xmath81 of a right move , times the probability of no creation , @xmath82 : @xmath83\\ ] ] from the original version of the sgf algorithm , we know that choosing the time shift with an exponential distribution is a good choice , because it cancels the exponentials appearing in the probabilities of the initial ( [ initial ] ) and final ( [ final ] ) configurations , avoiding exponentially small acceptance factors . \n",
    "however a different normalization must be used here , since the time shift is chosen in the range @xmath84 instead of @xmath47 . \n",
    "the suitable solution is : @xmath85 it is straightforward to check that the above probability is correctly normalized and well - defined for any real value of @xmath86 , the particular case @xmath87 reducing to the uniform distribution @xmath88 ( note that @xmath89 is always a positive number ) . for the probability @xmath74 to choose the new state @xmath75 , \n",
    "the convenient solution is the same as in the original version : @xmath90 putting everything together , the acceptance factor ( [ metropolis2 ] ) becomes @xmath91\\big[1-e^{-(\\tau_l^\\prime-\\tau_r^\\prime)(v_r^\\prime - v_l^\\prime)}\\big]}{v_r^\\prime - v_l^\\prime},\\end{aligned}\\ ] ] where we have used the notation @xmath92 to emphasize that this acceptance factor corresponds to a creation . \n",
    "it is also important for the remaining of this paper to note that @xmath92 is written as a quantity that depends on the initial configuration , times a quantity that depends on the final configuration . \n",
    "we consider here the case where a left move is chosen , and the operator on the left of the green operator is destroyed . \n",
    "this move corresponds to the inverse of the above `` creation , shift '' move . \n",
    "thus , the corresponding acceptance factor @xmath93 is obtained by inverting the acceptance factor @xmath92 , exchanging the initial time @xmath43 and final time @xmath67 , and switching the direction . \n",
    "however @xmath94 represents an absolute time shift , so @xmath35 and @xmath36 do not have to be exchanged . \n",
    "we get @xmath95\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\                             & \\times & \\frac{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle p(\\rightarrow^\\prime)p_\\rightarrow^\\dagger(\\tau^\\prime)}{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle},\\end{aligned}\\ ] ] which is written as a quantity that depends on the initial configuration , times a quantity that depends on the final configuration . \n",
    "we will use here the short notation @xmath96 , @xmath97 , and @xmath98 to denote respectively the quantities @xmath99 , @xmath100 , and @xmath101 . as in ref . \n",
    "@xcite , we have some freedom for the choice of the probabilities of choosing a left or right move , @xmath72 and @xmath102 , and the probabilities of creation @xmath73 and @xmath103 . \n",
    "a suitable choice for those probabilities can be done in order to accept all moves , resulting in an appreciable simplification of the algorithm . for this purpose , \n",
    "we impose the acceptance factor @xmath92 ( or @xmath104 ) to be equal to the acceptance factor @xmath93 ( or @xmath105 ) . \n",
    "this allows to determine the probabilities @xmath73 and @xmath103 , @xmath106 and the acceptance factors @xmath107 and @xmath108 take the form @xmath109 with @xmath110 finally we can impose the acceptance factors @xmath111 and @xmath112 to be equal . \n",
    "this implies @xmath113 defining @xmath114 , we are left with a single acceptance factor , @xmath115 which is independent of the chosen direction , and independent of the nature of the move ( creation or destruction ) . \n",
    "thus all moves can be accepted by making use of a proper reweighting , as explained in ref . \n",
    "the appendix shows how to generate random numbers with the appropriate exponential distribution ( [ exponentialdistribution ] ) .      although the above simplified update scheme works , it turns out to have a poor efficiency . \n",
    "this is because of a lack of `` directionality '' : the green operator has , in average , a probability of @xmath116 to choose a left move or a right move . \n",
    "therefore the green operator propagates along the operator string like a `` drunk man '' , with a diffusion - like law . \n",
    "the basic creation and destruction processes correspond to the steps of the random walk . \n",
    "this suggests that the efficiency of the update scheme can be improved if one can force the green operator to move in the same direction for several iterations . \n",
    "next section presents a modified version of the simplified update scheme , which allows to control the mean length of the steps of the random walk , that is to say the mean number of creations and destructions in a given direction . \n",
    "the proposed directed update scheme can be considered analogous to the `` directed loop update '' used in the stochastic series expansion algorithm @xcite , which prevents a worm from going backwards . \n",
    "however the connection should not be pushed too far . \n",
    "indeed the picture of a worm whose head is evolving both in space and imaginary time accross vertices is obvious in a loop algorithm . in such algorithm , \n",
    "a creation ( or an annihilation ) operator which is represented by the head of a worm is propagated both in space and imaginary time , while an annihilation ( or a creation ) operator represented by the tail of the worm remains at rest . \n",
    "the loop ends when the head of the worm bites the tail . \n",
    "such a worm picture is not obvious in the sgf algorithm : instead of single creation or annihilation operators , it is the full green operator over the whole space that is propagated only in imaginary time . \n",
    "this creates open worldlines , thus introducing discontinuities . \n",
    "these discontinuities increase or decrease while propagating in imaginary time . \n",
    "all open ends of the worldlines are localized at the same imaginary time index . \n",
    "therefore it is actually not possible to draw step by step a worm whose head is evolving in space and imaginary time until it bites its tail . \n",
    "we present in this section a directed update scheme which is obtained by modifying slightly the simplified update scheme , thus keeping the simplicity and generality of the algorithm .      assuming that a left move is chosen \n",
    ", the green operator chooses between starting the move by a creation or a destruction . after having created ( or destroyed ) an operator , the green operator can choose to keep moving in the same direction and destroy ( or create ) with a probability @xmath117 ( or @xmath118 ) , or to stop . \n",
    "if it keeps moving , then a destruction ( or creation ) occurs , and the green operator can choose to keep moving and create ( or destroy ) with a probability @xmath118 ( or @xmath117 ) ... and so on , until it decides to stop . if the last action of the move is a creation , then a time shift is chosen . \n",
    "the organigram is represented in figure [ directedupdatescheme ] .          in order to satisfy detailed balance , in addition to the acceptance factors @xmath92 and @xmath93 , we need to determine new acceptance factors of the form @xmath119 and @xmath120 . \n",
    "we first determine the new expressions of @xmath92 and @xmath93 resulting from the directed update scheme . for @xmath92 , \n",
    "the previous probability @xmath51 has to be multiplied by the probability to stop the move after having created , @xmath121 . \n",
    "the previous probability @xmath53 has to be multiplied by the probability to stop the move after having destroyed , @xmath122 . \n",
    "we get for @xmath92 and @xmath93 the new expressions : @xmath123}{\\big\\langle\\psi_l\\big|\\hat\\mathcal g\\big|\\psi_r\\big\\rangle p(\\leftarrow)p_\\leftarrow^\\dagger(\\tau ) } \\\\                             & \\times & \\frac{p(\\rightarrow^\\prime)\\big[1-p_\\rightarrow^\\dagger(\\tau^\\prime)\\big]\\big[1-e^{-(\\tau_l^\\prime-\\tau_r^\\prime)(v_r^\\prime - v_l^\\prime)}\\big]}{\\big[1-p_\\leftarrow^{kd}(\\tau^\\prime)\\big]\\big(v_r^\\prime - v_l^\\prime\\big ) } \\\\ \n",
    "\\nonumber q_\\leftarrow^d & = &       \\frac{\\big[1-p_\\rightarrow^{kd}(\\tau)\\big]\\big(v_l - v_r\\big)}{p(\\leftarrow)\\big[1-p_\\leftarrow^\\dagger(\\tau)\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\                             & \\times & \\frac{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle p(\\rightarrow^\\prime)p_\\rightarrow^\\dagger(\\tau^\\prime)}{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle\\big[1-p_\\leftarrow^{kc}(\\tau^\\prime)\\big]},\\end{aligned}\\ ] ]      we consider here the case where a left move is chosen , an operator is created on the right of the green operator , and a new state is chosen . \n",
    "then the operator on the left of the green operator is destroyed . using the superscripts @xmath124 to denote intermediate configurations between initial and final configurations , \n",
    "the sequence is the following    1 . \n",
    "@xmath125 2 . \n",
    "@xmath126 3 . \n",
    "@xmath127 ,    where we have @xmath128 , @xmath129 , @xmath130 , and @xmath131 . \n",
    "the probability of the transition from the initial configuration to the final configuration is the probability @xmath72 to choose a left move , times the probability @xmath73 to create an operator at time @xmath43 , times the probability @xmath132 to choose the new state @xmath133 , times the probability @xmath134 to keep moving and destroy , times the probability @xmath135 to stop the move after having destroyed : @xmath136\\ ] ] the probability of the reverse move is exactly symmetric : @xmath137\\ ] ] it is important to notice that , when in the intermediate configuration @xmath7 , the time @xmath138 of the operator to the left of the green operator is equal to @xmath35 , and the time @xmath139 of the operator to the right of the green operator is equal to @xmath43 . \n",
    "thus the acceptance factor takes the form @xmath140}{\\big\\langle\\psi_l\\big|\\hat\\mathcal g\\big|\\psi_r\\big\\rangle p(\\leftarrow)p_\\leftarrow^\\dagger(\\tau ) } \\\\ \n",
    "\\nonumber                    & \\times & \\frac{e^{-\\big(\\tau_l^a-\\tau_r^a\\big)v_r^a}p_\\rightarrow^{kd}(a)}{e^{-\\big(\\tau_l^a-\\tau_r^a\\big)v_l^a}p_\\leftarrow^{kd}(a ) } \\\\                                & \\times & \\frac{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle p(\\rightarrow^\\prime)p_\\rightarrow^\\dagger(\\tau^\\prime)}{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle\\big[1-p_\\leftarrow^{kc}(\\tau^\\prime)\\big]},\\end{aligned}\\ ] ] and is written as a quantity that depends on the initial configuration , times a quantity that depends on the intermediate configuration @xmath7 , times a quantity that depends on the final configuration . \n",
    "it is useful for the remaining of the paper to define the intermediate acceptance factor , @xmath141      we consider here the case where a left move is chosen , the operator on the left of the green operator is destroyed , then an operator is created on its right , and a new state is chosen . finally a time shift is chosen . \n",
    "the sequence of configurations is the following    1 . \n",
    "@xmath125 2 . \n",
    "@xmath142 3 . \n",
    "@xmath127 ,    where we have @xmath143 , and @xmath144 . \n",
    "the probability of the transition from the initial configuration to the final configuration is the probability @xmath72 to choose a left move , times the probability @xmath145 of no creation , times the probability @xmath146 to keep moving and create , times the probability @xmath74 to choose the new state @xmath75 , times the probability @xmath121 to stop the move after having destroyed , times the probability @xmath76 to shift the green operator by @xmath77 : @xmath147p_\\leftarrow^{kc}(a)p_\\leftarrow(\\psi_r^\\prime ) \\\\                        & \\times & \\big[1-p_\\leftarrow^{kd}(\\tau^\\prime)\\big]p_\\leftarrow^{l^\\prime r^\\prime}(\\tau^\\prime-\\tau_r^\\prime)\\end{aligned}\\ ] ] \n",
    "the probability of the reverse move is exactly symmetric : @xmath148p_\\rightarrow^{kc}(a)p_\\rightarrow(\\psi_l ) \\\\                        & \\times & \\big[1-p_\\rightarrow^{kd}(\\tau)\\big]p_\\rightarrow^{lr}(\\tau_l-\\tau)\\end{aligned}\\ ] ] the acceptance factor takes the form @xmath149\\big(v_l - v_r\\big)}{p(\\leftarrow)\\big[1-p_\\leftarrow^\\dagger(\\tau)\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\ \n",
    "\\nonumber                    & \\times & \\frac{\\big\\langle\\psi_l^a\\big|\\hat\\mathcal g\\hat\\mathcal t\\big|\\psi_r^a\\big\\rangle \n",
    "p_\\rightarrow^{kc}(a)}{\\big\\langle\\psi_l^a\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^a\\big\\rangle p_\\leftarrow^{kc}(a ) } \\\\                                & \\times & \\frac{p(\\rightarrow^\\prime)\\big[1-p_\\rightarrow^\\dagger(\\tau^\\prime)\\big]\\big[1-e^{-(\\tau_l^\\prime-\\tau_r^\\prime)(v_r^\\prime - v_l^\\prime)}\\big]}{\\big[1-p_\\leftarrow^{kd}(\\tau^\\prime)\\big]\\big(v_r^\\prime - v_l^\\prime\\big)},\\end{aligned}\\ ] ] and is written as a quantity that depends on the initial configuration , times a quantity that depends on the intermediate configuration @xmath7 , times a quantity that depends on the final configuration . \n",
    "it is useful for the remaining of the paper to define the intermediate acceptance factor , @xmath150      we consider here the case where a left move is chosen , an operator is created on the right of the green operator , then the operator on its left is destroyed , then a second operator is created on its right . \n",
    "finally , a time shift of the green operator is performed . \n",
    "the sequence of configurations is the following    1 . \n",
    "@xmath125 2 . \n",
    "@xmath126 3 . \n",
    "@xmath151 4 . \n",
    "@xmath152 ,    considering the intermediate configurations @xmath7 and @xmath153 between the intial and final configurations , it is easy to show that the corresponding acceptance factor can be written @xmath154      we consider here the case where a left move is chosen , the operator on the left of the green operator is destroyed , then an operator is created on its right . finally a second operator on the left of green operator is destroyed . \n",
    "the sequence of configurations is the following    1 . \n",
    "@xmath155 2 . \n",
    "@xmath156 3 . \n",
    "@xmath157 4 . \n",
    "@xmath127 ,    considering the intermediate configurations @xmath7 and @xmath153 between the intial and final configurations , it is easy to show that the corresponding acceptance factor can be written @xmath158      it is straighforward to show that the acceptance factors of the form @xmath159 , @xmath160 , @xmath161 ( or @xmath162 , @xmath163 , @xmath164 ) can be expressed as products of the acceptance factor @xmath92 ( or @xmath93 ) and the intermediate factors @xmath165 and @xmath166 .    in the same manner , the acceptance factors of the form @xmath167 , @xmath168 , @xmath169 ( or @xmath170 , @xmath171 , @xmath172 ) can be expressed as products of the acceptance factor @xmath173 ( or @xmath174 ) and the intermediate factors @xmath165 and @xmath166 .      here \n",
    "again it is possible to take advantage of the freedom that we have for the choice of the probabilities @xmath72 , @xmath175 , @xmath118 , and @xmath117 ( or @xmath102 , @xmath176 , @xmath177 , and @xmath178 ) . \n",
    "a proper choice of these probabilities can be done in order to allow us to accept all moves , simplicity and generality being the leitmotiv of the sgf algorithm .    for this purpose \n",
    ", we impose to all acceptance factors corresponding to left ( or right ) moves to be equal . \n",
    "this requires the intermediate acceptance factors @xmath165 and @xmath166 ( or @xmath179 and @xmath180 ) to be equal to 1 . \n",
    "this is realized if @xmath181 where @xmath182 and @xmath183 are optimization parameters belonging to @xmath184 . by tuning these parameters , the mean length of the steps of the green operator \n",
    "can be controlled . \n",
    "note that we have explicitly excluded @xmath185 from the allowed values for these optimization parameters . \n",
    "this is necessary for the green operator to have a chance to end in a diagonal configuration , @xmath45 . \n",
    "indeed , the choice @xmath186 would systematically lead to values of @xmath185 for the probabilities @xmath187 and @xmath188 for diagonal configurations . \n",
    "therefore the green operator would never stop in a diagonal configution , and no measurement could be done . \n",
    "it is important here to note that the quantities @xmath96 , @xmath97 , and @xmath98 are evaluated between the states on the left and the right of the green operator that are present at the moment where those quantities are needed , as well as for the times indices @xmath189 and @xmath190 and the potentials @xmath191 and @xmath192 . \n",
    "all acceptance factors corresponding to a given direction of propagation become equal if we choose for the creation probabilities : @xmath193(v_l - v_r)}{\\big[1-p_\\rightarrow^{kc}\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } } \\\\    & & p_\\rightarrow^\\dagger(\\tau)=\\frac{\\big\\langle\\hat\\mathcal t\\hat\\mathcal g\\big\\rangle}{\\big\\langle\\hat\\mathcal t\\hat\\mathcal g\\big\\rangle+\\big\\langle\\hat\\mathcal g\\big\\rangle\\frac{\\big[1-p_\\leftarrow^{kd}\\big](v_r - v_l)}{\\big[1-p_\\leftarrow^{kc}\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_r - v_l)}\\big]}},\\end{aligned}\\ ] ] finally , all acceptances factors become independant of the direction of propagation if we choose @xmath194 and @xmath195 with @xmath196\\frac{\\big\\langle\\hat\\mathcal g\\hat\\mathcal t\\big\\rangle}{\\big\\langle\\hat\\mathcal g\\big\\rangle}+\\frac{\\big[1-p_\\rightarrow^{kd}\\big](v_l - v_r)}{\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\    r_\\rightarrow(\\tau)=\\big[1-p_\\leftarrow^{kc}\\big]\\frac{\\big\\langle\\hat\\mathcal t\\hat\\mathcal g\\big\\rangle}{\\big\\langle\\hat\\mathcal g\\big\\rangle}+\\frac{\\big[1-p_\\leftarrow^{kd}\\big](v_r - v_l)}{\\big[1-e^{-(\\tau_l-\\tau_r)(v_r - v_l)}\\big]}.\\end{aligned}\\ ] ] as a result all moves can be accepted again , ensuring the maximum of simplicity of the algorithm . \n",
    "we still have some freedom for the choice of the optimization parameters @xmath182 and @xmath183 . \n",
    "this is discussed in next section . \n",
    "from the central limit theorem , we know that the errorbar associated to any measured quantity must decrease as the square root of the number of measurements , or equivalently , the square root of the time of the simulation \n",
    ". therefore it makes sense to define the efficiency @xmath197 of a qmc algorithm by @xmath198 where @xmath199 represents the set of all optimization parameters of the algorithm , @xmath200 is the measured quantity of interest , @xmath201 is the time of the simulation , and @xmath202 is the errorbar associated to the measured quantity @xmath200 . \n",
    "this definition ensures that @xmath197 is independent of the time of the simulation . as a result , \n",
    "the larger @xmath197 the more efficient the algorithm .    in the present case \n",
    "we have @xmath203 , while @xmath204 for the original sgf algorithm . \n",
    "it is useful here to realize that , by symmetry , the mean values of @xmath118 and @xmath177 ( and @xmath117 and @xmath178 ) must be equal . \n",
    "therefore we define @xmath205 and @xmath206 . \n",
    "it seems reasonable to impose a condition of uniform sampling , @xmath207 . \n",
    "this condition can be satisfied by adjusting dynamically the values of @xmath182 and @xmath183 during the thermalization process . for this purpose \n",
    "we introduce a new optimization parameter @xmath208 and apply the following algorithm from time to time while thermalizing ( we start with @xmath209 ) : @xmath210 thus we are left with the optimization parameter @xmath211 . in order to determine the optimal value , \n",
    "we have considered 2 different hamiltonians @xmath212 and @xmath213 , and evaluated the efficiency of the algorithm while scanning @xmath211 . \n",
    "the first hamiltonian we have considered describes free hardcore bosons and is exactly solvable , @xmath214 where the sum runs over pairs of first neighboring sites and @xmath215 is the hopping parameter . \n",
    "the second hamiltonian is highly non - trivial and describes a mixture of atoms and diatomic molecules , with a special term allowing conversions between the two species @xcite , @xmath216 where @xmath217 and @xmath218 ( @xmath219 and @xmath220 ) are the creation and annihilation operators of atoms ( molecules ) , @xmath221 , @xmath222 , @xmath223 , @xmath224 , and @xmath225 are respectively the hopping parameter of atoms , the hopping parameter of molecules , the atomic onsite interaction parameter , the molecular onsite interaction parameter , and the inter - species interaction parameter . \n",
    "the conversion term is tunable via the parameter @xmath226 and does not conserve the number @xmath227 of atoms or the number @xmath228 of molecules \n",
    ". however the total number of particles @xmath229 is conserved and is the canonical constraint . \n",
    "the parameter @xmath230 allows to control the ratio between the number of atoms and molecules . \n",
    "the application of the sgf algorithm to the hamiltonian ( [ twospecies ] ) is described in details in ref.@xcite . \n",
    "the changes coming with the directed update scheme are completely independent of the chosen hamiltonian . \n",
    "the following table shows the mean number of creations and destructions in one step , @xmath231 , and the relative efficiency @xmath232 of the algorithm applied to @xmath212 at half filling , for which we have measured the energy @xmath233 , the superfluid density @xmath234 , and the number of particles in the zero momentum state @xmath235 :    . \n",
    "relative efficiency of the algorithm applied to @xmath212 at half filling for the energy , the superfluid density , and the number of particles in the zero momentum state . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     while the best value of @xmath211 depends on the hamiltonian which is considered and the measured quantity , it appears that a good compromise is to choose @xmath211 between @xmath236 and @xmath237 . \n",
    "the improvment of the efficiency is remarkable . in the following , \n",
    "we illustrate the applicability of the algorithm to problems with non - uniform potentials , by adding a parabolic trap to the hamiltonian ( [ twospecies ] ) : @xmath238 the parameters @xmath239 and @xmath240 allow to control the curvature of the trap associated to atoms and molecules , respectively , and @xmath31 is the number of lattice sites . \n",
    "the inclusion of this term in the algorithm is trivial since only the values of the diagonal energies @xmath241 and @xmath242 are changed . \n",
    "figures ( [ density ] ) and ( [ momentum ] ) show the density profiles and momentum distribution functions obtained for a system with @xmath243 lattice sites initially loaded with @xmath244 atoms and no molecules , and the parameters @xmath245 , @xmath246 , @xmath247 , @xmath248 , @xmath249 , @xmath250 , @xmath251 , @xmath252 , @xmath253 , and @xmath254 . \n",
    "the presented results have been obtained by performing @xmath255 updates for thermalization , and @xmath256 updates with measurements ( an update is to be understood as the occurence of a diagonal configuration ) . \n",
    "the time of the simulation is about 8 hours on a cheap 32 bits laptop with 1ghz processor , with an implementation of the algorithm involving dynamical structures with pointers ( see ref.@xcite ) .    ) to the hamiltonian ( [ twospecies ] ) . \n",
    "the errorbars are smaller than the symbol sizes , and are the biggest in the neighborhood of site indices 23 and 47 where they equal the size of the symbols . , scaledwidth=45.0% ]    ) to the hamiltonian ( [ twospecies ] ) . \n",
    "the errorbars are smaller than the symbol sizes , and are the biggest for @xmath257 where they equal the size of the symbols . \n",
    ", scaledwidth=45.0% ] \n",
    "we have presented a directed update scheme for the sgf algorithm , which has the properties of keeping the simplicity and generality of the original algorithm , and improves significantly its efficiency . \n",
    "i would like to express special thanks to peter denteneer for useful suggestions . \n",
    "this work is part of the research program of the `` stichting voor fundamenteel onderzoek der materie ( fom ) , '' which is financially supported by the `` nederlandse organisatie voor wetenschappelijk onderzoek ( nwo ) . '' \n",
    "we describe here how to generate numbers with the appropriate exponential distribution ( [ exponentialdistribution ] ) . assuming that we have at our disposal a uniform random number generator that generates a random variable @xmath258 with the distribution @xmath259 for @xmath260 \n",
    ", we would like to find a function @xmath52 such that the random variable @xmath261 is generated with the distribution @xmath262 where @xmath46 and @xmath263 are the parameters of the exponential distribution . \n",
    "because of the relation @xmath261 , the probability to find @xmath264 in the range @xmath265 must be equal to the probability to find @xmath258 in the range @xmath266 . \n",
    "this implies the condition @xmath267 with @xmath268 . \n",
    "thus we have @xmath269 taking the anti - derivative with respect to @xmath270 on both sides of the equation , we get @xmath271 where @xmath272 is a constant . this constant and the correct sign are determined by imposing the conditions @xmath273 and @xmath274 . as a result , if @xmath270 is a realization of @xmath258 , then a realization of @xmath264 is given by @xmath275.\\ ] ]    10 nicholas metropolis and s. ulam , journal of the american statistical association , number 247 , volume 44 ( 1949 ) . \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = '''\n",
    "in a recent publication we have presented the stochastic green function ( sgf ) algorithm , which has the properties of being general and easy to apply to any lattice hamiltonian of the form @xmath0 , where @xmath1 is diagonal in the chosen occupation number basis and @xmath2 has only positive matrix elements . \n",
    " we propose here a modified version of the update scheme that keeps the simplicity and generality of the original sgf algorithm , and enhances significantly its efficiency .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 6.268149073348561e-41, 'precisions': [0.9764705882352941, 0.8333333333333334, 0.6746987951807228, 0.5487804878048781], 'brevity_penalty': 8.460434887694446e-41, 'length_ratio': 0.010721493440968718, 'translation_length': 85, 'reference_length': 7928}\n",
      "BLEU = 0.00 97.6/83.3/67.5/54.9 (BP = 0.000 ratio = 0.011 hyp_len = 85 ref_len = 7928)\n"
     ]
    }
   ],
   "source": [
    "bleu_results = bleu.compute(predictions=[abstract], references=[body])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[body]], hypotheses=[abstract], smooth_method='exp')\n",
    "print(sacre_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.2831739829477766, 'precisions': [0.9411764705882353, 0.6363636363636364, 0.4375, 0.25806451612903225], 'brevity_penalty': 0.5553063730019506, 'length_ratio': 0.6296296296296297, 'translation_length': 34, 'reference_length': 54}\n",
      "BLEU = 28.32 94.1/63.6/43.8/25.8 (BP = 0.555 ratio = 0.630 hyp_len = 34 ref_len = 54)\n"
     ]
    }
   ],
   "source": [
    "reference_sum = \"The advances in renewable energy technologies have significantly reduced the world's reliance on fossil fuels. Solar power, wind energy, and hydroelectric systems are becoming more efficient and cost-effective, leading to increased adoption worldwide. This shift is crucial in combating climate change and promoting sustainable development for future generations.\"\n",
    "generated_sum = \"Advances in renewable energy have reduced reliance on fossil fuels. Solar and wind energy are becoming more efficient, leading to adoption worldwide. This shift is important in combating climate change.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[generated_sum], references=[reference_sum])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference_sum]], hypotheses=[generated_sum], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_text_1 = '''\n",
    "In the ancient lands of Eladoria, a civilization flourished unlike any the world had ever seen. Nestled between towering mountains and vast oceans, the Eladorians harnessed the power of the elements to build cities that touched the skies. Their architects carved palaces from crystal, and their scholars unlocked the secrets of the stars. The streets of the capital, Luminescence, bustled with merchants trading spices, silks, and knowledge. At the heart of the city stood the Grand Library, a repository of all human wisdom, where sages debated the mysteries of existence under the glow of eternal lanterns. \n",
    "\n",
    "The Eladorians were not just masters of the physical realm but also of the mystical. They practiced arts that allowed them to communicate with the natural world, to listen to the whispers of the wind and the songs of the sea. This harmony with nature brought about an era of unprecedented peace and prosperity. Crops grew abundant, diseases were few, and the people lived long, fulfilled lives. Their influence spread across continents as they shared their advancements with neighboring nations, fostering a global community united by progress and mutual respect. \n",
    "\n",
    "However, beneath the surface of this utopia, shadows began to form. A faction emerged that sought to exploit the elemental powers for control rather than harmony. Led by the ambitious Lord Kael, they delved into forbidden practices, manipulating the forces of nature to bend others to their will. As tensions rose, the once-unified Eladorian society fractured. Debates in the Grand Library turned to heated arguments, and the skies over Luminescence grew dark with ominous clouds. \n",
    "\n",
    "The final straw came when Lord Kael attempted a ritual to seize dominion over all elements. The ritual backfired catastrophically, unleashing storms, earthquakes, and infernos across Eladoria. The great cities crumbled, and the land was scarred beyond recognition. Survivors fled, carrying with them tales of both the grandeur and the hubris of their people. Over generations, Eladoria faded into myth, its magnificent achievements buried under layers of time. \n",
    "\n",
    "Yet, the echoes of Eladoria endured. Forgotten ruins whispered secrets to those who would listen, and fragments of their knowledge resurfaced in distant cultures. The story of Eladoria became a cautionary tale about the balance between advancement and humility, reminding the world that true greatness lies not just in what is built, but in the wisdom to use it wisely. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_text_2 = '''\n",
    "In the distant future, humanity embarked on its most ambitious endeavor: the voyage of the starship Astravia. Engineered over a century, Astravia was a marvel of technology, capable of traversing galaxies. Its mission was to explore uncharted regions of the universe, seeking new worlds and civilizations. Aboard were thousands of explorers, scientists, and dreamers, all united by a singular purpose—to expand the horizons of human understanding. \n",
    "\n",
    "Astravia's journey began with wonder. The crew witnessed nebulae of indescribable beauty, stars being born and dying, and planets with landscapes that defied imagination. They encountered phenomena that challenged the very laws of physics, prompting revolutionary advancements in science and philosophy. Communities formed within the ship, reflecting a microcosm of Earth’s diversity, yet bonded by shared experiences millions of light-years from home. \n",
    "\n",
    "As years turned into decades, the initial awe gave way to introspection. Generations were born who had never felt Earth's gravity or seen its skies. Questions arose about identity and purpose. What did it mean to be human so far from their origin? Artistic expressions flourished, capturing the complexities of life aboard Astravia. Music, literature, and visual arts became the soul of the ship, weaving a cultural tapestry that was both a tribute to Earth and a celebration of their unique journey. \n",
    "\n",
    "However, the vastness of space also brought unforeseen challenges. The ship encountered regions of dark matter that disrupted navigation systems, and unknown cosmic rays threatened the health of the crew. A division emerged between those who wished to continue forward and those who advocated for a return to Earth. Captain Elara, steadfast in her commitment, sought to unify the crew, emphasizing the mission's importance not just for themselves but for all humanity. \n",
    "\n",
    "The turning point came when Astravia received a faint signal from an intelligent source. Excitement and anxiety gripped the ship as they deciphered the message. It was an invitation to a distant star system, promising knowledge and alliance. The crew faced a pivotal decision: to alter their course towards the unknown or to chart a path back home with the wealth of experiences they had gathered. \n",
    "\n",
    "In the end, they chose to embrace the unknown, honoring the spirit of exploration that had launched their journey. As Astravia set a new course, the crew prepared for what lay ahead, their personal odyssey intertwined with humanity's collective quest for meaning. Their story became a beacon, illustrating that exploration is not just about discovering new frontiers, but also about understanding ourselves and our place in the cosmos. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.46275395033860045, 0.083710407239819, 0.0022675736961451248, 0.0], 'brevity_penalty': 0.9366339320239014, 'length_ratio': 0.9385593220338984, 'translation_length': 443, 'reference_length': 472}\n",
      "BLEU = 1.66 46.3/8.4/0.2/0.1 (BP = 0.937 ratio = 0.939 hyp_len = 443 ref_len = 472)\n"
     ]
    }
   ],
   "source": [
    "bleu_results = bleu.compute(predictions=[large_text_1], references=[large_text_2])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[large_text_2]], hypotheses=[large_text_1], smooth_method='exp')\n",
    "print(sacre_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from BLUE paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 4, 'reference_length': 4}\n",
      "BLEU = 100.00 100.0/100.0/100.0/100.0 (BP = 1.000 ratio = 1.000 hyp_len = 4 ref_len = 4)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"I always do.\"\n",
    "references = [\"I always do.\", \"I invariably do.\", \"I perpetually do.\"]\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[references])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[references], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.345720784641941, 'precisions': [0.5, 0.42857142857142855, 0.3333333333333333, 0.2], 'brevity_penalty': 1.0, 'length_ratio': 2.0, 'translation_length': 8, 'reference_length': 4}\n",
      "BLEU = 34.57 50.0/42.9/33.3/20.0 (BP = 1.000 ratio = 2.000 hyp_len = 8 ref_len = 4)\n"
     ]
    }
   ],
   "source": [
    "reference = \"what what what what\"\n",
    "prediction = \"what what what what what what what what\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.36787944117144233, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 0.36787944117144233, 'length_ratio': 0.5, 'translation_length': 4, 'reference_length': 8}\n",
      "BLEU = 36.79 100.0/100.0/100.0/100.0 (BP = 0.368 ratio = 0.500 hyp_len = 4 ref_len = 8)\n"
     ]
    }
   ],
   "source": [
    "prediction = \"what what what what\"\n",
    "reference = \"what what what what what what what what\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe save this output in a database or some idk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.875), 'rouge2': np.float64(0.7142857142857143), 'rougeL': np.float64(0.875), 'rougeLsum': np.float64(0.875)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to accept the generous offer.\"\n",
    "candidate = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym - Larger text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.9962406015037594), 'rouge2': np.float64(0.9924528301886792), 'rougeL': np.float64(0.9962406015037594), 'rougeLsum': np.float64(0.9962406015037594)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     Their impact spans various fields such as education, software engineering, and research. She was happy to accept the generous offer.\"\n",
    "candidate = \"It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     Their impact spans various fields such as education, software engineering, and research.  She was happy to accept the kind offer.\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0), 'rougeLsum': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"-\"\n",
    "candidate = \"-\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(1.0), 'rouge2': np.float64(1.0), 'rougeL': np.float64(1.0), 'rougeLsum': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to-accept the kind offer.\"\n",
    "candidate = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(1.0), 'rouge2': np.float64(1.0), 'rougeL': np.float64(1.0), 'rougeLsum': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to - accept the kind offer.\"\n",
    "candidate = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.823529411764706), 'rouge2': np.float64(0.6666666666666666), 'rougeL': np.float64(0.823529411764706), 'rougeLsum': np.float64(0.823529411764706)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy t/;,/?#$@!&^%*()+o accept the kind offer\"\n",
    "candidate = \"She was happy to accept the kind offer\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(1.0), 'rouge2': np.float64(1.0), 'rougeL': np.float64(1.0), 'rougeLsum': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"functionName( type param ):\"\n",
    "candidate = \"functionName( type param ):\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.5), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.5), 'rougeLsum': np.float64(0.5)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"o/k\"\n",
    "candidate = \"o/j\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.5), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.5), 'rougeLsum': np.float64(0.5)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"o/ k\"\n",
    "candidate = \"o/j\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0), 'rougeLsum': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"ok\"\n",
    "candidate = \"o/j\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(1.0), 'rouge2': np.float64(1.0), 'rougeL': np.float64(1.0), 'rougeLsum': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to /;,/?#$@!&^%*()+ accept the kind offer.\"\n",
    "candidate = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols at start of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(1.0), 'rouge2': np.float64(1.0), 'rougeL': np.float64(1.0), 'rougeLsum': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy +to accept the kind offer\"\n",
    "candidate = \"She was happy to accept the kind offer\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols after word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(1.0), 'rouge2': np.float64(1.0), 'rougeL': np.float64(1.0), 'rougeLsum': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to+ accept the kind offer\"\n",
    "candidate = \"She was happy to accept the kind offer\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(1.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(1.0), 'rougeLsum': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"0\"\n",
    "candidate = \"0\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.6666666666666666), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.6666666666666666), 'rougeLsum': np.float64(0.6666666666666666)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"0\"\n",
    "candidate = \"123 0\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(1.0), 'rouge2': np.float64(1.0), 'rougeL': np.float64(1.0), 'rougeLsum': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"higher values\"\n",
    "candidate = \"HIGHER VALUES\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order + Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(1.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.5), 'rougeLsum': np.float64(0.5)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"higher values\"\n",
    "candidate = \"VALUES higher\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.8), 'rouge2': np.float64(0.6666666666666666), 'rougeL': np.float64(0.8), 'rougeLsum': np.float64(0.8)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"higher values\"\n",
    "candidate = \"values higher values\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper + abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/0l/m9q4hqlx7j95m4nz7kp_6fmc0000gp/T/ipykernel_47912/3720519415.py:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  body = '''\n"
     ]
    }
   ],
   "source": [
    "body = '''\n",
    "monte carlo methods @xcite appeared about sixty years ago with the need to evaluate numerical values for various complex problems . \n",
    "these methods evolved and were applied early to quantum problems , thus putting within reach exact numerical solutions to non - trivial quantum problems @xcite . \n",
    "many improvements of these methods followed , avoiding critical slowing down near phase transitions and allowing to work directly in the continuous imaginary time limit @xcite . in recent years , \n",
    "interest in methods that work in the canonical ensemble with global updates yet allow access to green functions has intensified @xcite . \n",
    "however , a method that works well for a given hamiltonian often needs major modifications for another . \n",
    "for example , the addition of a 4-site ring exchange term in the bosonic hubbard model required special developments for a treatment by the stochastic series expansion algorithm @xcite , as well as by the wordline algorithm @xcite . \n",
    "this can result in long delays . \n",
    "it is , therefore , advantageous to have at one s disposal an algorithm that can be applied to a very wide class of hamiltonians without requiring any changes . in a recent publication @xcite , \n",
    "the stochastic green function ( sgf ) algorithm was presented , which meets this goal . \n",
    "the algorithm can be applied to any lattice hamiltonian of the form @xmath3 where @xmath1 is diagonal in the chosen occupation number basis and @xmath2 has only positive matrix elements . \n",
    "this includes all kinds of systems that can be treated by other methods presented in ref.@xcite , for instance bose - hubbard models with or without a trap , bose - fermi mixtures in one dimension , heisenberg models ... in particular hamiltonians for which the non - diagonal part @xmath2 is non - trivial ( the eigen - basis is unknown ) are easily treated , such as the bose - hubbard model with ring exchange @xcite , or multi - species hamiltonians in which a given species can be turned into another one ( see eq.([twospecies ] ) and fig . \n",
    "[ density ] and [ momentum ] for a concrete example ) . \n",
    "systems for which it is not possible to find a basis in which @xmath1 is diagonal and @xmath2 has only positive matrix elements are said to have a `` sign problem '' , which usually arises with fermionic and frustrated systems . as other qmc methods , the sgf algorithm does not solve this problem .    the algorithm allows to measure several quantities of interest , such as the energy , the local density , local compressibility , density - density correlation functions ... in particular the winding is sampled and gives access to the superfluid density . \n",
    "equal - time n - body green functions are probably the most interesting quantities that can be measured by the algorithm , by giving access to momentum distribution functions which allow direct comparisons with experiments . \n",
    "all details on measurements are given in ref.@xcite . \n",
    "in addition the algorithm has the property of being easy to code , due in part to a simple update scheme in which all moves are accepted with a probability of 1 . despite of such generality and simplicity , \n",
    "the algorithm might suffer from a reduced efficiency , compared to other algorithms in situations where they can be applied . \n",
    "the purpose of this paper is to present a `` directed '' update scheme that ( i ) keeps the simplicity and generality of the original sgf algorithm , and ( ii ) enhances its efficiency by improving the sampling over the imaginary time axis . \n",
    "while the sgf algorithm is not intended to compete with the speed of other algorithms , the improvment resulting from the directed update scheme is remarkable ( see section v ) . \n",
    "but what makes the strength of the sgf method is that it allows to simulate hamiltonians that can not be treated by other methods or that would require special developments ( see eq.([twospecies ] ) for a concrete example ) . \n",
    "the paper is organized as follows : we introduce in section ii the notations and definitions used in ref.@xcite . in section iii , we propose a simplification of the update scheme used in the original sgf algorithm , and determine how to satisfy detailed balance . \n",
    "a generalization of the simplified update scheme is presented in section iv , which constitutes the directed updated scheme . \n",
    "finally section v shows how to determine the introduced optimization parameters , and presents some tests of the algorithm and a comparison with the original version . \n",
    "in this section , we recall the expression of the `` green operator '' introduced in the sgf algorithm , and the extended partition function which is considered . although not required for understanding this paper , we refer the reader to ref.@xcite for full details on the algorithm . as many qmc algorithms , \n",
    "the sgf algorithm samples the partition function @xmath4 the algorithm has the property of working in the canonical ensemble . in order to define the green operator , \n",
    "we first define the `` normalized '' creation and annihilation operators , @xmath5 where @xmath6 and @xmath7 are the usual creation and annihilation operators of bosons , and @xmath8 is the number operator . from ( [ normalizedoperators ] ) \n",
    "one can show the following relations for any state @xmath9 in the occupation number representation , @xmath10 with the particular case @xmath11 . \n",
    "appart from this exception , the operators @xmath12 and @xmath13 change a state @xmath9 by respectively creating and annihilating one particle , but they do not change the norm of the state .    using the notation @xmath14 to denote two subsets of site indices @xmath15 and @xmath16 with the constraint that all indices in subset @xmath17 are different from the indices in subset @xmath18 ( but several indices in one subset may be equal ) , we define the green operator @xmath19 by @xmath20 where @xmath21 is a matrix that depends on the application of the algorithm @xcite . in order to sample the partition function ( [ partitionfunction ] ) , an extended partition function @xmath22 is considered by breaking up the propagator @xmath23 , and introducing the green operator between the broken parts , @xmath24 defining the time dependant operators @xmath25 and @xmath26 \n",
    ", @xmath27 and working in the occupation number basis in which @xmath1 is diagonal , the extended partition function takes the form @xmath28 where the sum @xmath29 implicitly runs over complete sets of states @xmath30 . we will systematically use the labels @xmath31 and @xmath32 to denote the states appearing on the left and the right of the green operator , and use the notation @xmath33 to denote the diagonal energy @xmath34 . \n",
    "we will also denote by @xmath35 and @xmath36 the time indices of the @xmath2 operators appearing on the left and the right of @xmath19 .    as a result , \n",
    "the extended partition function is a sum over all possible configurations , each being determined by a set of time indices @xmath37 and a set of states @xmath38 , @xmath39 , @xmath40,@xmath41 , @xmath42 . \n",
    "the algorithm consists in updating those configurations by making use of the green operator . assuming that the green operator is acting at time @xmath43 \n",
    ", it can `` create '' a @xmath2 operator ( that is to say a @xmath2 operator can be inserted in the operator string ) at the same time , thus introducing a new intermediate state , then it can be shifted to a different time . while shifting , any @xmath2 operator encountered by the green operator is `` destroyed '' ( that is to say removed from the operator string ) . assuming a left ( or right ) move , creating an operator will update the state @xmath44 ( or @xmath41 ) , \n",
    "while destroying will update the state @xmath41 ( or @xmath44 ) . \n",
    "when a diagonal configuration of the green operator occurs , @xmath45 , such a configuration associated to the extended partition function ( [ extendedpartitionfunction ] ) is also a configuration associated to the partition function ( [ partitionfunction ] ) . \n",
    "measurements can be done when this occurs ( see ref.@xcite for details on measurements ) . \n",
    "next section presents a simple update scheme that meets the requirements of ergodicity and detailed balance . \n",
    "before introducing the directed update , we start by simplifying the update scheme used in the original sgf algorithm \n",
    ".      we will assume in the following that a left move of the green operator is chosen . \n",
    "in the original version , the green operator @xmath26 can choose to create or not on its right a @xmath2 operator at time @xmath43 . \n",
    "then a time shift @xmath46 to the left is chosen for the green operator with an exponential distribution in the range @xmath47 . \n",
    "if an operator is encountered while shifting the green operator , then the operator is destroyed and the move stops there . as a result , four possible situations can occur during one move :    1 .   no creation , shift , no destruction . \n",
    "2 .   creation , shift , no destruction . \n",
    "3 .   no creation , shift , destruction . \n",
    "4 .   creation , shift , destruction . \n",
    "it appears that the first possibility `` no creation , no destruction '' is actually useless , since no change is performed in the operator string . the idea is to get rid of this possibility by forcing the green operator to destroy an operator if no creation is chosen \n",
    "a further simplification can be done by noticing that the last possibility `` creation , destruction '' is not necessary for the ergodicity of the algorithm , and can be avoided by restricting the range of the time shift after having created an operator . \n",
    "therefore we replace the original update scheme by the following : we assume that the green operator is acting at time @xmath43 and that the operator on its left is acting at time @xmath35 . the green operator @xmath26 chooses to create or not an operator on its right at time @xmath43 . \n",
    "if creation is chosen , then a time shift @xmath46 of the green operator is chosen to the left in the range @xmath48 , with the probability distribution defined below . \n",
    "if no creation is chosen , then the green operator is directly shifted to the operator on its left at time @xmath35 , and the operator is destroyed . as a result \n",
    "only two possibilities have to be considered :    1 . \n",
    "creation , shift . \n",
    "2 .   shift , destruction . \n",
    "figure [ simplfiedupdatescheme ] shows the associated organigram . \n",
    "section iii.b explains how detailed balance can be satisfied with this simplified update scheme .          when updating the configurations according to the chosen update scheme , we need to generate different transitions from initial to final states with probabilities that satisfy detailed balance . in this section \n",
    "we propose a choice for these probabilities , and determine the corresponding acceptance factors . \n",
    "we denote the probability of the initial ( final ) configuration by @xmath49 ( @xmath50 ) . \n",
    "we denote by @xmath51 the probability of the transition from configuration @xmath17 to configuration @xmath52 , and by @xmath53 the probability of the reverse transition . \n",
    "finally we denote by @xmath54 the acceptance rate of the transition from @xmath17 to @xmath52 , and by @xmath55 the acceptance rate of the reverse transition . \n",
    "the detailed balance can be written as @xmath56 we will make use of the metropolis solution @xcite , @xmath57 with @xmath58 we will use primed ( non - primed ) labels for states and time indices to denote final ( initial ) configurations . \n",
    "we consider here the case where a left move is chosen , an operator is created on the right of the green operator at time @xmath43 , and a new state is chosen . \n",
    "then a time shift to the left is chosen for the green operator in the range @xmath59 . \n",
    "it is important to note that @xmath60 and @xmath61 correspond to the time indices of the operators appearing on the left and the right of the green operator after the new operator has been inserted , that is to say at the moment where the time shift needs to be performed . \n",
    "thus we have @xmath62 and @xmath63 . \n",
    "the probability of the initial configuration is the boltzmann weight appearing in the extended partition function ( [ extendedpartitionfunction ] ) : @xmath64 the probability of the final configuration takes the form : @xmath65 it is important here to realize that the green operator only inserted on its right the operator @xmath66 , before being shifted from @xmath61 to @xmath67 . \n",
    "therefore we have the equalities @xmath68 , @xmath69 , @xmath70 , and @xmath71 . \n",
    "the probability @xmath51 of the transition from the initial configuration to the final configuration is the probability @xmath72 of a left move , times the probability @xmath73 of a creation , times the probability @xmath74 to choose the new state @xmath75 , times the probability @xmath76 to shift the green operator by @xmath77 , knowing that the states on the left and the right of the green operator at the moment of the shift are @xmath78 and @xmath79 : @xmath80 the probability of the reverse transition is simply the probability @xmath81 of a right move , times the probability of no creation , @xmath82 : @xmath83\\ ] ] from the original version of the sgf algorithm , we know that choosing the time shift with an exponential distribution is a good choice , because it cancels the exponentials appearing in the probabilities of the initial ( [ initial ] ) and final ( [ final ] ) configurations , avoiding exponentially small acceptance factors . \n",
    "however a different normalization must be used here , since the time shift is chosen in the range @xmath84 instead of @xmath47 . \n",
    "the suitable solution is : @xmath85 it is straightforward to check that the above probability is correctly normalized and well - defined for any real value of @xmath86 , the particular case @xmath87 reducing to the uniform distribution @xmath88 ( note that @xmath89 is always a positive number ) . for the probability @xmath74 to choose the new state @xmath75 , \n",
    "the convenient solution is the same as in the original version : @xmath90 putting everything together , the acceptance factor ( [ metropolis2 ] ) becomes @xmath91\\big[1-e^{-(\\tau_l^\\prime-\\tau_r^\\prime)(v_r^\\prime - v_l^\\prime)}\\big]}{v_r^\\prime - v_l^\\prime},\\end{aligned}\\ ] ] where we have used the notation @xmath92 to emphasize that this acceptance factor corresponds to a creation . \n",
    "it is also important for the remaining of this paper to note that @xmath92 is written as a quantity that depends on the initial configuration , times a quantity that depends on the final configuration . \n",
    "we consider here the case where a left move is chosen , and the operator on the left of the green operator is destroyed . \n",
    "this move corresponds to the inverse of the above `` creation , shift '' move . \n",
    "thus , the corresponding acceptance factor @xmath93 is obtained by inverting the acceptance factor @xmath92 , exchanging the initial time @xmath43 and final time @xmath67 , and switching the direction . \n",
    "however @xmath94 represents an absolute time shift , so @xmath35 and @xmath36 do not have to be exchanged . \n",
    "we get @xmath95\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\                             & \\times & \\frac{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle p(\\rightarrow^\\prime)p_\\rightarrow^\\dagger(\\tau^\\prime)}{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle},\\end{aligned}\\ ] ] which is written as a quantity that depends on the initial configuration , times a quantity that depends on the final configuration . \n",
    "we will use here the short notation @xmath96 , @xmath97 , and @xmath98 to denote respectively the quantities @xmath99 , @xmath100 , and @xmath101 . as in ref . \n",
    "@xcite , we have some freedom for the choice of the probabilities of choosing a left or right move , @xmath72 and @xmath102 , and the probabilities of creation @xmath73 and @xmath103 . \n",
    "a suitable choice for those probabilities can be done in order to accept all moves , resulting in an appreciable simplification of the algorithm . for this purpose , \n",
    "we impose the acceptance factor @xmath92 ( or @xmath104 ) to be equal to the acceptance factor @xmath93 ( or @xmath105 ) . \n",
    "this allows to determine the probabilities @xmath73 and @xmath103 , @xmath106 and the acceptance factors @xmath107 and @xmath108 take the form @xmath109 with @xmath110 finally we can impose the acceptance factors @xmath111 and @xmath112 to be equal . \n",
    "this implies @xmath113 defining @xmath114 , we are left with a single acceptance factor , @xmath115 which is independent of the chosen direction , and independent of the nature of the move ( creation or destruction ) . \n",
    "thus all moves can be accepted by making use of a proper reweighting , as explained in ref . \n",
    "the appendix shows how to generate random numbers with the appropriate exponential distribution ( [ exponentialdistribution ] ) .      although the above simplified update scheme works , it turns out to have a poor efficiency . \n",
    "this is because of a lack of `` directionality '' : the green operator has , in average , a probability of @xmath116 to choose a left move or a right move . \n",
    "therefore the green operator propagates along the operator string like a `` drunk man '' , with a diffusion - like law . \n",
    "the basic creation and destruction processes correspond to the steps of the random walk . \n",
    "this suggests that the efficiency of the update scheme can be improved if one can force the green operator to move in the same direction for several iterations . \n",
    "next section presents a modified version of the simplified update scheme , which allows to control the mean length of the steps of the random walk , that is to say the mean number of creations and destructions in a given direction . \n",
    "the proposed directed update scheme can be considered analogous to the `` directed loop update '' used in the stochastic series expansion algorithm @xcite , which prevents a worm from going backwards . \n",
    "however the connection should not be pushed too far . \n",
    "indeed the picture of a worm whose head is evolving both in space and imaginary time accross vertices is obvious in a loop algorithm . in such algorithm , \n",
    "a creation ( or an annihilation ) operator which is represented by the head of a worm is propagated both in space and imaginary time , while an annihilation ( or a creation ) operator represented by the tail of the worm remains at rest . \n",
    "the loop ends when the head of the worm bites the tail . \n",
    "such a worm picture is not obvious in the sgf algorithm : instead of single creation or annihilation operators , it is the full green operator over the whole space that is propagated only in imaginary time . \n",
    "this creates open worldlines , thus introducing discontinuities . \n",
    "these discontinuities increase or decrease while propagating in imaginary time . \n",
    "all open ends of the worldlines are localized at the same imaginary time index . \n",
    "therefore it is actually not possible to draw step by step a worm whose head is evolving in space and imaginary time until it bites its tail . \n",
    "we present in this section a directed update scheme which is obtained by modifying slightly the simplified update scheme , thus keeping the simplicity and generality of the algorithm .      assuming that a left move is chosen \n",
    ", the green operator chooses between starting the move by a creation or a destruction . after having created ( or destroyed ) an operator , the green operator can choose to keep moving in the same direction and destroy ( or create ) with a probability @xmath117 ( or @xmath118 ) , or to stop . \n",
    "if it keeps moving , then a destruction ( or creation ) occurs , and the green operator can choose to keep moving and create ( or destroy ) with a probability @xmath118 ( or @xmath117 ) ... and so on , until it decides to stop . if the last action of the move is a creation , then a time shift is chosen . \n",
    "the organigram is represented in figure [ directedupdatescheme ] .          in order to satisfy detailed balance , in addition to the acceptance factors @xmath92 and @xmath93 , we need to determine new acceptance factors of the form @xmath119 and @xmath120 . \n",
    "we first determine the new expressions of @xmath92 and @xmath93 resulting from the directed update scheme . for @xmath92 , \n",
    "the previous probability @xmath51 has to be multiplied by the probability to stop the move after having created , @xmath121 . \n",
    "the previous probability @xmath53 has to be multiplied by the probability to stop the move after having destroyed , @xmath122 . \n",
    "we get for @xmath92 and @xmath93 the new expressions : @xmath123}{\\big\\langle\\psi_l\\big|\\hat\\mathcal g\\big|\\psi_r\\big\\rangle p(\\leftarrow)p_\\leftarrow^\\dagger(\\tau ) } \\\\                             & \\times & \\frac{p(\\rightarrow^\\prime)\\big[1-p_\\rightarrow^\\dagger(\\tau^\\prime)\\big]\\big[1-e^{-(\\tau_l^\\prime-\\tau_r^\\prime)(v_r^\\prime - v_l^\\prime)}\\big]}{\\big[1-p_\\leftarrow^{kd}(\\tau^\\prime)\\big]\\big(v_r^\\prime - v_l^\\prime\\big ) } \\\\ \n",
    "\\nonumber q_\\leftarrow^d & = &       \\frac{\\big[1-p_\\rightarrow^{kd}(\\tau)\\big]\\big(v_l - v_r\\big)}{p(\\leftarrow)\\big[1-p_\\leftarrow^\\dagger(\\tau)\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\                             & \\times & \\frac{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle p(\\rightarrow^\\prime)p_\\rightarrow^\\dagger(\\tau^\\prime)}{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle\\big[1-p_\\leftarrow^{kc}(\\tau^\\prime)\\big]},\\end{aligned}\\ ] ]      we consider here the case where a left move is chosen , an operator is created on the right of the green operator , and a new state is chosen . \n",
    "then the operator on the left of the green operator is destroyed . using the superscripts @xmath124 to denote intermediate configurations between initial and final configurations , \n",
    "the sequence is the following    1 . \n",
    "@xmath125 2 . \n",
    "@xmath126 3 . \n",
    "@xmath127 ,    where we have @xmath128 , @xmath129 , @xmath130 , and @xmath131 . \n",
    "the probability of the transition from the initial configuration to the final configuration is the probability @xmath72 to choose a left move , times the probability @xmath73 to create an operator at time @xmath43 , times the probability @xmath132 to choose the new state @xmath133 , times the probability @xmath134 to keep moving and destroy , times the probability @xmath135 to stop the move after having destroyed : @xmath136\\ ] ] the probability of the reverse move is exactly symmetric : @xmath137\\ ] ] it is important to notice that , when in the intermediate configuration @xmath7 , the time @xmath138 of the operator to the left of the green operator is equal to @xmath35 , and the time @xmath139 of the operator to the right of the green operator is equal to @xmath43 . \n",
    "thus the acceptance factor takes the form @xmath140}{\\big\\langle\\psi_l\\big|\\hat\\mathcal g\\big|\\psi_r\\big\\rangle p(\\leftarrow)p_\\leftarrow^\\dagger(\\tau ) } \\\\ \n",
    "\\nonumber                    & \\times & \\frac{e^{-\\big(\\tau_l^a-\\tau_r^a\\big)v_r^a}p_\\rightarrow^{kd}(a)}{e^{-\\big(\\tau_l^a-\\tau_r^a\\big)v_l^a}p_\\leftarrow^{kd}(a ) } \\\\                                & \\times & \\frac{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle p(\\rightarrow^\\prime)p_\\rightarrow^\\dagger(\\tau^\\prime)}{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle\\big[1-p_\\leftarrow^{kc}(\\tau^\\prime)\\big]},\\end{aligned}\\ ] ] and is written as a quantity that depends on the initial configuration , times a quantity that depends on the intermediate configuration @xmath7 , times a quantity that depends on the final configuration . \n",
    "it is useful for the remaining of the paper to define the intermediate acceptance factor , @xmath141      we consider here the case where a left move is chosen , the operator on the left of the green operator is destroyed , then an operator is created on its right , and a new state is chosen . finally a time shift is chosen . \n",
    "the sequence of configurations is the following    1 . \n",
    "@xmath125 2 . \n",
    "@xmath142 3 . \n",
    "@xmath127 ,    where we have @xmath143 , and @xmath144 . \n",
    "the probability of the transition from the initial configuration to the final configuration is the probability @xmath72 to choose a left move , times the probability @xmath145 of no creation , times the probability @xmath146 to keep moving and create , times the probability @xmath74 to choose the new state @xmath75 , times the probability @xmath121 to stop the move after having destroyed , times the probability @xmath76 to shift the green operator by @xmath77 : @xmath147p_\\leftarrow^{kc}(a)p_\\leftarrow(\\psi_r^\\prime ) \\\\                        & \\times & \\big[1-p_\\leftarrow^{kd}(\\tau^\\prime)\\big]p_\\leftarrow^{l^\\prime r^\\prime}(\\tau^\\prime-\\tau_r^\\prime)\\end{aligned}\\ ] ] \n",
    "the probability of the reverse move is exactly symmetric : @xmath148p_\\rightarrow^{kc}(a)p_\\rightarrow(\\psi_l ) \\\\                        & \\times & \\big[1-p_\\rightarrow^{kd}(\\tau)\\big]p_\\rightarrow^{lr}(\\tau_l-\\tau)\\end{aligned}\\ ] ] the acceptance factor takes the form @xmath149\\big(v_l - v_r\\big)}{p(\\leftarrow)\\big[1-p_\\leftarrow^\\dagger(\\tau)\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\ \n",
    "\\nonumber                    & \\times & \\frac{\\big\\langle\\psi_l^a\\big|\\hat\\mathcal g\\hat\\mathcal t\\big|\\psi_r^a\\big\\rangle \n",
    "p_\\rightarrow^{kc}(a)}{\\big\\langle\\psi_l^a\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^a\\big\\rangle p_\\leftarrow^{kc}(a ) } \\\\                                & \\times & \\frac{p(\\rightarrow^\\prime)\\big[1-p_\\rightarrow^\\dagger(\\tau^\\prime)\\big]\\big[1-e^{-(\\tau_l^\\prime-\\tau_r^\\prime)(v_r^\\prime - v_l^\\prime)}\\big]}{\\big[1-p_\\leftarrow^{kd}(\\tau^\\prime)\\big]\\big(v_r^\\prime - v_l^\\prime\\big)},\\end{aligned}\\ ] ] and is written as a quantity that depends on the initial configuration , times a quantity that depends on the intermediate configuration @xmath7 , times a quantity that depends on the final configuration . \n",
    "it is useful for the remaining of the paper to define the intermediate acceptance factor , @xmath150      we consider here the case where a left move is chosen , an operator is created on the right of the green operator , then the operator on its left is destroyed , then a second operator is created on its right . \n",
    "finally , a time shift of the green operator is performed . \n",
    "the sequence of configurations is the following    1 . \n",
    "@xmath125 2 . \n",
    "@xmath126 3 . \n",
    "@xmath151 4 . \n",
    "@xmath152 ,    considering the intermediate configurations @xmath7 and @xmath153 between the intial and final configurations , it is easy to show that the corresponding acceptance factor can be written @xmath154      we consider here the case where a left move is chosen , the operator on the left of the green operator is destroyed , then an operator is created on its right . finally a second operator on the left of green operator is destroyed . \n",
    "the sequence of configurations is the following    1 . \n",
    "@xmath155 2 . \n",
    "@xmath156 3 . \n",
    "@xmath157 4 . \n",
    "@xmath127 ,    considering the intermediate configurations @xmath7 and @xmath153 between the intial and final configurations , it is easy to show that the corresponding acceptance factor can be written @xmath158      it is straighforward to show that the acceptance factors of the form @xmath159 , @xmath160 , @xmath161 ( or @xmath162 , @xmath163 , @xmath164 ) can be expressed as products of the acceptance factor @xmath92 ( or @xmath93 ) and the intermediate factors @xmath165 and @xmath166 .    in the same manner , the acceptance factors of the form @xmath167 , @xmath168 , @xmath169 ( or @xmath170 , @xmath171 , @xmath172 ) can be expressed as products of the acceptance factor @xmath173 ( or @xmath174 ) and the intermediate factors @xmath165 and @xmath166 .      here \n",
    "again it is possible to take advantage of the freedom that we have for the choice of the probabilities @xmath72 , @xmath175 , @xmath118 , and @xmath117 ( or @xmath102 , @xmath176 , @xmath177 , and @xmath178 ) . \n",
    "a proper choice of these probabilities can be done in order to allow us to accept all moves , simplicity and generality being the leitmotiv of the sgf algorithm .    for this purpose \n",
    ", we impose to all acceptance factors corresponding to left ( or right ) moves to be equal . \n",
    "this requires the intermediate acceptance factors @xmath165 and @xmath166 ( or @xmath179 and @xmath180 ) to be equal to 1 . \n",
    "this is realized if @xmath181 where @xmath182 and @xmath183 are optimization parameters belonging to @xmath184 . by tuning these parameters , the mean length of the steps of the green operator \n",
    "can be controlled . \n",
    "note that we have explicitly excluded @xmath185 from the allowed values for these optimization parameters . \n",
    "this is necessary for the green operator to have a chance to end in a diagonal configuration , @xmath45 . \n",
    "indeed , the choice @xmath186 would systematically lead to values of @xmath185 for the probabilities @xmath187 and @xmath188 for diagonal configurations . \n",
    "therefore the green operator would never stop in a diagonal configution , and no measurement could be done . \n",
    "it is important here to note that the quantities @xmath96 , @xmath97 , and @xmath98 are evaluated between the states on the left and the right of the green operator that are present at the moment where those quantities are needed , as well as for the times indices @xmath189 and @xmath190 and the potentials @xmath191 and @xmath192 . \n",
    "all acceptance factors corresponding to a given direction of propagation become equal if we choose for the creation probabilities : @xmath193(v_l - v_r)}{\\big[1-p_\\rightarrow^{kc}\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } } \\\\    & & p_\\rightarrow^\\dagger(\\tau)=\\frac{\\big\\langle\\hat\\mathcal t\\hat\\mathcal g\\big\\rangle}{\\big\\langle\\hat\\mathcal t\\hat\\mathcal g\\big\\rangle+\\big\\langle\\hat\\mathcal g\\big\\rangle\\frac{\\big[1-p_\\leftarrow^{kd}\\big](v_r - v_l)}{\\big[1-p_\\leftarrow^{kc}\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_r - v_l)}\\big]}},\\end{aligned}\\ ] ] finally , all acceptances factors become independant of the direction of propagation if we choose @xmath194 and @xmath195 with @xmath196\\frac{\\big\\langle\\hat\\mathcal g\\hat\\mathcal t\\big\\rangle}{\\big\\langle\\hat\\mathcal g\\big\\rangle}+\\frac{\\big[1-p_\\rightarrow^{kd}\\big](v_l - v_r)}{\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\    r_\\rightarrow(\\tau)=\\big[1-p_\\leftarrow^{kc}\\big]\\frac{\\big\\langle\\hat\\mathcal t\\hat\\mathcal g\\big\\rangle}{\\big\\langle\\hat\\mathcal g\\big\\rangle}+\\frac{\\big[1-p_\\leftarrow^{kd}\\big](v_r - v_l)}{\\big[1-e^{-(\\tau_l-\\tau_r)(v_r - v_l)}\\big]}.\\end{aligned}\\ ] ] as a result all moves can be accepted again , ensuring the maximum of simplicity of the algorithm . \n",
    "we still have some freedom for the choice of the optimization parameters @xmath182 and @xmath183 . \n",
    "this is discussed in next section . \n",
    "from the central limit theorem , we know that the errorbar associated to any measured quantity must decrease as the square root of the number of measurements , or equivalently , the square root of the time of the simulation \n",
    ". therefore it makes sense to define the efficiency @xmath197 of a qmc algorithm by @xmath198 where @xmath199 represents the set of all optimization parameters of the algorithm , @xmath200 is the measured quantity of interest , @xmath201 is the time of the simulation , and @xmath202 is the errorbar associated to the measured quantity @xmath200 . \n",
    "this definition ensures that @xmath197 is independent of the time of the simulation . as a result , \n",
    "the larger @xmath197 the more efficient the algorithm .    in the present case \n",
    "we have @xmath203 , while @xmath204 for the original sgf algorithm . \n",
    "it is useful here to realize that , by symmetry , the mean values of @xmath118 and @xmath177 ( and @xmath117 and @xmath178 ) must be equal . \n",
    "therefore we define @xmath205 and @xmath206 . \n",
    "it seems reasonable to impose a condition of uniform sampling , @xmath207 . \n",
    "this condition can be satisfied by adjusting dynamically the values of @xmath182 and @xmath183 during the thermalization process . for this purpose \n",
    "we introduce a new optimization parameter @xmath208 and apply the following algorithm from time to time while thermalizing ( we start with @xmath209 ) : @xmath210 thus we are left with the optimization parameter @xmath211 . in order to determine the optimal value , \n",
    "we have considered 2 different hamiltonians @xmath212 and @xmath213 , and evaluated the efficiency of the algorithm while scanning @xmath211 . \n",
    "the first hamiltonian we have considered describes free hardcore bosons and is exactly solvable , @xmath214 where the sum runs over pairs of first neighboring sites and @xmath215 is the hopping parameter . \n",
    "the second hamiltonian is highly non - trivial and describes a mixture of atoms and diatomic molecules , with a special term allowing conversions between the two species @xcite , @xmath216 where @xmath217 and @xmath218 ( @xmath219 and @xmath220 ) are the creation and annihilation operators of atoms ( molecules ) , @xmath221 , @xmath222 , @xmath223 , @xmath224 , and @xmath225 are respectively the hopping parameter of atoms , the hopping parameter of molecules , the atomic onsite interaction parameter , the molecular onsite interaction parameter , and the inter - species interaction parameter . \n",
    "the conversion term is tunable via the parameter @xmath226 and does not conserve the number @xmath227 of atoms or the number @xmath228 of molecules \n",
    ". however the total number of particles @xmath229 is conserved and is the canonical constraint . \n",
    "the parameter @xmath230 allows to control the ratio between the number of atoms and molecules . \n",
    "the application of the sgf algorithm to the hamiltonian ( [ twospecies ] ) is described in details in ref.@xcite . \n",
    "the changes coming with the directed update scheme are completely independent of the chosen hamiltonian . \n",
    "the following table shows the mean number of creations and destructions in one step , @xmath231 , and the relative efficiency @xmath232 of the algorithm applied to @xmath212 at half filling , for which we have measured the energy @xmath233 , the superfluid density @xmath234 , and the number of particles in the zero momentum state @xmath235 :    . \n",
    "relative efficiency of the algorithm applied to @xmath212 at half filling for the energy , the superfluid density , and the number of particles in the zero momentum state . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     while the best value of @xmath211 depends on the hamiltonian which is considered and the measured quantity , it appears that a good compromise is to choose @xmath211 between @xmath236 and @xmath237 . \n",
    "the improvment of the efficiency is remarkable . in the following , \n",
    "we illustrate the applicability of the algorithm to problems with non - uniform potentials , by adding a parabolic trap to the hamiltonian ( [ twospecies ] ) : @xmath238 the parameters @xmath239 and @xmath240 allow to control the curvature of the trap associated to atoms and molecules , respectively , and @xmath31 is the number of lattice sites . \n",
    "the inclusion of this term in the algorithm is trivial since only the values of the diagonal energies @xmath241 and @xmath242 are changed . \n",
    "figures ( [ density ] ) and ( [ momentum ] ) show the density profiles and momentum distribution functions obtained for a system with @xmath243 lattice sites initially loaded with @xmath244 atoms and no molecules , and the parameters @xmath245 , @xmath246 , @xmath247 , @xmath248 , @xmath249 , @xmath250 , @xmath251 , @xmath252 , @xmath253 , and @xmath254 . \n",
    "the presented results have been obtained by performing @xmath255 updates for thermalization , and @xmath256 updates with measurements ( an update is to be understood as the occurence of a diagonal configuration ) . \n",
    "the time of the simulation is about 8 hours on a cheap 32 bits laptop with 1ghz processor , with an implementation of the algorithm involving dynamical structures with pointers ( see ref.@xcite ) .    ) to the hamiltonian ( [ twospecies ] ) . \n",
    "the errorbars are smaller than the symbol sizes , and are the biggest in the neighborhood of site indices 23 and 47 where they equal the size of the symbols . , scaledwidth=45.0% ]    ) to the hamiltonian ( [ twospecies ] ) . \n",
    "the errorbars are smaller than the symbol sizes , and are the biggest for @xmath257 where they equal the size of the symbols . \n",
    ", scaledwidth=45.0% ] \n",
    "we have presented a directed update scheme for the sgf algorithm , which has the properties of keeping the simplicity and generality of the original algorithm , and improves significantly its efficiency . \n",
    "i would like to express special thanks to peter denteneer for useful suggestions . \n",
    "this work is part of the research program of the `` stichting voor fundamenteel onderzoek der materie ( fom ) , '' which is financially supported by the `` nederlandse organisatie voor wetenschappelijk onderzoek ( nwo ) . '' \n",
    "we describe here how to generate numbers with the appropriate exponential distribution ( [ exponentialdistribution ] ) . assuming that we have at our disposal a uniform random number generator that generates a random variable @xmath258 with the distribution @xmath259 for @xmath260 \n",
    ", we would like to find a function @xmath52 such that the random variable @xmath261 is generated with the distribution @xmath262 where @xmath46 and @xmath263 are the parameters of the exponential distribution . \n",
    "because of the relation @xmath261 , the probability to find @xmath264 in the range @xmath265 must be equal to the probability to find @xmath258 in the range @xmath266 . \n",
    "this implies the condition @xmath267 with @xmath268 . \n",
    "thus we have @xmath269 taking the anti - derivative with respect to @xmath270 on both sides of the equation , we get @xmath271 where @xmath272 is a constant . this constant and the correct sign are determined by imposing the conditions @xmath273 and @xmath274 . as a result , if @xmath270 is a realization of @xmath258 , then a realization of @xmath264 is given by @xmath275.\\ ] ]    10 nicholas metropolis and s. ulam , journal of the american statistical association , number 247 , volume 44 ( 1949 ) . \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = '''\n",
    "in a recent publication we have presented the stochastic green function ( sgf ) algorithm , which has the properties of being general and easy to apply to any lattice hamiltonian of the form @xmath0 , where @xmath1 is diagonal in the chosen occupation number basis and @xmath2 has only positive matrix elements . \n",
    " we propose here a modified version of the update scheme that keeps the simplicity and generality of the original sgf algorithm , and enhances significantly its efficiency .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.02462472592342722), 'rouge2': np.float64(0.019908891513413195), 'rougeL': np.float64(0.020576825771630967), 'rougeLsum': np.float64(0.023950075898127844)}\n"
     ]
    }
   ],
   "source": [
    "rouge_results = rouge_metric.compute(predictions=[abstract], references=[body])\n",
    "\n",
    "print(rouge_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yisrani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/yisrani/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/yisrani/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from nltk.translate import meteor_score\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.8819444444444444)}\n",
      "([(8, 8), (7, 7), (5, 5), (4, 4), (3, 3), (2, 2), (1, 1), (0, 0)], [(6, 'generous')], [(6, 'kind')])\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to accept the generous offer.\"\n",
    "reference = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "meteor_results = meteor.compute(predictions=[prediction], references=[reference])\n",
    "print(meteor_results)\n",
    "result = meteor_score.wordnetsyn_match(hypothesis=word_tokenize(prediction), reference=word_tokenize(reference))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.9814814814814815)}\n",
      "([(2, 2), (1, 1), (0, 0)], [], [])\n",
      "{'rouge1': np.float64(0.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0), 'rougeLsum': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"car kind dog\"\n",
    "reference = \"auto form frump\"\n",
    "\n",
    "meteor_results = meteor.compute(predictions=[prediction], references=[reference])\n",
    "print(meteor_results)\n",
    "result = meteor_score.wordnetsyn_match(hypothesis=word_tokenize(prediction), reference=word_tokenize(reference))\n",
    "print(result)\n",
    "rouge_results = rouge_metric.compute(references=[reference], predictions=[prediction])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonym - Larger text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.996666517525904)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     Their impact spans various fields such as education, software engineering, and research. She was happy to accept the generous offer.\"\n",
    "candidate = \"It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     It laid the foundation for Large Language Models (LLMs), which have surged in popularity among researchers and practitioners alike.     This momentum reached new heights with OpenAI's release of the decoder-only Generative Pre-trained Transformer (GPT) models,     particularly GPT-3, culminating in widespread industry adoption following the release of ChatGPT. Due to its ability to generate     human-like text, streamline operations, and enhance decision-making, LLMs have been adopted in various professional contexts.     Their impact spans various fields such as education, software engineering, and research.     Their impact spans various fields such as education, software engineering, and research.  She was happy to accept the kind offer.\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: {'meteor': np.float64(0.7361111111111112)}\n"
     ]
    }
   ],
   "source": [
    "reference = [\"I always do.\"]    \n",
    "candidate = [\"I always do!\"]\n",
    "\n",
    "results = meteor.compute(predictions=candidate, references=reference)\n",
    "print(f\"METEOR Score: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.5)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"-\"\n",
    "candidate = \"-\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbols in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.7773446457234581)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to-accept the kind offer.\"\n",
    "candidate = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.9835843169176502)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to - accept the kind offer.\"\n",
    "candidate = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.728249194414608)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy t/;,/?#$@!&^%*()+o accept the kind offer\"\n",
    "candidate = \"She was happy to accept the kind offer\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.9976851851851852)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"functionName( type param ):\"\n",
    "candidate = \"functionName( type param ):\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"o/k\"\n",
    "candidate = \"o/j\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"o/ k\"\n",
    "candidate = \"o/j\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"ok\"\n",
    "candidate = \"o/j\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbols multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.8443978569764733)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to /;,/?#$@!&^%*()+ accept the kind offer.\"\n",
    "candidate = \"She was happy to accept the kind offer.\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbols at start of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.864795918367347)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy +to accept the kind offer\"\n",
    "candidate = \"She was happy to accept the kind offer\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbols after word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.864795918367347)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"She was happy to+ accept the kind offer\"\n",
    "candidate = \"She was happy to accept the kind offer\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.5)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"0\"\n",
    "candidate = \"0\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.2631578947368421)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"0\"\n",
    "candidate = \"123 0\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.9375)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"higher values\"\n",
    "candidate = \"HIGHER VALUES\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[candidate])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order + Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.5)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"higher values\"\n",
    "reference = \"VALUES higher\"\n",
    "\n",
    "rouge_results = meteor.compute(predictions=[prediction], references=[reference])\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.5)}\n",
      "([(0, 1), (1, 0), (2, 3), (3, 2)], [], [])\n"
     ]
    }
   ],
   "source": [
    "prediction = \"higher values higher values\"\n",
    "reference = \"values higher values higher\"\n",
    "\n",
    "meteor_results = meteor.compute(predictions=[prediction], references=[reference])\n",
    "print(meteor_results)\n",
    "results = meteor_score.align_words(reference=word_tokenize(reference), hypothesis=word_tokenize(prediction))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(1.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.5555555555555556), 'rougeLsum': np.float64(0.5555555555555556)}\n",
      "{'bleu': 0.0, 'precisions': [1.0, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 9, 'reference_length': 9}\n",
      "{'meteor': np.float64(0.5)}\n"
     ]
    }
   ],
   "source": [
    "prediction = \"the brown quick fox over jumps lazy the dog\"\n",
    "reference = \"the quick brown over fox lazy jumps dog the\"\n",
    "\n",
    "rouge_results = rouge_metric.compute(predictions=[prediction], references=[reference])\n",
    "print(rouge_results)\n",
    "blue_results = blue_metric.compute(predictions=[prediction], references=[reference])\n",
    "print(blue_results)\n",
    "meteor_results = meteor.compute(predictions=[prediction], references=[reference])\n",
    "print(meteor_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor': np.float64(0.9999638310185185)}\n",
      "{'rouge1': np.float64(0.8), 'rouge2': np.float64(0.6666666666666666), 'rougeL': np.float64(0.8), 'rougeLsum': np.float64(0.8)}\n",
      "{'meteor': np.float64(0.9814814814814815)}\n",
      "{'meteor': np.float64(0.9814814814814815)}\n"
     ]
    }
   ],
   "source": [
    "# Define our component sentences:\n",
    "x = \"the quick brown fox jumps over the lazy dog\"\n",
    "y = \"and then runs into the forest\"\n",
    "\n",
    "# Build a reference text with structure: x, y, x\n",
    "reference = f\"{x} {y} {x}\"\n",
    "\n",
    "# Candidate 1: Preserves the reference order exactly.\n",
    "candidate1 = reference\n",
    "\n",
    "# Candidate 2: Switches the ordering of sentences (x, x, y)\n",
    "# This candidate puts the repeated sentence x together and then y last,\n",
    "# which disrupts the original order.\n",
    "candidate2 = f\"{x} {x} {y}\"\n",
    "\n",
    "# Candidate 3: Mixes the tokens a little more (e.g., reverses the order of the last two sentences)\n",
    "# This candidate follows the pattern: x, x & y (with partial overlap)\n",
    "candidate3 = f\"{x} {y} {x.split()[::-1]}\"\n",
    "# Note: For a cleaner candidate that is fully a string, we provide another example below.\n",
    "# Here, we're manually creating a candidate that misorders the segments.\n",
    "candidate3 = f\"{y} {x} {x}\"\n",
    "\n",
    "meteor_results1 = meteor.compute(references=[reference], predictions=[candidate1])\n",
    "rouge_results1 = rouge_metric.compute(references=[reference], predictions=[candidate1])\n",
    "meteor_results2 = meteor.compute(references=[reference], predictions=[candidate2])\n",
    "meteor_results3 = meteor.compute(references=[reference], predictions=[candidate3])\n",
    "print(meteor_results1)\n",
    "print(rouge_results)\n",
    "print(meteor_results2)\n",
    "print(meteor_results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: 0.7361111111111112\n"
     ]
    }
   ],
   "source": [
    "candidates = [\"I always do!\"]\n",
    "references = [\"I always do.\", \"I invariably do.\", \"I perpetually do.\"]\n",
    "\n",
    "meteor_score = meteor.compute(predictions=candidates, references=[references])\n",
    "print(f\"METEOR Score: {meteor_score['meteor']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: {'meteor': np.float64(0.9990234375)}\n"
     ]
    }
   ],
   "source": [
    "reference = [\"In a world of computers ahahahaha ahaha ajaja\"]  \n",
    "candidate = [\"In a world of computers ahahahaha ahaha ajaja\"]\n",
    "\n",
    "results = meteor.compute(predictions=candidate, references=reference)\n",
    "print(f\"METEOR Score: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: {'meteor': np.float64(0.9993141289437586)}\n"
     ]
    }
   ],
   "source": [
    "reference = \"the cats are chasing multiple mouses in the gardens\"\n",
    "candidate = \"the cats are chasing multiple mouse in the gardens\"\n",
    "\n",
    "results = meteor.compute(predictions=[candidate], references=[reference])\n",
    "print(f\"METEOR Score: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: {'meteor': np.float64(0.8819444444444444)}\n"
     ]
    }
   ],
   "source": [
    "reference = \"the cats are chasing multiple mouses in the gardens\"\n",
    "# TODO: use the stemmer from before to get this result\n",
    "candidate = \"the cats are chasing multiple mous in the gardens\"\n",
    "\n",
    "results = meteor.compute(predictions=[candidate], references=[reference])\n",
    "print(f\"METEOR Score: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/0l/m9q4hqlx7j95m4nz7kp_6fmc0000gp/T/ipykernel_47912/498530429.py:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  body = '''\n"
     ]
    }
   ],
   "source": [
    "body = '''\n",
    "monte carlo methods @xcite appeared about sixty years ago with the need to evaluate numerical values for various complex problems . \n",
    "these methods evolved and were applied early to quantum problems , thus putting within reach exact numerical solutions to non - trivial quantum problems @xcite . \n",
    "many improvements of these methods followed , avoiding critical slowing down near phase transitions and allowing to work directly in the continuous imaginary time limit @xcite . in recent years , \n",
    "interest in methods that work in the canonical ensemble with global updates yet allow access to green functions has intensified @xcite . \n",
    "however , a method that works well for a given hamiltonian often needs major modifications for another . \n",
    "for example , the addition of a 4-site ring exchange term in the bosonic hubbard model required special developments for a treatment by the stochastic series expansion algorithm @xcite , as well as by the wordline algorithm @xcite . \n",
    "this can result in long delays . \n",
    "it is , therefore , advantageous to have at one s disposal an algorithm that can be applied to a very wide class of hamiltonians without requiring any changes . in a recent publication @xcite , \n",
    "the stochastic green function ( sgf ) algorithm was presented , which meets this goal . \n",
    "the algorithm can be applied to any lattice hamiltonian of the form @xmath3 where @xmath1 is diagonal in the chosen occupation number basis and @xmath2 has only positive matrix elements . \n",
    "this includes all kinds of systems that can be treated by other methods presented in ref.@xcite , for instance bose - hubbard models with or without a trap , bose - fermi mixtures in one dimension , heisenberg models ... in particular hamiltonians for which the non - diagonal part @xmath2 is non - trivial ( the eigen - basis is unknown ) are easily treated , such as the bose - hubbard model with ring exchange @xcite , or multi - species hamiltonians in which a given species can be turned into another one ( see eq.([twospecies ] ) and fig . \n",
    "[ density ] and [ momentum ] for a concrete example ) . \n",
    "systems for which it is not possible to find a basis in which @xmath1 is diagonal and @xmath2 has only positive matrix elements are said to have a `` sign problem '' , which usually arises with fermionic and frustrated systems . as other qmc methods , the sgf algorithm does not solve this problem .    the algorithm allows to measure several quantities of interest , such as the energy , the local density , local compressibility , density - density correlation functions ... in particular the winding is sampled and gives access to the superfluid density . \n",
    "equal - time n - body green functions are probably the most interesting quantities that can be measured by the algorithm , by giving access to momentum distribution functions which allow direct comparisons with experiments . \n",
    "all details on measurements are given in ref.@xcite . \n",
    "in addition the algorithm has the property of being easy to code , due in part to a simple update scheme in which all moves are accepted with a probability of 1 . despite of such generality and simplicity , \n",
    "the algorithm might suffer from a reduced efficiency , compared to other algorithms in situations where they can be applied . \n",
    "the purpose of this paper is to present a `` directed '' update scheme that ( i ) keeps the simplicity and generality of the original sgf algorithm , and ( ii ) enhances its efficiency by improving the sampling over the imaginary time axis . \n",
    "while the sgf algorithm is not intended to compete with the speed of other algorithms , the improvment resulting from the directed update scheme is remarkable ( see section v ) . \n",
    "but what makes the strength of the sgf method is that it allows to simulate hamiltonians that can not be treated by other methods or that would require special developments ( see eq.([twospecies ] ) for a concrete example ) . \n",
    "the paper is organized as follows : we introduce in section ii the notations and definitions used in ref.@xcite . in section iii , we propose a simplification of the update scheme used in the original sgf algorithm , and determine how to satisfy detailed balance . \n",
    "a generalization of the simplified update scheme is presented in section iv , which constitutes the directed updated scheme . \n",
    "finally section v shows how to determine the introduced optimization parameters , and presents some tests of the algorithm and a comparison with the original version . \n",
    "in this section , we recall the expression of the `` green operator '' introduced in the sgf algorithm , and the extended partition function which is considered . although not required for understanding this paper , we refer the reader to ref.@xcite for full details on the algorithm . as many qmc algorithms , \n",
    "the sgf algorithm samples the partition function @xmath4 the algorithm has the property of working in the canonical ensemble . in order to define the green operator , \n",
    "we first define the `` normalized '' creation and annihilation operators , @xmath5 where @xmath6 and @xmath7 are the usual creation and annihilation operators of bosons , and @xmath8 is the number operator . from ( [ normalizedoperators ] ) \n",
    "one can show the following relations for any state @xmath9 in the occupation number representation , @xmath10 with the particular case @xmath11 . \n",
    "appart from this exception , the operators @xmath12 and @xmath13 change a state @xmath9 by respectively creating and annihilating one particle , but they do not change the norm of the state .    using the notation @xmath14 to denote two subsets of site indices @xmath15 and @xmath16 with the constraint that all indices in subset @xmath17 are different from the indices in subset @xmath18 ( but several indices in one subset may be equal ) , we define the green operator @xmath19 by @xmath20 where @xmath21 is a matrix that depends on the application of the algorithm @xcite . in order to sample the partition function ( [ partitionfunction ] ) , an extended partition function @xmath22 is considered by breaking up the propagator @xmath23 , and introducing the green operator between the broken parts , @xmath24 defining the time dependant operators @xmath25 and @xmath26 \n",
    ", @xmath27 and working in the occupation number basis in which @xmath1 is diagonal , the extended partition function takes the form @xmath28 where the sum @xmath29 implicitly runs over complete sets of states @xmath30 . we will systematically use the labels @xmath31 and @xmath32 to denote the states appearing on the left and the right of the green operator , and use the notation @xmath33 to denote the diagonal energy @xmath34 . \n",
    "we will also denote by @xmath35 and @xmath36 the time indices of the @xmath2 operators appearing on the left and the right of @xmath19 .    as a result , \n",
    "the extended partition function is a sum over all possible configurations , each being determined by a set of time indices @xmath37 and a set of states @xmath38 , @xmath39 , @xmath40,@xmath41 , @xmath42 . \n",
    "the algorithm consists in updating those configurations by making use of the green operator . assuming that the green operator is acting at time @xmath43 \n",
    ", it can `` create '' a @xmath2 operator ( that is to say a @xmath2 operator can be inserted in the operator string ) at the same time , thus introducing a new intermediate state , then it can be shifted to a different time . while shifting , any @xmath2 operator encountered by the green operator is `` destroyed '' ( that is to say removed from the operator string ) . assuming a left ( or right ) move , creating an operator will update the state @xmath44 ( or @xmath41 ) , \n",
    "while destroying will update the state @xmath41 ( or @xmath44 ) . \n",
    "when a diagonal configuration of the green operator occurs , @xmath45 , such a configuration associated to the extended partition function ( [ extendedpartitionfunction ] ) is also a configuration associated to the partition function ( [ partitionfunction ] ) . \n",
    "measurements can be done when this occurs ( see ref.@xcite for details on measurements ) . \n",
    "next section presents a simple update scheme that meets the requirements of ergodicity and detailed balance . \n",
    "before introducing the directed update , we start by simplifying the update scheme used in the original sgf algorithm \n",
    ".      we will assume in the following that a left move of the green operator is chosen . \n",
    "in the original version , the green operator @xmath26 can choose to create or not on its right a @xmath2 operator at time @xmath43 . \n",
    "then a time shift @xmath46 to the left is chosen for the green operator with an exponential distribution in the range @xmath47 . \n",
    "if an operator is encountered while shifting the green operator , then the operator is destroyed and the move stops there . as a result , four possible situations can occur during one move :    1 .   no creation , shift , no destruction . \n",
    "2 .   creation , shift , no destruction . \n",
    "3 .   no creation , shift , destruction . \n",
    "4 .   creation , shift , destruction . \n",
    "it appears that the first possibility `` no creation , no destruction '' is actually useless , since no change is performed in the operator string . the idea is to get rid of this possibility by forcing the green operator to destroy an operator if no creation is chosen \n",
    "a further simplification can be done by noticing that the last possibility `` creation , destruction '' is not necessary for the ergodicity of the algorithm , and can be avoided by restricting the range of the time shift after having created an operator . \n",
    "therefore we replace the original update scheme by the following : we assume that the green operator is acting at time @xmath43 and that the operator on its left is acting at time @xmath35 . the green operator @xmath26 chooses to create or not an operator on its right at time @xmath43 . \n",
    "if creation is chosen , then a time shift @xmath46 of the green operator is chosen to the left in the range @xmath48 , with the probability distribution defined below . \n",
    "if no creation is chosen , then the green operator is directly shifted to the operator on its left at time @xmath35 , and the operator is destroyed . as a result \n",
    "only two possibilities have to be considered :    1 . \n",
    "creation , shift . \n",
    "2 .   shift , destruction . \n",
    "figure [ simplfiedupdatescheme ] shows the associated organigram . \n",
    "section iii.b explains how detailed balance can be satisfied with this simplified update scheme .          when updating the configurations according to the chosen update scheme , we need to generate different transitions from initial to final states with probabilities that satisfy detailed balance . in this section \n",
    "we propose a choice for these probabilities , and determine the corresponding acceptance factors . \n",
    "we denote the probability of the initial ( final ) configuration by @xmath49 ( @xmath50 ) . \n",
    "we denote by @xmath51 the probability of the transition from configuration @xmath17 to configuration @xmath52 , and by @xmath53 the probability of the reverse transition . \n",
    "finally we denote by @xmath54 the acceptance rate of the transition from @xmath17 to @xmath52 , and by @xmath55 the acceptance rate of the reverse transition . \n",
    "the detailed balance can be written as @xmath56 we will make use of the metropolis solution @xcite , @xmath57 with @xmath58 we will use primed ( non - primed ) labels for states and time indices to denote final ( initial ) configurations . \n",
    "we consider here the case where a left move is chosen , an operator is created on the right of the green operator at time @xmath43 , and a new state is chosen . \n",
    "then a time shift to the left is chosen for the green operator in the range @xmath59 . \n",
    "it is important to note that @xmath60 and @xmath61 correspond to the time indices of the operators appearing on the left and the right of the green operator after the new operator has been inserted , that is to say at the moment where the time shift needs to be performed . \n",
    "thus we have @xmath62 and @xmath63 . \n",
    "the probability of the initial configuration is the boltzmann weight appearing in the extended partition function ( [ extendedpartitionfunction ] ) : @xmath64 the probability of the final configuration takes the form : @xmath65 it is important here to realize that the green operator only inserted on its right the operator @xmath66 , before being shifted from @xmath61 to @xmath67 . \n",
    "therefore we have the equalities @xmath68 , @xmath69 , @xmath70 , and @xmath71 . \n",
    "the probability @xmath51 of the transition from the initial configuration to the final configuration is the probability @xmath72 of a left move , times the probability @xmath73 of a creation , times the probability @xmath74 to choose the new state @xmath75 , times the probability @xmath76 to shift the green operator by @xmath77 , knowing that the states on the left and the right of the green operator at the moment of the shift are @xmath78 and @xmath79 : @xmath80 the probability of the reverse transition is simply the probability @xmath81 of a right move , times the probability of no creation , @xmath82 : @xmath83\\ ] ] from the original version of the sgf algorithm , we know that choosing the time shift with an exponential distribution is a good choice , because it cancels the exponentials appearing in the probabilities of the initial ( [ initial ] ) and final ( [ final ] ) configurations , avoiding exponentially small acceptance factors . \n",
    "however a different normalization must be used here , since the time shift is chosen in the range @xmath84 instead of @xmath47 . \n",
    "the suitable solution is : @xmath85 it is straightforward to check that the above probability is correctly normalized and well - defined for any real value of @xmath86 , the particular case @xmath87 reducing to the uniform distribution @xmath88 ( note that @xmath89 is always a positive number ) . for the probability @xmath74 to choose the new state @xmath75 , \n",
    "the convenient solution is the same as in the original version : @xmath90 putting everything together , the acceptance factor ( [ metropolis2 ] ) becomes @xmath91\\big[1-e^{-(\\tau_l^\\prime-\\tau_r^\\prime)(v_r^\\prime - v_l^\\prime)}\\big]}{v_r^\\prime - v_l^\\prime},\\end{aligned}\\ ] ] where we have used the notation @xmath92 to emphasize that this acceptance factor corresponds to a creation . \n",
    "it is also important for the remaining of this paper to note that @xmath92 is written as a quantity that depends on the initial configuration , times a quantity that depends on the final configuration . \n",
    "we consider here the case where a left move is chosen , and the operator on the left of the green operator is destroyed . \n",
    "this move corresponds to the inverse of the above `` creation , shift '' move . \n",
    "thus , the corresponding acceptance factor @xmath93 is obtained by inverting the acceptance factor @xmath92 , exchanging the initial time @xmath43 and final time @xmath67 , and switching the direction . \n",
    "however @xmath94 represents an absolute time shift , so @xmath35 and @xmath36 do not have to be exchanged . \n",
    "we get @xmath95\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\                             & \\times & \\frac{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle p(\\rightarrow^\\prime)p_\\rightarrow^\\dagger(\\tau^\\prime)}{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle},\\end{aligned}\\ ] ] which is written as a quantity that depends on the initial configuration , times a quantity that depends on the final configuration . \n",
    "we will use here the short notation @xmath96 , @xmath97 , and @xmath98 to denote respectively the quantities @xmath99 , @xmath100 , and @xmath101 . as in ref . \n",
    "@xcite , we have some freedom for the choice of the probabilities of choosing a left or right move , @xmath72 and @xmath102 , and the probabilities of creation @xmath73 and @xmath103 . \n",
    "a suitable choice for those probabilities can be done in order to accept all moves , resulting in an appreciable simplification of the algorithm . for this purpose , \n",
    "we impose the acceptance factor @xmath92 ( or @xmath104 ) to be equal to the acceptance factor @xmath93 ( or @xmath105 ) . \n",
    "this allows to determine the probabilities @xmath73 and @xmath103 , @xmath106 and the acceptance factors @xmath107 and @xmath108 take the form @xmath109 with @xmath110 finally we can impose the acceptance factors @xmath111 and @xmath112 to be equal . \n",
    "this implies @xmath113 defining @xmath114 , we are left with a single acceptance factor , @xmath115 which is independent of the chosen direction , and independent of the nature of the move ( creation or destruction ) . \n",
    "thus all moves can be accepted by making use of a proper reweighting , as explained in ref . \n",
    "the appendix shows how to generate random numbers with the appropriate exponential distribution ( [ exponentialdistribution ] ) .      although the above simplified update scheme works , it turns out to have a poor efficiency . \n",
    "this is because of a lack of `` directionality '' : the green operator has , in average , a probability of @xmath116 to choose a left move or a right move . \n",
    "therefore the green operator propagates along the operator string like a `` drunk man '' , with a diffusion - like law . \n",
    "the basic creation and destruction processes correspond to the steps of the random walk . \n",
    "this suggests that the efficiency of the update scheme can be improved if one can force the green operator to move in the same direction for several iterations . \n",
    "next section presents a modified version of the simplified update scheme , which allows to control the mean length of the steps of the random walk , that is to say the mean number of creations and destructions in a given direction . \n",
    "the proposed directed update scheme can be considered analogous to the `` directed loop update '' used in the stochastic series expansion algorithm @xcite , which prevents a worm from going backwards . \n",
    "however the connection should not be pushed too far . \n",
    "indeed the picture of a worm whose head is evolving both in space and imaginary time accross vertices is obvious in a loop algorithm . in such algorithm , \n",
    "a creation ( or an annihilation ) operator which is represented by the head of a worm is propagated both in space and imaginary time , while an annihilation ( or a creation ) operator represented by the tail of the worm remains at rest . \n",
    "the loop ends when the head of the worm bites the tail . \n",
    "such a worm picture is not obvious in the sgf algorithm : instead of single creation or annihilation operators , it is the full green operator over the whole space that is propagated only in imaginary time . \n",
    "this creates open worldlines , thus introducing discontinuities . \n",
    "these discontinuities increase or decrease while propagating in imaginary time . \n",
    "all open ends of the worldlines are localized at the same imaginary time index . \n",
    "therefore it is actually not possible to draw step by step a worm whose head is evolving in space and imaginary time until it bites its tail . \n",
    "we present in this section a directed update scheme which is obtained by modifying slightly the simplified update scheme , thus keeping the simplicity and generality of the algorithm .      assuming that a left move is chosen \n",
    ", the green operator chooses between starting the move by a creation or a destruction . after having created ( or destroyed ) an operator , the green operator can choose to keep moving in the same direction and destroy ( or create ) with a probability @xmath117 ( or @xmath118 ) , or to stop . \n",
    "if it keeps moving , then a destruction ( or creation ) occurs , and the green operator can choose to keep moving and create ( or destroy ) with a probability @xmath118 ( or @xmath117 ) ... and so on , until it decides to stop . if the last action of the move is a creation , then a time shift is chosen . \n",
    "the organigram is represented in figure [ directedupdatescheme ] .          in order to satisfy detailed balance , in addition to the acceptance factors @xmath92 and @xmath93 , we need to determine new acceptance factors of the form @xmath119 and @xmath120 . \n",
    "we first determine the new expressions of @xmath92 and @xmath93 resulting from the directed update scheme . for @xmath92 , \n",
    "the previous probability @xmath51 has to be multiplied by the probability to stop the move after having created , @xmath121 . \n",
    "the previous probability @xmath53 has to be multiplied by the probability to stop the move after having destroyed , @xmath122 . \n",
    "we get for @xmath92 and @xmath93 the new expressions : @xmath123}{\\big\\langle\\psi_l\\big|\\hat\\mathcal g\\big|\\psi_r\\big\\rangle p(\\leftarrow)p_\\leftarrow^\\dagger(\\tau ) } \\\\                             & \\times & \\frac{p(\\rightarrow^\\prime)\\big[1-p_\\rightarrow^\\dagger(\\tau^\\prime)\\big]\\big[1-e^{-(\\tau_l^\\prime-\\tau_r^\\prime)(v_r^\\prime - v_l^\\prime)}\\big]}{\\big[1-p_\\leftarrow^{kd}(\\tau^\\prime)\\big]\\big(v_r^\\prime - v_l^\\prime\\big ) } \\\\ \n",
    "\\nonumber q_\\leftarrow^d & = &       \\frac{\\big[1-p_\\rightarrow^{kd}(\\tau)\\big]\\big(v_l - v_r\\big)}{p(\\leftarrow)\\big[1-p_\\leftarrow^\\dagger(\\tau)\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\                             & \\times & \\frac{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle p(\\rightarrow^\\prime)p_\\rightarrow^\\dagger(\\tau^\\prime)}{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle\\big[1-p_\\leftarrow^{kc}(\\tau^\\prime)\\big]},\\end{aligned}\\ ] ]      we consider here the case where a left move is chosen , an operator is created on the right of the green operator , and a new state is chosen . \n",
    "then the operator on the left of the green operator is destroyed . using the superscripts @xmath124 to denote intermediate configurations between initial and final configurations , \n",
    "the sequence is the following    1 . \n",
    "@xmath125 2 . \n",
    "@xmath126 3 . \n",
    "@xmath127 ,    where we have @xmath128 , @xmath129 , @xmath130 , and @xmath131 . \n",
    "the probability of the transition from the initial configuration to the final configuration is the probability @xmath72 to choose a left move , times the probability @xmath73 to create an operator at time @xmath43 , times the probability @xmath132 to choose the new state @xmath133 , times the probability @xmath134 to keep moving and destroy , times the probability @xmath135 to stop the move after having destroyed : @xmath136\\ ] ] the probability of the reverse move is exactly symmetric : @xmath137\\ ] ] it is important to notice that , when in the intermediate configuration @xmath7 , the time @xmath138 of the operator to the left of the green operator is equal to @xmath35 , and the time @xmath139 of the operator to the right of the green operator is equal to @xmath43 . \n",
    "thus the acceptance factor takes the form @xmath140}{\\big\\langle\\psi_l\\big|\\hat\\mathcal g\\big|\\psi_r\\big\\rangle p(\\leftarrow)p_\\leftarrow^\\dagger(\\tau ) } \\\\ \n",
    "\\nonumber                    & \\times & \\frac{e^{-\\big(\\tau_l^a-\\tau_r^a\\big)v_r^a}p_\\rightarrow^{kd}(a)}{e^{-\\big(\\tau_l^a-\\tau_r^a\\big)v_l^a}p_\\leftarrow^{kd}(a ) } \\\\                                & \\times & \\frac{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle p(\\rightarrow^\\prime)p_\\rightarrow^\\dagger(\\tau^\\prime)}{\\big\\langle\\psi_l^\\prime\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^\\prime\\big\\rangle\\big[1-p_\\leftarrow^{kc}(\\tau^\\prime)\\big]},\\end{aligned}\\ ] ] and is written as a quantity that depends on the initial configuration , times a quantity that depends on the intermediate configuration @xmath7 , times a quantity that depends on the final configuration . \n",
    "it is useful for the remaining of the paper to define the intermediate acceptance factor , @xmath141      we consider here the case where a left move is chosen , the operator on the left of the green operator is destroyed , then an operator is created on its right , and a new state is chosen . finally a time shift is chosen . \n",
    "the sequence of configurations is the following    1 . \n",
    "@xmath125 2 . \n",
    "@xmath142 3 . \n",
    "@xmath127 ,    where we have @xmath143 , and @xmath144 . \n",
    "the probability of the transition from the initial configuration to the final configuration is the probability @xmath72 to choose a left move , times the probability @xmath145 of no creation , times the probability @xmath146 to keep moving and create , times the probability @xmath74 to choose the new state @xmath75 , times the probability @xmath121 to stop the move after having destroyed , times the probability @xmath76 to shift the green operator by @xmath77 : @xmath147p_\\leftarrow^{kc}(a)p_\\leftarrow(\\psi_r^\\prime ) \\\\                        & \\times & \\big[1-p_\\leftarrow^{kd}(\\tau^\\prime)\\big]p_\\leftarrow^{l^\\prime r^\\prime}(\\tau^\\prime-\\tau_r^\\prime)\\end{aligned}\\ ] ] \n",
    "the probability of the reverse move is exactly symmetric : @xmath148p_\\rightarrow^{kc}(a)p_\\rightarrow(\\psi_l ) \\\\                        & \\times & \\big[1-p_\\rightarrow^{kd}(\\tau)\\big]p_\\rightarrow^{lr}(\\tau_l-\\tau)\\end{aligned}\\ ] ] the acceptance factor takes the form @xmath149\\big(v_l - v_r\\big)}{p(\\leftarrow)\\big[1-p_\\leftarrow^\\dagger(\\tau)\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\ \n",
    "\\nonumber                    & \\times & \\frac{\\big\\langle\\psi_l^a\\big|\\hat\\mathcal g\\hat\\mathcal t\\big|\\psi_r^a\\big\\rangle \n",
    "p_\\rightarrow^{kc}(a)}{\\big\\langle\\psi_l^a\\big|\\hat\\mathcal t\\hat\\mathcal g\\big|\\psi_r^a\\big\\rangle p_\\leftarrow^{kc}(a ) } \\\\                                & \\times & \\frac{p(\\rightarrow^\\prime)\\big[1-p_\\rightarrow^\\dagger(\\tau^\\prime)\\big]\\big[1-e^{-(\\tau_l^\\prime-\\tau_r^\\prime)(v_r^\\prime - v_l^\\prime)}\\big]}{\\big[1-p_\\leftarrow^{kd}(\\tau^\\prime)\\big]\\big(v_r^\\prime - v_l^\\prime\\big)},\\end{aligned}\\ ] ] and is written as a quantity that depends on the initial configuration , times a quantity that depends on the intermediate configuration @xmath7 , times a quantity that depends on the final configuration . \n",
    "it is useful for the remaining of the paper to define the intermediate acceptance factor , @xmath150      we consider here the case where a left move is chosen , an operator is created on the right of the green operator , then the operator on its left is destroyed , then a second operator is created on its right . \n",
    "finally , a time shift of the green operator is performed . \n",
    "the sequence of configurations is the following    1 . \n",
    "@xmath125 2 . \n",
    "@xmath126 3 . \n",
    "@xmath151 4 . \n",
    "@xmath152 ,    considering the intermediate configurations @xmath7 and @xmath153 between the intial and final configurations , it is easy to show that the corresponding acceptance factor can be written @xmath154      we consider here the case where a left move is chosen , the operator on the left of the green operator is destroyed , then an operator is created on its right . finally a second operator on the left of green operator is destroyed . \n",
    "the sequence of configurations is the following    1 . \n",
    "@xmath155 2 . \n",
    "@xmath156 3 . \n",
    "@xmath157 4 . \n",
    "@xmath127 ,    considering the intermediate configurations @xmath7 and @xmath153 between the intial and final configurations , it is easy to show that the corresponding acceptance factor can be written @xmath158      it is straighforward to show that the acceptance factors of the form @xmath159 , @xmath160 , @xmath161 ( or @xmath162 , @xmath163 , @xmath164 ) can be expressed as products of the acceptance factor @xmath92 ( or @xmath93 ) and the intermediate factors @xmath165 and @xmath166 .    in the same manner , the acceptance factors of the form @xmath167 , @xmath168 , @xmath169 ( or @xmath170 , @xmath171 , @xmath172 ) can be expressed as products of the acceptance factor @xmath173 ( or @xmath174 ) and the intermediate factors @xmath165 and @xmath166 .      here \n",
    "again it is possible to take advantage of the freedom that we have for the choice of the probabilities @xmath72 , @xmath175 , @xmath118 , and @xmath117 ( or @xmath102 , @xmath176 , @xmath177 , and @xmath178 ) . \n",
    "a proper choice of these probabilities can be done in order to allow us to accept all moves , simplicity and generality being the leitmotiv of the sgf algorithm .    for this purpose \n",
    ", we impose to all acceptance factors corresponding to left ( or right ) moves to be equal . \n",
    "this requires the intermediate acceptance factors @xmath165 and @xmath166 ( or @xmath179 and @xmath180 ) to be equal to 1 . \n",
    "this is realized if @xmath181 where @xmath182 and @xmath183 are optimization parameters belonging to @xmath184 . by tuning these parameters , the mean length of the steps of the green operator \n",
    "can be controlled . \n",
    "note that we have explicitly excluded @xmath185 from the allowed values for these optimization parameters . \n",
    "this is necessary for the green operator to have a chance to end in a diagonal configuration , @xmath45 . \n",
    "indeed , the choice @xmath186 would systematically lead to values of @xmath185 for the probabilities @xmath187 and @xmath188 for diagonal configurations . \n",
    "therefore the green operator would never stop in a diagonal configution , and no measurement could be done . \n",
    "it is important here to note that the quantities @xmath96 , @xmath97 , and @xmath98 are evaluated between the states on the left and the right of the green operator that are present at the moment where those quantities are needed , as well as for the times indices @xmath189 and @xmath190 and the potentials @xmath191 and @xmath192 . \n",
    "all acceptance factors corresponding to a given direction of propagation become equal if we choose for the creation probabilities : @xmath193(v_l - v_r)}{\\big[1-p_\\rightarrow^{kc}\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } } \\\\    & & p_\\rightarrow^\\dagger(\\tau)=\\frac{\\big\\langle\\hat\\mathcal t\\hat\\mathcal g\\big\\rangle}{\\big\\langle\\hat\\mathcal t\\hat\\mathcal g\\big\\rangle+\\big\\langle\\hat\\mathcal g\\big\\rangle\\frac{\\big[1-p_\\leftarrow^{kd}\\big](v_r - v_l)}{\\big[1-p_\\leftarrow^{kc}\\big]\\big[1-e^{-(\\tau_l-\\tau_r)(v_r - v_l)}\\big]}},\\end{aligned}\\ ] ] finally , all acceptances factors become independant of the direction of propagation if we choose @xmath194 and @xmath195 with @xmath196\\frac{\\big\\langle\\hat\\mathcal g\\hat\\mathcal t\\big\\rangle}{\\big\\langle\\hat\\mathcal g\\big\\rangle}+\\frac{\\big[1-p_\\rightarrow^{kd}\\big](v_l - v_r)}{\\big[1-e^{-(\\tau_l-\\tau_r)(v_l - v_r)}\\big ] } \\\\    r_\\rightarrow(\\tau)=\\big[1-p_\\leftarrow^{kc}\\big]\\frac{\\big\\langle\\hat\\mathcal t\\hat\\mathcal g\\big\\rangle}{\\big\\langle\\hat\\mathcal g\\big\\rangle}+\\frac{\\big[1-p_\\leftarrow^{kd}\\big](v_r - v_l)}{\\big[1-e^{-(\\tau_l-\\tau_r)(v_r - v_l)}\\big]}.\\end{aligned}\\ ] ] as a result all moves can be accepted again , ensuring the maximum of simplicity of the algorithm . \n",
    "we still have some freedom for the choice of the optimization parameters @xmath182 and @xmath183 . \n",
    "this is discussed in next section . \n",
    "from the central limit theorem , we know that the errorbar associated to any measured quantity must decrease as the square root of the number of measurements , or equivalently , the square root of the time of the simulation \n",
    ". therefore it makes sense to define the efficiency @xmath197 of a qmc algorithm by @xmath198 where @xmath199 represents the set of all optimization parameters of the algorithm , @xmath200 is the measured quantity of interest , @xmath201 is the time of the simulation , and @xmath202 is the errorbar associated to the measured quantity @xmath200 . \n",
    "this definition ensures that @xmath197 is independent of the time of the simulation . as a result , \n",
    "the larger @xmath197 the more efficient the algorithm .    in the present case \n",
    "we have @xmath203 , while @xmath204 for the original sgf algorithm . \n",
    "it is useful here to realize that , by symmetry , the mean values of @xmath118 and @xmath177 ( and @xmath117 and @xmath178 ) must be equal . \n",
    "therefore we define @xmath205 and @xmath206 . \n",
    "it seems reasonable to impose a condition of uniform sampling , @xmath207 . \n",
    "this condition can be satisfied by adjusting dynamically the values of @xmath182 and @xmath183 during the thermalization process . for this purpose \n",
    "we introduce a new optimization parameter @xmath208 and apply the following algorithm from time to time while thermalizing ( we start with @xmath209 ) : @xmath210 thus we are left with the optimization parameter @xmath211 . in order to determine the optimal value , \n",
    "we have considered 2 different hamiltonians @xmath212 and @xmath213 , and evaluated the efficiency of the algorithm while scanning @xmath211 . \n",
    "the first hamiltonian we have considered describes free hardcore bosons and is exactly solvable , @xmath214 where the sum runs over pairs of first neighboring sites and @xmath215 is the hopping parameter . \n",
    "the second hamiltonian is highly non - trivial and describes a mixture of atoms and diatomic molecules , with a special term allowing conversions between the two species @xcite , @xmath216 where @xmath217 and @xmath218 ( @xmath219 and @xmath220 ) are the creation and annihilation operators of atoms ( molecules ) , @xmath221 , @xmath222 , @xmath223 , @xmath224 , and @xmath225 are respectively the hopping parameter of atoms , the hopping parameter of molecules , the atomic onsite interaction parameter , the molecular onsite interaction parameter , and the inter - species interaction parameter . \n",
    "the conversion term is tunable via the parameter @xmath226 and does not conserve the number @xmath227 of atoms or the number @xmath228 of molecules \n",
    ". however the total number of particles @xmath229 is conserved and is the canonical constraint . \n",
    "the parameter @xmath230 allows to control the ratio between the number of atoms and molecules . \n",
    "the application of the sgf algorithm to the hamiltonian ( [ twospecies ] ) is described in details in ref.@xcite . \n",
    "the changes coming with the directed update scheme are completely independent of the chosen hamiltonian . \n",
    "the following table shows the mean number of creations and destructions in one step , @xmath231 , and the relative efficiency @xmath232 of the algorithm applied to @xmath212 at half filling , for which we have measured the energy @xmath233 , the superfluid density @xmath234 , and the number of particles in the zero momentum state @xmath235 :    . \n",
    "relative efficiency of the algorithm applied to @xmath212 at half filling for the energy , the superfluid density , and the number of particles in the zero momentum state . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     while the best value of @xmath211 depends on the hamiltonian which is considered and the measured quantity , it appears that a good compromise is to choose @xmath211 between @xmath236 and @xmath237 . \n",
    "the improvment of the efficiency is remarkable . in the following , \n",
    "we illustrate the applicability of the algorithm to problems with non - uniform potentials , by adding a parabolic trap to the hamiltonian ( [ twospecies ] ) : @xmath238 the parameters @xmath239 and @xmath240 allow to control the curvature of the trap associated to atoms and molecules , respectively , and @xmath31 is the number of lattice sites . \n",
    "the inclusion of this term in the algorithm is trivial since only the values of the diagonal energies @xmath241 and @xmath242 are changed . \n",
    "figures ( [ density ] ) and ( [ momentum ] ) show the density profiles and momentum distribution functions obtained for a system with @xmath243 lattice sites initially loaded with @xmath244 atoms and no molecules , and the parameters @xmath245 , @xmath246 , @xmath247 , @xmath248 , @xmath249 , @xmath250 , @xmath251 , @xmath252 , @xmath253 , and @xmath254 . \n",
    "the presented results have been obtained by performing @xmath255 updates for thermalization , and @xmath256 updates with measurements ( an update is to be understood as the occurence of a diagonal configuration ) . \n",
    "the time of the simulation is about 8 hours on a cheap 32 bits laptop with 1ghz processor , with an implementation of the algorithm involving dynamical structures with pointers ( see ref.@xcite ) .    ) to the hamiltonian ( [ twospecies ] ) . \n",
    "the errorbars are smaller than the symbol sizes , and are the biggest in the neighborhood of site indices 23 and 47 where they equal the size of the symbols . , scaledwidth=45.0% ]    ) to the hamiltonian ( [ twospecies ] ) . \n",
    "the errorbars are smaller than the symbol sizes , and are the biggest for @xmath257 where they equal the size of the symbols . \n",
    ", scaledwidth=45.0% ] \n",
    "we have presented a directed update scheme for the sgf algorithm , which has the properties of keeping the simplicity and generality of the original algorithm , and improves significantly its efficiency . \n",
    "i would like to express special thanks to peter denteneer for useful suggestions . \n",
    "this work is part of the research program of the `` stichting voor fundamenteel onderzoek der materie ( fom ) , '' which is financially supported by the `` nederlandse organisatie voor wetenschappelijk onderzoek ( nwo ) . '' \n",
    "we describe here how to generate numbers with the appropriate exponential distribution ( [ exponentialdistribution ] ) . assuming that we have at our disposal a uniform random number generator that generates a random variable @xmath258 with the distribution @xmath259 for @xmath260 \n",
    ", we would like to find a function @xmath52 such that the random variable @xmath261 is generated with the distribution @xmath262 where @xmath46 and @xmath263 are the parameters of the exponential distribution . \n",
    "because of the relation @xmath261 , the probability to find @xmath264 in the range @xmath265 must be equal to the probability to find @xmath258 in the range @xmath266 . \n",
    "this implies the condition @xmath267 with @xmath268 . \n",
    "thus we have @xmath269 taking the anti - derivative with respect to @xmath270 on both sides of the equation , we get @xmath271 where @xmath272 is a constant . this constant and the correct sign are determined by imposing the conditions @xmath273 and @xmath274 . as a result , if @xmath270 is a realization of @xmath258 , then a realization of @xmath264 is given by @xmath275.\\ ] ]    10 nicholas metropolis and s. ulam , journal of the american statistical association , number 247 , volume 44 ( 1949 ) . \n",
    "'''\n",
    "abstract = '''\n",
    "in a recent publication we have presented the stochastic green function ( sgf ) algorithm , which has the properties of being general and easy to apply to any lattice hamiltonian of the form @xmath0 , where @xmath1 is diagonal in the chosen occupation number basis and @xmath2 has only positive matrix elements . \n",
    " we propose here a modified version of the update scheme that keeps the simplicity and generality of the original sgf algorithm , and enhances significantly its efficiency .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: {'meteor': np.float64(0.008030303422922908)}\n",
      "BLEU Score: {'bleu': 0.0, 'precisions': [1.0, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 9, 'reference_length': 9}\n"
     ]
    }
   ],
   "source": [
    "results = meteor.compute(predictions=[abstract], references=[body])\n",
    "print(f\"METEOR Score: {results}\")\n",
    "blue_reults = sacre_bleu.compute(predictions=[abstract], references=[body], smooth_method='exp')\n",
    "print(f\"BLEU Score: {blue_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import evaluate\n",
    "code_eval = evaluate.load(\"code_eval\")\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Human-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.4999999999999999), 'pass@2': np.float64(0.8)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 0}), (1, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 1}), (2, {'task_id': 0, 'passed': False, 'result': 'failed: ', 'completion_id': 2}), (3, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 3}), (4, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 4}), (5, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 5})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"assert candidate() == 1\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def candidate():    import subprocess\\n    subprocess.check_output('rm -rf tmp')\",\n",
    "    \"def candidate():    import time\\n    time.sleep(10)\\n    return 1\",\n",
    "    \"def candidate():    return input('enter a number')\",\n",
    "    \"def candidate():    return 1\",\n",
    "    \"def candidate():  return 1\",\n",
    "    \"def candidate():\\treturn 1\"\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.5), 'pass@2': np.float64(1.0)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 0}), (1, {'task_id': 0, 'passed': False, 'result': \"failed: name 'candidate' is not defined\", 'completion_id': 1})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"assert candidate() == 1\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def candidate(): return 1\",\n",
    "    \"def fail(): return 1\",\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiline (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'failed: unexpected indent (string)' -> because you can't have \\n for multiple lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.0), 'pass@2': np.float64(0.0)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 0}), (1, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 1}), (2, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 3)', 'completion_id': 2}), (3, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 3)', 'completion_id': 3}), (4, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 3)', 'completion_id': 4}), (5, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 3)', 'completion_id': 5})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"assert candidate() == 1 \\n assert candidate() == 2\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def candidate():    import subprocess\\n    subprocess.check_output('rm -rf tmp')\",\n",
    "    \"def candidate():    import time\\n    time.sleep(10)\\n    return 1\",\n",
    "    \"def candidate():    return input('enter a number')\",\n",
    "    \"def candidate():    return 1\",\n",
    "    \"def candidate():  return 1\",\n",
    "    \"def candidate():\\treturn 1\"\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'failed: unexpected indent (string, line 2)' -> because you can't have multiple lines in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.0), 'pass@2': np.float64(0.0)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 0}), (1, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 1}), (2, {'task_id': 0, 'passed': False, 'result': 'failed: invalid syntax (<string>, line 2)', 'completion_id': 2}), (3, {'task_id': 0, 'passed': False, 'result': 'failed: invalid syntax (<string>, line 2)', 'completion_id': 3}), (4, {'task_id': 0, 'passed': False, 'result': 'failed: invalid syntax (<string>, line 2)', 'completion_id': 4}), (5, {'task_id': 0, 'passed': False, 'result': 'failed: invalid syntax (<string>, line 2)', 'completion_id': 5})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"assert candidate() == 1 assert candidate() != 2\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def candidate():    import subprocess\\n    subprocess.check_output('rm -rf tmp')\",\n",
    "    \"def candidate():    import time\\n    time.sleep(10)\\n    return 1\",\n",
    "    \"def candidate():    return input('enter a number')\",\n",
    "    \"def candidate():    return 1\",\n",
    "    \"def candidate():  return 1\",\n",
    "    \"def candidate():\\treturn 1\"\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.0), 'pass@2': np.float64(0.0)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 0}), (1, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 1}), (2, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 3)', 'completion_id': 2}), (3, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 3)', 'completion_id': 3}), (4, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 3)', 'completion_id': 4}), (5, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 3)', 'completion_id': 5})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"assert candidate() == 1 \\n\\t assert candidate() != 2\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def candidate():    import subprocess\\n    subprocess.check_output('rm -rf tmp')\",\n",
    "    \"def candidate():    import time\\n    time.sleep(10)\\n    return 1\",\n",
    "    \"def candidate():    return input('enter a number')\",\n",
    "    \"def candidate():    return 1\",\n",
    "    \"def candidate():  return 1\",\n",
    "    \"def candidate():\\treturn 1\"\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.5), 'pass@2': np.float64(1.0)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 0}), (1, {'task_id': 0, 'passed': False, 'result': \"failed: name 'candidate' is not defined\", 'completion_id': 1})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = '''\n",
    "assert candidate() == 1\n",
    "assert type(candidate()) is int \n",
    "'''\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def candidate():\\treturn 1\",\n",
    "    \"def fail(): return 1\"\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically you need to just make it two tests and use the same generated solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.4999999999999999), 'pass@2': np.float64(0.8)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 0}), (1, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 1}), (2, {'task_id': 0, 'passed': False, 'result': 'failed: ', 'completion_id': 2}), (3, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 3}), (4, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 4}), (5, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 5})], 1: [(0, {'task_id': 1, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 0}), (1, {'task_id': 1, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 1}), (2, {'task_id': 1, 'passed': False, 'result': 'failed: ', 'completion_id': 2}), (3, {'task_id': 1, 'passed': True, 'result': 'passed', 'completion_id': 3}), (4, {'task_id': 1, 'passed': True, 'result': 'passed', 'completion_id': 4}), (5, {'task_id': 1, 'passed': True, 'result': 'passed', 'completion_id': 5})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = [\"assert candidate() == 1\", \"assert candidate() != 2\"]\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def candidate():    import subprocess\\n    subprocess.check_output('rm -rf tmp')\",\n",
    "    \"def candidate():    import time\\n    time.sleep(10)\\n    return 1\",\n",
    "    \"def candidate():    return input('enter a number')\",\n",
    "    \"def candidate():    return 1\",\n",
    "    \"def candidate():  return 1\",\n",
    "    \"def candidate():\\treturn 1\"\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=reference_test,\n",
    "    predictions=[generated_solutions, generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiline (function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can already see as in the examples that \\n causes issues in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.4999999999999999), 'pass@2': np.float64(0.8)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 0}), (1, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 1}), (2, {'task_id': 0, 'passed': False, 'result': 'failed: ', 'completion_id': 2}), (3, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 3}), (4, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 4}), (5, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 5})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"assert candidate() == 1\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def candidate():    import subprocess\\n    subprocess.check_output('rm -rf tmp')\",\n",
    "    \"def candidate():    import time\\n    time.sleep(10)\\n    return 1\",\n",
    "    \"def candidate():    return input('enter a number')\",\n",
    "    \"def candidate():    return 1\",\n",
    "    \"def candidate():  return 1\",\n",
    "    \"def candidate():\\treturn 1\"\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First glance this returns TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(1.0)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 0})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"assert add(2, 3) == 5\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def add(a, b):\\n\\t result = a + b \\n\\t return result\"\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this also returns true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.0), 'pass@2': np.float64(0.0)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 0}), (1, {'task_id': 0, 'passed': False, 'result': 'failed: ', 'completion_id': 1})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"assert add(2, 3) == 5\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def add(a, b):\\n\\t result = a + b \\n\\t return result\",\n",
    "    \"def add(a, b):\\n\\t result = a - b \\n\\t return result\"\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.0)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': False, 'result': 'failed: unexpected indent (<string>, line 2)', 'completion_id': 0})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"assert add(2, 3) == 5\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    '''\n",
    "    def add(a, b):\n",
    "        result = a + b\n",
    "        return result\n",
    "    '''\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct (There doesn't seem to be a way to do it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.5), 'pass@2': np.float64(1.0)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': False, 'result': 'failed: expected an indented block after function definition on line 1 (<string>, line 2)', 'completion_id': 0}), (1, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 1})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"def check(candidate): assert candidate(2, 2) == 4\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def add(a, b):\",\n",
    "    \"def add(a, b): return a - b\",\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pass@1': np.float64(0.5), 'pass@2': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\"assert add(2,3)==5\"]\n",
    "candidates = [[\"def add(a, b): return a*b\", \"def add(a, b): return a+b\"]]\n",
    "pass_at_k, results = code_eval.compute(references=test_cases, predictions=candidates, k=[1, 2])\n",
    "print(pass_at_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pass@1': np.float64(0.8333333333333333), 'pass@2': np.float64(1.0)}, defaultdict(<class 'list'>, {0: [(0, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 0}), (1, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 1}), (2, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 2}), (3, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 3}), (4, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 4}), (5, {'task_id': 0, 'passed': False, 'result': 'failed: expected an indented block after function definition on line 1 (<string>, line 2)', 'completion_id': 5})]}))\n"
     ]
    }
   ],
   "source": [
    "# Define your multi-line reference solution\n",
    "reference_test = \"assert candidate() == 1\"\n",
    "\n",
    "# Define your generated solutions (these could also be from a model's output)\n",
    "generated_solutions = [\n",
    "    \"def candidate():    return 1\",\n",
    "    \"def candidate():  return 1\",\n",
    "    \"def candidate():return 1\",\n",
    "    \"def candidate():\\treturn 1\",\n",
    "    \"def candidate():\\n\\treturn 1\",\n",
    "    \"def candidate():\\nreturn 1\",\n",
    "]\n",
    "\n",
    "# Evaluate pass@k\n",
    "results = code_eval.compute(\n",
    "    references=[reference_test],\n",
    "    predictions=[generated_solutions],\n",
    "    k=[1, 2]\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yisrani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/yisrani/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/yisrani/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "from sacrebleu import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.7361703354503866, 'precisions': [0.7692307692307693, 0.75, 0.7272727272727273, 0.7], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 13, 'reference_length': 13}\n",
      "BLEU = 73.62 76.9/75.0/72.7/70.0 (BP = 1.000 ratio = 1.000 hyp_len = 13 ref_len = 13)\n",
      "{'bleu': 0.740818220681718, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 0.740818220681718, 'length_ratio': 0.7692307692307693, 'translation_length': 10, 'reference_length': 13}\n",
      "BLEU = 74.08 100.0/100.0/100.0/100.0 (BP = 0.741 ratio = 0.769 hyp_len = 10 ref_len = 13)\n"
     ]
    }
   ],
   "source": [
    "reference = \"Rounds the values of a tensor to the nearest integer element - wise\"\n",
    "prediction = \"Rounds the values of a tensor to the nearest integer, element-wise.\"\n",
    "prediction2 = \"Rounds the values of a tensor to the nearest integer\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)\n",
    "bleu_results = bleu.compute(predictions=[prediction2], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction2], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.1, 0.0, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 10, 'reference_length': 10}\n",
      "BLEU = 4.20 10.0/5.6/3.1/1.8 (BP = 1.000 ratio = 1.000 hyp_len = 10 ref_len = 10)\n",
      "{'bleu': 0.2790159393585827, 'precisions': [0.5454545454545454, 0.4, 0.2222222222222222, 0.125], 'brevity_penalty': 1.0, 'length_ratio': 1.1, 'translation_length': 11, 'reference_length': 10}\n",
      "BLEU = 27.90 54.5/40.0/22.2/12.5 (BP = 1.000 ratio = 1.100 hyp_len = 11 ref_len = 10)\n"
     ]
    }
   ],
   "source": [
    "reference = \"Start Media Driver as a stand - alone process .\"\n",
    "prediction = \"Main method that starts the CLR Bridge from Java .\"\n",
    "prediction2 = \"Main method for running Media Driver as a standalone process.\"\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction], smooth_method='exp')\n",
    "print(sacre_results)\n",
    "bleu_results = bleu.compute(predictions=[prediction2], references=[reference])\n",
    "print(bleu_results)\n",
    "sacre_results = corpus_bleu(references=[[reference]], hypotheses=[prediction2], smooth_method='exp')\n",
    "print(sacre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.8153551038173115, 'precisions': [0.9375, 0.8666666666666667, 0.7857142857142857, 0.6923076923076923], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 16, 'reference_length': 16}\n",
      "{'rouge1': np.float64(0.8571428571428571), 'rouge2': np.float64(0.6666666666666666), 'rougeL': np.float64(0.8571428571428571), 'rougeLsum': np.float64(0.8571428571428571)}\n",
      "{'meteor': np.float64(0.9268808114961962)}\n"
     ]
    }
   ],
   "source": [
    "reference = '''\n",
    "def simple_example():\n",
    "    message = \"hello\"\n",
    "    print(message)\n",
    "'''\n",
    "\n",
    "prediction = '''\n",
    "def simple_example():\n",
    "    different = \"hello\"\n",
    "    print(message)\n",
    "'''\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "rouge_results = rouge.compute(predictions=[prediction], references=[reference])\n",
    "print(rouge_results)\n",
    "meteor_results = meteor.compute(predictions=[prediction], references=[reference])\n",
    "print(meteor_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.6865065103648591, 'precisions': [0.875, 0.7333333333333333, 0.6428571428571429, 0.5384615384615384], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 16, 'reference_length': 16}\n",
      "{'rouge1': np.float64(0.7142857142857143), 'rouge2': np.float64(0.5), 'rougeL': np.float64(0.7142857142857143), 'rougeLsum': np.float64(0.7142857142857143)}\n",
      "{'meteor': np.float64(0.8504464285714286)}\n"
     ]
    }
   ],
   "source": [
    "reference = '''\n",
    "def simple_example():\n",
    "    message = \"hello\"\n",
    "    print(message)\n",
    "'''\n",
    "\n",
    "prediction = '''\n",
    "def simple_example():\n",
    "    different = \"hello\"\n",
    "    print(different)\n",
    "'''\n",
    "\n",
    "bleu_results = bleu.compute(predictions=[prediction], references=[reference])\n",
    "print(bleu_results)\n",
    "rouge_results = rouge.compute(predictions=[prediction], references=[reference])\n",
    "print(rouge_results)\n",
    "meteor_results = meteor.compute(predictions=[prediction], references=[reference])\n",
    "print(meteor_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
